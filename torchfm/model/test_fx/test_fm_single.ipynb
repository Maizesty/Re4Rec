{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace success!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import fm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.fx import subgraph_rewriter, symbolic_trace\n",
    "import utils\n",
    "from torch.fx import Proxy, Graph, GraphModule\n",
    "from torch.fx.passes.utils.matcher_utils import SubgraphMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pattern_replace_and_matcher_for_FM(traced,\n",
    "                                                  redency_part_slice,unredency_part_slice,\n",
    "                                                  target_node_name,match_func = None\n",
    "                                                ):\n",
    "  from torch.fx.passes.utils.matcher_utils import SubgraphMatcher\n",
    "\n",
    "  class PatternClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.embed = torch.nn.Embedding(1, 1)\n",
    "      # self.offsets = torch.as_tensor(np.array((0, *np.cumsum([10,10])[:-1]), dtype=np.int64))\n",
    "      self.offsets = nn.Parameter(torch.as_tensor(np.array((0, *np.cumsum([10,10])[:-1]), dtype=np.int64)),False)\n",
    "    def forward(self,x):\n",
    "      x = x + self.offsets\n",
    "      x = self.embed(x)\n",
    "      square_of_sum = torch.sum(x, dim=1) ** 2\n",
    "      sum_of_square = torch.sum(x ** 2, dim=1)\n",
    "      ix = square_of_sum - sum_of_square    \n",
    "      ix = torch.sum(ix, dim=1, keepdim=True)  \n",
    "      return 0.5 * ix\n",
    "  def _match(match,ori,pat):\n",
    "    return True \n",
    "  # env  = utils.get_env(traced)\n",
    "  # target_node = env[target_node_name]\n",
    "  # target_node_mod = utils.get_target_mod(traced,target_node_name,\"_\")\n",
    "  pattern = PatternClass()  \n",
    "  pattern_trace = symbolic_trace(pattern)\n",
    "  pattern_graph = pattern_trace.graph\n",
    "  original_graph = traced.graph\n",
    "  matcher =  SubgraphMatcher(pattern_graph, match_output=False, match_placeholder=False,\n",
    "                              remove_overlapping_matches=True)\n",
    "  _matches = matcher.match(original_graph)\n",
    "  match_filters = [_match if match_func is None else match_func]\n",
    "  _matches = [\n",
    "      m for m in _matches\n",
    "      if all(match_filter(m, original_graph, pattern_graph)\n",
    "              for match_filter in match_filters)\n",
    "  ]  \n",
    "  # 因为在过滤器中做了限制应该只有一个符合要求的\n",
    "  _matched = _matches[0]\n",
    "  pattern_env = utils.get_env(pattern_trace)\n",
    "  node_map = _matched.nodes_map\n",
    "  pn = pattern_env['offsets']\n",
    "  offsets_node = node_map[pn]\n",
    "  offsets_val = utils.get_target_mod(traced,offsets_node.target)\n",
    "  \n",
    "  embed_node = node_map[pattern_env['embed']]\n",
    "  embed_node_module = utils.get_target_mod(traced,embed_node.target)\n",
    "  \n",
    "\n",
    "  \n",
    "  \n",
    "  class ReplacementClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.redency_offset = nn.Parameter(torch.as_tensor(np.array(offsets_val[redency_part_slice[1]], dtype=np.int64)),False)\n",
    "      self.unredency_offset = nn.Parameter(torch.as_tensor(np.array(offsets_val[unredency_part_slice[1]], dtype=np.int64)),False)\n",
    "      embedding_config = embed_node_module.weight.data.shape\n",
    "      self.embed = nn.Embedding(embedding_config[0],embedding_config[1])\n",
    "      self.embed.weight.data.copy_(embed_node_module.weight.data)\n",
    "    def forward(self,x):\n",
    "      redency_part = x[redency_part_slice] + self.redency_offset\n",
    "      unredency_part = x[unredency_part_slice] + self.unredency_offset\n",
    "      redency_embed = self.embed(redency_part)\n",
    "      unredency_embed = self.embed(unredency_part)\n",
    "      redency_embed_sum = torch.sum(redency_embed,dim=0)\n",
    "      unredency_embed_sum = torch.sum(unredency_embed,dim=1)\n",
    "      square_of_sum = (redency_embed_sum + unredency_embed_sum) ** 2\n",
    "      redency_embed_square_sum = torch.sum(redency_embed ** 2,dim=0)\n",
    "      unredency_embed_square_sum = torch.sum(unredency_embed ** 2,dim=1)\n",
    "      sum_of_square = redency_embed_square_sum + unredency_embed_square_sum\n",
    "      ix = square_of_sum - sum_of_square\n",
    "      ix = torch.sum(ix,dim = 1,keepdim=True)\n",
    "      return 0.5 * ix\n",
    "    \n",
    "  \n",
    "  return pattern,ReplacementClass(),_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def workload_fm(dim,redundancy):\n",
    "  print(f\"now gen workload of fm with config: dim: {dim}\")\n",
    "  fm_model_ori = fm.FactorizationMachineModel([100 for i in range(100)],dim)\n",
    "  fm_model_traced_ori = symbolic_trace(fm_model_ori)\n",
    "  \n",
    "  fm_model_modify = fm.FactorizationMachineModel([100 for i in range(100)],dim)\n",
    "  fm_model_traced_modify = symbolic_trace(fm_model_modify)\n",
    "  pattern,replace,match = gen_pattern_replace_and_matcher_for_FM(fm_model_traced_modify,\n",
    "                                                                      (0,slice(None,redundancy,None)),(slice(None,None,None),slice(redundancy,None,None)),\n",
    "                                                                      \"linear_offsets\")\n",
    "  matches = subgraph_rewriter.replace_pattern_with_filters(fm_model_traced_modify, pattern, replace,[match])\n",
    "  return fm_model_traced_ori,fm_model_traced_modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of fm with config: dim: 128\n"
     ]
    }
   ],
   "source": [
    "ori , modify = workload_fm(128,80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.8 ms ± 4.12 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():ori(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 ms ± 1.71 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of fm with config: dim: 128\n"
     ]
    }
   ],
   "source": [
    "ori , modify = workload_fm(128,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.2 ms ± 2.86 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of fm with config: dim: 128\n"
     ]
    }
   ],
   "source": [
    "ori , modify = workload_fm(128,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.4 ms ± 2.1 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of fm with config: dim: 128\n"
     ]
    }
   ],
   "source": [
    "ori , modify = workload_fm(128,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.5 ms ± 2.94 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class FactorizationMachineModel(torch.nn.Module):\n",
      "    def forward(self, x):\n",
      "        # No stacktrace found for following nodes\n",
      "        linear_offsets = self.linear.offsets\n",
      "        add = x + linear_offsets;  linear_offsets = None\n",
      "        linear_fc = self.linear.fc(add);  add = None\n",
      "        sum_1 = torch.sum(linear_fc, dim = 1);  linear_fc = None\n",
      "        linear_bias = self.linear.bias\n",
      "        add_1 = sum_1 + linear_bias;  sum_1 = linear_bias = None\n",
      "        embedding_offsets = self.embedding.offsets\n",
      "        add_2 = x + embedding_offsets;  x = embedding_offsets = None\n",
      "        embedding_embedding = self.embedding.embedding(add_2);  add_2 = None\n",
      "        sum_2 = torch.sum(embedding_embedding, dim = 1)\n",
      "        pow_1 = sum_2 ** 2;  sum_2 = None\n",
      "        pow_2 = embedding_embedding ** 2;  embedding_embedding = None\n",
      "        sum_3 = torch.sum(pow_2, dim = 1);  pow_2 = None\n",
      "        sub = pow_1 - sum_3;  pow_1 = sum_3 = None\n",
      "        sum_4 = torch.sum(sub, dim = 1, keepdim = True);  sub = None\n",
      "        mul = 0.5 * sum_4;  sum_4 = None\n",
      "        add_3 = add_1 + mul;  add_1 = mul = None\n",
      "        squeeze = add_3.squeeze(1);  add_3 = None\n",
      "        sigmoid = torch.sigmoid(squeeze);  squeeze = None\n",
      "        return sigmoid\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'class FactorizationMachineModel(torch.nn.Module):\\n    def forward(self, x):\\n        # No stacktrace found for following nodes\\n        linear_offsets = self.linear.offsets\\n        add = x + linear_offsets;  linear_offsets = None\\n        linear_fc = self.linear.fc(add);  add = None\\n        sum_1 = torch.sum(linear_fc, dim = 1);  linear_fc = None\\n        linear_bias = self.linear.bias\\n        add_1 = sum_1 + linear_bias;  sum_1 = linear_bias = None\\n        embedding_offsets = self.embedding.offsets\\n        add_2 = x + embedding_offsets;  x = embedding_offsets = None\\n        embedding_embedding = self.embedding.embedding(add_2);  add_2 = None\\n        sum_2 = torch.sum(embedding_embedding, dim = 1)\\n        pow_1 = sum_2 ** 2;  sum_2 = None\\n        pow_2 = embedding_embedding ** 2;  embedding_embedding = None\\n        sum_3 = torch.sum(pow_2, dim = 1);  pow_2 = None\\n        sub = pow_1 - sum_3;  pow_1 = sum_3 = None\\n        sum_4 = torch.sum(sub, dim = 1, keepdim = True);  sub = None\\n        mul = 0.5 * sum_4;  sum_4 = None\\n        add_3 = add_1 + mul;  add_1 = mul = None\\n        squeeze = add_3.squeeze(1);  add_3 = None\\n        sigmoid = torch.sigmoid(squeeze);  squeeze = None\\n        return sigmoid\\n        '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori.print_readable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total true time 38.69748115539551 ms\n",
      "total time: 51.5139102935791 ms\n",
      "Op type        Op                     Average runtime (ms)    Pct total runtime\n",
      "-------------  -------------------  ----------------------  -------------------\n",
      "call_module    embedding_embedding              16.9585              32.9202\n",
      "call_function  pow_2                            12.4335              24.1363\n",
      "call_function  sum_2                             3.04627              5.9135\n",
      "call_function  sum_3                             2.63882              5.12253\n",
      "call_function  add                               0.842333             1.63516\n",
      "call_module    linear_fc                         0.714302             1.38662\n",
      "call_function  add_2                             0.558615             1.0844\n",
      "call_function  sub                               0.513315             0.996459\n",
      "call_function  pow_1                             0.434399             0.843265\n",
      "call_function  sum_1                             0.119925             0.2328\n",
      "call_function  sum_4                             0.100851             0.195774\n",
      "call_function  mul                               0.0808239            0.156897\n",
      "call_method    squeeze                           0.043869             0.0851596\n",
      "call_function  sigmoid                           0.0350475            0.0680351\n",
      "placeholder    x                                 0.0348091            0.0675723\n",
      "call_function  add_1                             0.0348091            0.0675723\n",
      "get_attr       linear_bias                       0.0319481            0.0620184\n",
      "call_function  add_3                             0.0288486            0.0560017\n",
      "get_attr       linear_offsets                    0.0197887            0.0384144\n",
      "get_attr       embedding_offsets                 0.0154972            0.0300835\n",
      "output         output                            0.0112057            0.0217527\n"
     ]
    }
   ],
   "source": [
    "interp = utils.ProfilingInterpreter(ori)\n",
    "interp.run(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))\n",
    "print(interp.summary(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pattern_replace_and_matcher_for_FM_pow(traced,\n",
    "                                                  redency_part_slice,unredency_part_slice,\n",
    "                                                  target_node_name,batch = 4096,match_func = None\n",
    "                                                ):\n",
    "  from torch.fx.passes.utils.matcher_utils import SubgraphMatcher\n",
    "\n",
    "  class PatternClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.embed = torch.nn.Embedding(1, 1)\n",
    "      self.offsets = torch.as_tensor(np.array((0, *np.cumsum([10,10])[:-1]), dtype=np.int64))\n",
    "\n",
    "    def forward(self,x):\n",
    "      x = x + self.offsets\n",
    "      x = self.embed(x)\n",
    "      square_of_sum = torch.sum(x, dim=1) ** 2\n",
    "      sum_of_square = torch.sum(x ** 2, dim=1)\n",
    "      ix = square_of_sum - sum_of_square    \n",
    "      ix = torch.sum(ix, dim=1, keepdim=True)  \n",
    "      return 0.5 * ix\n",
    "  def _match(match,ori,pat):\n",
    "    return True \n",
    "  # env  = utils.get_env(traced)\n",
    "  # target_node = env[target_node_name]\n",
    "  # target_node_mod = utils.get_target_mod(traced,target_node_name,\"_\")\n",
    "  pattern = PatternClass()  \n",
    "  pattern_trace = symbolic_trace(pattern)\n",
    "  pattern_graph = pattern_trace.graph\n",
    "  original_graph = traced.graph\n",
    "  matcher =  SubgraphMatcher(pattern_graph, match_output=False, match_placeholder=False,\n",
    "                              remove_overlapping_matches=True)\n",
    "  _matches = matcher.match(original_graph)\n",
    "  match_filters = [_match if match_func is None else match_func]\n",
    "  _matches = [\n",
    "      m for m in _matches\n",
    "      if all(match_filter(m, original_graph, pattern_graph)\n",
    "              for match_filter in match_filters)\n",
    "  ]  \n",
    "  # 因为在过滤器中做了限制应该只有一个符合要求的\n",
    "  _matched = _matches[0]\n",
    "  pattern_env = utils.get_env(pattern_trace)\n",
    "  node_map = _matched.nodes_map\n",
    "  pn = pattern_env['offsets']\n",
    "  offsets_node = node_map[pn]\n",
    "  offsets_val = utils.get_target_mod(traced,offsets_node.target)\n",
    "  \n",
    "  embed_node = node_map[pattern_env['embed']]\n",
    "  embed_node_module = utils.get_target_mod(traced,embed_node.target)\n",
    "  \n",
    "\n",
    "  \n",
    "  \n",
    "  class ReplacementClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.offsets = offsets_val\n",
    "      self.redency_offset = torch.as_tensor(np.array(offsets_val[redency_part_slice[1]], dtype=np.int64))\n",
    "      self.unredency_offset = torch.as_tensor(np.array(offsets_val[unredency_part_slice[1]], dtype=np.int64))\n",
    "      embedding_config = embed_node_module.weight.data.shape\n",
    "      self.embed = nn.Embedding(embedding_config[0],embedding_config[1])\n",
    "      self.embed.weight.data.copy_(embed_node_module.weight.data)\n",
    "    def forward(self,x):\n",
    "      x = self.offsets  + x\n",
    "      x = self.embed(x)\n",
    "      square_of_sum = torch.sum(x, dim=1) ** 2\n",
    "      \n",
    "      \n",
    "      \n",
    "      redency_part_embed_pow = x[redency_part_slice]  ** 2\n",
    "      unredency_part_embed_pow = x[unredency_part_slice] ** 2\n",
    "      redency_part_embed_pow = redency_part_embed_pow.repeat(batch,1,1)\n",
    "      embed = torch.concat([redency_part_embed_pow,unredency_part_embed_pow], dim = 1)\n",
    "      sum_of_square = torch.sum(embed, dim = 1)\n",
    "      ix = square_of_sum - sum_of_square\n",
    "      ix = torch.sum(ix,dim = 1,keepdim=True)\n",
    "      return 0.5 * ix\n",
    "    \n",
    "  \n",
    "  return pattern,ReplacementClass(),_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def workload_fm_pow(dim,redundancy):\n",
    "  print(f\"now gen workload of fm with config: dim: {dim}\")\n",
    "  fm_model_ori = fm.FactorizationMachineModel([100 for i in range(100)],dim)\n",
    "  fm_model_traced_ori = symbolic_trace(fm_model_ori)\n",
    "  \n",
    "  fm_model_modify = fm.FactorizationMachineModel([100 for i in range(100)],dim)\n",
    "  fm_model_traced_modify = symbolic_trace(fm_model_modify)\n",
    "  pattern,replace,match = gen_pattern_replace_and_matcher_for_FM_pow(fm_model_traced_modify,\n",
    "                                                                      (0,slice(None,redundancy,None)),(slice(None,None,None),slice(redundancy,None,None)),\n",
    "                                                                      \"linear_offsets\")\n",
    "  matches = subgraph_rewriter.replace_pattern_with_filters(fm_model_traced_modify, pattern, replace,[match])\n",
    "  return fm_model_traced_ori,fm_model_traced_modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of fm with config: dim: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node offsets target offsets offsets of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori , modify = workload_fm_pow(128,80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.4 ms ± 4.28 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():ori(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.5 ms ± 4.33 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pattern_replace_and_matcher_for_FM_reduce(traced,\n",
    "                                                  redency_part_slice,unredency_part_slice,\n",
    "                                                  target_node_name,batch = 4096,match_func = None\n",
    "                                                ):\n",
    "  from torch.fx.passes.utils.matcher_utils import SubgraphMatcher\n",
    "\n",
    "  class PatternClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.embed = torch.nn.Embedding(1, 1)\n",
    "      # self.offsets = torch.as_tensor(np.array((0, *np.cumsum([10,10])[:-1]), dtype=np.int64))\n",
    "      self.offsets = nn.Parameter(torch.as_tensor(np.array((0, *np.cumsum([10,10])[:-1]), dtype=np.int64)),False)\n",
    "    def forward(self,x):\n",
    "      x = x + self.offsets\n",
    "      x = self.embed(x)\n",
    "      square_of_sum = torch.sum(x, dim=1) ** 2\n",
    "      sum_of_square = torch.sum(x ** 2, dim=1)\n",
    "      ix = square_of_sum - sum_of_square    \n",
    "      ix = torch.sum(ix, dim=1, keepdim=True)  \n",
    "      return 0.5 * ix\n",
    "  def _match(match,ori,pat):\n",
    "    return True \n",
    "  # env  = utils.get_env(traced)\n",
    "  # target_node = env[target_node_name]\n",
    "  # target_node_mod = utils.get_target_mod(traced,target_node_name,\"_\")\n",
    "  pattern = PatternClass()  \n",
    "  pattern_trace = symbolic_trace(pattern)\n",
    "  pattern_graph = pattern_trace.graph\n",
    "  original_graph = traced.graph\n",
    "  matcher =  SubgraphMatcher(pattern_graph, match_output=False, match_placeholder=False,\n",
    "                              remove_overlapping_matches=True)\n",
    "  _matches = matcher.match(original_graph)\n",
    "  match_filters = [_match if match_func is None else match_func]\n",
    "  _matches = [\n",
    "      m for m in _matches\n",
    "      if all(match_filter(m, original_graph, pattern_graph)\n",
    "              for match_filter in match_filters)\n",
    "  ]  \n",
    "  # 因为在过滤器中做了限制应该只有一个符合要求的\n",
    "  _matched = _matches[0]\n",
    "  pattern_env = utils.get_env(pattern_trace)\n",
    "  node_map = _matched.nodes_map\n",
    "  pn = pattern_env['offsets']\n",
    "  offsets_node = node_map[pn]\n",
    "  offsets_val = utils.get_target_mod(traced,offsets_node.target)\n",
    "  \n",
    "  embed_node = node_map[pattern_env['embed']]\n",
    "  embed_node_module = utils.get_target_mod(traced,embed_node.target)\n",
    "  \n",
    "\n",
    "  \n",
    "  \n",
    "  class ReplacementClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.offsets = offsets_val\n",
    "      self.redency_offset = nn.Parameter(torch.as_tensor(np.array(offsets_val[redency_part_slice[1]], dtype=np.int64)),False)\n",
    "      self.unredency_offset = nn.Parameter(torch.as_tensor(np.array(offsets_val[unredency_part_slice[1]], dtype=np.int64)),False)\n",
    "      embedding_config = embed_node_module.weight.data.shape\n",
    "      self.embed = nn.Embedding(embedding_config[0],embedding_config[1])\n",
    "      self.embed.weight.data.copy_(embed_node_module.weight.data)\n",
    "    def forward(self,x):\n",
    "      x = self.offsets  + x\n",
    "      x = self.embed(x)\n",
    "      square_of_sum = torch.sum(x, dim=1) ** 2\n",
    "      pow_x = x ** 2\n",
    "      \n",
    "      \n",
    "      redency_part_embed_pow = pow_x[redency_part_slice] \n",
    "      unredency_part_embed_pow = pow_x[unredency_part_slice] \n",
    "      redency_embed_square_sum = torch.sum(redency_part_embed_pow ,dim=0)\n",
    "      unredency_embed_square_sum = torch.sum(unredency_part_embed_pow ,dim=1)\n",
    "      sum_of_square = redency_embed_square_sum + unredency_embed_square_sum\n",
    "      # sum_of_square = torch.sum(embed, dim = 1)\n",
    "      ix = square_of_sum - sum_of_square\n",
    "      ix = torch.sum(ix,dim = 1,keepdim=True)\n",
    "      return 0.5 * ix\n",
    "    \n",
    "  \n",
    "  return pattern,ReplacementClass(),_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def workload_fm_reduce(dim,redundancy):\n",
    "  print(f\"now gen workload of fm with config: dim: {dim}\")\n",
    "  fm_model_ori = fm.FactorizationMachineModel([100 for i in range(100)],dim)\n",
    "  fm_model_traced_ori = symbolic_trace(fm_model_ori)\n",
    "  \n",
    "  fm_model_modify = fm.FactorizationMachineModel([100 for i in range(100)],dim)\n",
    "  fm_model_traced_modify = symbolic_trace(fm_model_modify)\n",
    "  pattern,replace,match = gen_pattern_replace_and_matcher_for_FM_reduce(fm_model_traced_modify,\n",
    "                                                                      (0,slice(None,redundancy,None)),(slice(None,None,None),slice(redundancy,None,None)),\n",
    "                                                                      \"linear_offsets\")\n",
    "  matches = subgraph_rewriter.replace_pattern_with_filters(fm_model_traced_modify, pattern, replace,[match])\n",
    "  return fm_model_traced_ori,fm_model_traced_modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of fm with config: dim: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node offsets target offsets offsets of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori , modify = workload_fm_reduce(128,80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total true time 37.42265701293945 ms\n",
      "total time: 54.779052734375 ms\n",
      "Op type        Op                     Average runtime (ms)    Pct total runtime\n",
      "-------------  -------------------  ----------------------  -------------------\n",
      "call_module    embedding_embedding              16.974               30.9862\n",
      "call_function  pow_2                            12.1913              22.2554\n",
      "call_function  sum_2                             2.52962              4.61786\n",
      "call_function  sum_3                             2.43092              4.43767\n",
      "call_function  add                               0.763893             1.3945\n",
      "call_module    linear_fc                         0.64826              1.18341\n",
      "call_function  sub                               0.534773             0.976236\n",
      "call_function  add_2                             0.53072              0.968837\n",
      "call_function  pow_1                             0.224829             0.410428\n",
      "call_function  sum_1                             0.128746             0.235028\n",
      "call_function  sum_4                             0.110626             0.20195\n",
      "call_function  mul                               0.0870228            0.158861\n",
      "call_method    squeeze                           0.0431538            0.0787779\n",
      "call_function  sigmoid                           0.0388622            0.0709436\n",
      "call_function  add_1                             0.0369549            0.0674617\n",
      "get_attr       linear_bias                       0.0355244            0.0648503\n",
      "call_function  add_3                             0.0321865            0.058757\n",
      "placeholder    x                                 0.0288486            0.0526636\n",
      "get_attr       linear_offsets                    0.0207424            0.0378656\n",
      "get_attr       embedding_offsets                 0.0185966            0.0339485\n",
      "output         output                            0.013113             0.023938\n"
     ]
    }
   ],
   "source": [
    "interp = utils.ProfilingInterpreter(ori)\n",
    "interp.run(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))\n",
    "print(interp.summary(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total true time 37.03117370605469 ms\n",
      "total time: 54.56137657165527 ms\n",
      "Op type        Op                Average runtime (ms)    Pct total runtime\n",
      "-------------  --------------  ----------------------  -------------------\n",
      "call_module    embed                       16.7956              30.783\n",
      "call_function  pow_2                       12.7232              23.3191\n",
      "call_function  sum_2                        2.62594              4.81282\n",
      "call_function  sum_4                        0.896215             1.64258\n",
      "call_function  add                          0.795126             1.45731\n",
      "call_module    linear_fc                    0.729084             1.33626\n",
      "call_method    add_2                        0.55933              1.02514\n",
      "call_function  add_3                        0.521421             0.95566\n",
      "call_function  pow_1                        0.404358             0.741107\n",
      "call_function  getitem                      0.16737              0.306755\n",
      "call_function  sum_1                        0.134706             0.24689\n",
      "call_function  sub                          0.114679             0.210184\n",
      "call_function  sum_5                        0.0958443            0.175663\n",
      "call_function  mul                          0.0915527            0.167798\n",
      "call_function  sum_3                        0.0874996            0.160369\n",
      "call_method    squeeze                      0.0429153            0.0786552\n",
      "call_function  getitem_1                    0.041008             0.0751594\n",
      "call_function  sigmoid                      0.0398159            0.0729745\n",
      "call_function  add_1                        0.0371933            0.0681678\n",
      "placeholder    x                            0.0274181            0.0502519\n",
      "call_function  add_4                        0.026226             0.048067\n",
      "get_attr       linear_bias                  0.0259876            0.0476301\n",
      "get_attr       linear_offsets               0.0207424            0.0380167\n",
      "get_attr       offsets                      0.0150204            0.0275293\n",
      "output         output                       0.0128746            0.0235966\n"
     ]
    }
   ],
   "source": [
    "interp = utils.ProfilingInterpreter(modify)\n",
    "interp.run(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))\n",
    "print(interp.summary(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.1 ms ± 3.72 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():ori(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.6 ms ± 3.84 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of fm with config: dim: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node offsets target offsets offsets of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori , modify = workload_fm_reduce(32,80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.9 ms ± 2.22 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():ori(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.7 ms ± 2.04 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total true time 14.205694198608398 ms\n",
      "total time: 20.94292640686035 ms\n",
      "Op type        Op                     Average runtime (ms)    Pct total runtime\n",
      "-------------  -------------------  ----------------------  -------------------\n",
      "call_module    embedding_embedding               5.3072              25.3412\n",
      "call_function  pow_2                             4.10414             19.5968\n",
      "call_function  sum_3                             0.859499             4.10401\n",
      "call_function  add                               0.828028             3.95373\n",
      "call_function  sum_2                             0.745296             3.5587\n",
      "call_module    linear_fc                         0.671864             3.20807\n",
      "call_function  add_2                             0.581503             2.77661\n",
      "call_function  sum_1                             0.267506             1.27731\n",
      "call_function  pow_1                             0.18549              0.885691\n",
      "call_function  sub                               0.157833             0.753634\n",
      "call_function  sum_4                             0.0994205            0.474721\n",
      "call_function  mul                               0.0984669            0.470168\n",
      "call_method    squeeze                           0.0553131            0.264114\n",
      "call_function  add_1                             0.0510216            0.243622\n",
      "call_function  sigmoid                           0.0426769            0.203777\n",
      "call_function  add_3                             0.0333786            0.159379\n",
      "placeholder    x                                 0.0326633            0.155964\n",
      "get_attr       linear_bias                       0.0298023            0.142303\n",
      "get_attr       linear_offsets                    0.0228882            0.109288\n",
      "get_attr       embedding_offsets                 0.0185966            0.0887968\n",
      "output         output                            0.013113             0.0626131\n"
     ]
    }
   ],
   "source": [
    "interp = utils.ProfilingInterpreter(ori)\n",
    "interp.run(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))\n",
    "print(interp.summary(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total true time 14.345169067382812 ms\n",
      "total time: 20.27416229248047 ms\n",
      "Op type        Op                Average runtime (ms)    Pct total runtime\n",
      "-------------  --------------  ----------------------  -------------------\n",
      "call_module    embed                        5.16462             25.4739\n",
      "call_function  pow_2                        4.32205             21.318\n",
      "call_function  add                          0.828505             4.0865\n",
      "call_function  sum_2                        0.712395             3.51381\n",
      "call_module    linear_fc                    0.673771             3.3233\n",
      "call_method    add_2                        0.626326             3.08928\n",
      "call_function  add_3                        0.406504             2.00503\n",
      "call_function  sum_4                        0.264406             1.30415\n",
      "call_function  sub                          0.249386             1.23007\n",
      "call_function  sum_5                        0.169516             0.836116\n",
      "call_function  pow_1                        0.167131             0.824357\n",
      "call_function  getitem                      0.150919             0.744391\n",
      "call_function  sum_1                        0.123501             0.609154\n",
      "call_function  mul                          0.11158              0.550355\n",
      "call_function  sum_3                        0.0650883            0.321041\n",
      "call_method    squeeze                      0.0522137            0.257538\n",
      "call_function  sigmoid                      0.0445843            0.219907\n",
      "call_function  add_4                        0.0407696            0.201091\n",
      "call_function  add_1                        0.0369549            0.182276\n",
      "call_function  getitem_1                    0.030756             0.1517\n",
      "placeholder    x                            0.0295639            0.145821\n",
      "get_attr       linear_bias                  0.0240803            0.118773\n",
      "get_attr       linear_offsets               0.0209808            0.103486\n",
      "get_attr       offsets                      0.0154972            0.0764382\n",
      "output         output                       0.0140667            0.0693824\n"
     ]
    }
   ],
   "source": [
    "interp = utils.ProfilingInterpreter(modify)\n",
    "interp.run(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))\n",
    "print(interp.summary(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class FactorizationMachineModel(torch.nn.Module):\n",
      "    def forward(self, x):\n",
      "        # No stacktrace found for following nodes\n",
      "        linear_offsets = self.linear.offsets\n",
      "        add = x + linear_offsets;  linear_offsets = None\n",
      "        linear_fc = self.linear.fc(add);  add = None\n",
      "        sum_1 = torch.sum(linear_fc, dim = 1);  linear_fc = None\n",
      "        linear_bias = self.linear.bias\n",
      "        add_1 = sum_1 + linear_bias;  sum_1 = linear_bias = None\n",
      "        offsets = self.offsets\n",
      "        add_4 = offsets.add(x);  offsets = x = None\n",
      "        embed = self.embed(add_4);  add_4 = None\n",
      "        sum_5 = torch.sum(embed, dim = 1)\n",
      "        pow_3 = sum_5 ** 2;  sum_5 = None\n",
      "        pow_4 = embed ** 2;  embed = None\n",
      "        getitem = pow_4[(0, slice(None, 80, None))]\n",
      "        getitem_1 = pow_4[(slice(None, None, None), slice(80, None, None))];  pow_4 = None\n",
      "        sum_6 = torch.sum(getitem, dim = 0);  getitem = None\n",
      "        sum_7 = torch.sum(getitem_1, dim = 1);  getitem_1 = None\n",
      "        add_5 = sum_6 + sum_7;  sum_6 = sum_7 = None\n",
      "        sub_1 = pow_3 - add_5;  pow_3 = add_5 = None\n",
      "        sum_8 = torch.sum(sub_1, dim = 1, keepdim = True);  sub_1 = None\n",
      "        mul_1 = 0.5 * sum_8;  sum_8 = None\n",
      "        add_3 = add_1 + mul_1;  add_1 = mul_1 = None\n",
      "        squeeze = add_3.squeeze(1);  add_3 = None\n",
      "        sigmoid = torch.sigmoid(squeeze);  squeeze = None\n",
      "        return sigmoid\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'class FactorizationMachineModel(torch.nn.Module):\\n    def forward(self, x):\\n        # No stacktrace found for following nodes\\n        linear_offsets = self.linear.offsets\\n        add = x + linear_offsets;  linear_offsets = None\\n        linear_fc = self.linear.fc(add);  add = None\\n        sum_1 = torch.sum(linear_fc, dim = 1);  linear_fc = None\\n        linear_bias = self.linear.bias\\n        add_1 = sum_1 + linear_bias;  sum_1 = linear_bias = None\\n        offsets = self.offsets\\n        add_4 = offsets.add(x);  offsets = x = None\\n        embed = self.embed(add_4);  add_4 = None\\n        sum_5 = torch.sum(embed, dim = 1)\\n        pow_3 = sum_5 ** 2;  sum_5 = None\\n        pow_4 = embed ** 2;  embed = None\\n        getitem = pow_4[(0, slice(None, 80, None))]\\n        getitem_1 = pow_4[(slice(None, None, None), slice(80, None, None))];  pow_4 = None\\n        sum_6 = torch.sum(getitem, dim = 0);  getitem = None\\n        sum_7 = torch.sum(getitem_1, dim = 1);  getitem_1 = None\\n        add_5 = sum_6 + sum_7;  sum_6 = sum_7 = None\\n        sub_1 = pow_3 - add_5;  pow_3 = add_5 = None\\n        sum_8 = torch.sum(sub_1, dim = 1, keepdim = True);  sub_1 = None\\n        mul_1 = 0.5 * sum_8;  sum_8 = None\\n        add_3 = add_1 + mul_1;  add_1 = mul_1 = None\\n        squeeze = add_3.squeeze(1);  add_3 = None\\n        sigmoid = torch.sigmoid(squeeze);  squeeze = None\\n        return sigmoid\\n        '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modify.print_readable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name            target                                                      args                                                       kwargs\n",
      "-------------  --------------  ----------------------------------------------------------  ---------------------------------------------------------  ---------------------------\n",
      "placeholder    x               x                                                           ()                                                         {}\n",
      "get_attr       linear_offsets  linear.offsets                                              ()                                                         {}\n",
      "call_function  add             <built-in function add>                                     (x, linear_offsets)                                        {}\n",
      "call_module    linear_fc       linear.fc                                                   (add,)                                                     {}\n",
      "call_function  sum_1           <built-in method sum of type object at 0x7fa3116e2760>      (linear_fc,)                                               {'dim': 1}\n",
      "get_attr       linear_bias     linear.bias                                                 ()                                                         {}\n",
      "call_function  add_1           <built-in function add>                                     (sum_1, linear_bias)                                       {}\n",
      "get_attr       offsets         offsets                                                     ()                                                         {}\n",
      "call_method    add_4           add                                                         (offsets, x)                                               {}\n",
      "call_module    embed           embed                                                       (add_4,)                                                   {}\n",
      "call_function  sum_5           <built-in method sum of type object at 0x7fa3116e2760>      (embed,)                                                   {'dim': 1}\n",
      "call_function  pow_3           <built-in function pow>                                     (sum_5, 2)                                                 {}\n",
      "call_function  pow_4           <built-in function pow>                                     (embed, 2)                                                 {}\n",
      "call_function  getitem         <built-in function getitem>                                 (pow_4, (0, slice(None, 80, None)))                        {}\n",
      "call_function  getitem_1       <built-in function getitem>                                 (pow_4, (slice(None, None, None), slice(80, None, None)))  {}\n",
      "call_function  sum_6           <built-in method sum of type object at 0x7fa3116e2760>      (getitem,)                                                 {'dim': 0}\n",
      "call_function  sum_7           <built-in method sum of type object at 0x7fa3116e2760>      (getitem_1,)                                               {'dim': 1}\n",
      "call_function  add_5           <built-in function add>                                     (sum_6, sum_7)                                             {}\n",
      "call_function  sub_1           <built-in function sub>                                     (pow_3, add_5)                                             {}\n",
      "call_function  sum_8           <built-in method sum of type object at 0x7fa3116e2760>      (sub_1,)                                                   {'dim': 1, 'keepdim': True}\n",
      "call_function  mul_1           <built-in function mul>                                     (0.5, sum_8)                                               {}\n",
      "call_function  add_3           <built-in function add>                                     (add_1, mul_1)                                             {}\n",
      "call_method    squeeze         squeeze                                                     (add_3, 1)                                                 {}\n",
      "call_function  sigmoid         <built-in method sigmoid of type object at 0x7fa3116e2760>  (squeeze,)                                                 {}\n",
      "output         output          output                                                      (sigmoid,)                                                 {}\n"
     ]
    }
   ],
   "source": [
    "modify.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pattern_replace_and_matcher_for_FM_pow_reduce(traced,\n",
    "                                                  redency_part_slice,unredency_part_slice,\n",
    "                                                  target_node_name,batch = 4096,match_func = None\n",
    "                                                ):\n",
    "  from torch.fx.passes.utils.matcher_utils import SubgraphMatcher\n",
    "\n",
    "  class PatternClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.embed = torch.nn.Embedding(1, 1)\n",
    "      self.offsets = torch.as_tensor(np.array((0, *np.cumsum([10,10])[:-1]), dtype=np.int64))\n",
    "\n",
    "    def forward(self,x):\n",
    "      x = x + self.offsets\n",
    "      x = self.embed(x)\n",
    "      square_of_sum = torch.sum(x, dim=1) ** 2\n",
    "      sum_of_square = torch.sum(x ** 2, dim=1)\n",
    "      ix = square_of_sum - sum_of_square    \n",
    "      ix = torch.sum(ix, dim=1, keepdim=True)  \n",
    "      return 0.5 * ix\n",
    "  def _match(match,ori,pat):\n",
    "    return True \n",
    "  # env  = utils.get_env(traced)\n",
    "  # target_node = env[target_node_name]\n",
    "  # target_node_mod = utils.get_target_mod(traced,target_node_name,\"_\")\n",
    "  pattern = PatternClass()  \n",
    "  pattern_trace = symbolic_trace(pattern)\n",
    "  pattern_graph = pattern_trace.graph\n",
    "  original_graph = traced.graph\n",
    "  matcher =  SubgraphMatcher(pattern_graph, match_output=False, match_placeholder=False,\n",
    "                              remove_overlapping_matches=True)\n",
    "  _matches = matcher.match(original_graph)\n",
    "  match_filters = [_match if match_func is None else match_func]\n",
    "  _matches = [\n",
    "      m for m in _matches\n",
    "      if all(match_filter(m, original_graph, pattern_graph)\n",
    "              for match_filter in match_filters)\n",
    "  ]  \n",
    "  # 因为在过滤器中做了限制应该只有一个符合要求的\n",
    "  _matched = _matches[0]\n",
    "  pattern_env = utils.get_env(pattern_trace)\n",
    "  node_map = _matched.nodes_map\n",
    "  pn = pattern_env['offsets']\n",
    "  offsets_node = node_map[pn]\n",
    "  offsets_val = utils.get_target_mod(traced,offsets_node.target)\n",
    "  \n",
    "  embed_node = node_map[pattern_env['embed']]\n",
    "  embed_node_module = utils.get_target_mod(traced,embed_node.target)\n",
    "  \n",
    "\n",
    "  \n",
    "  \n",
    "  class ReplacementClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.offsets = offsets_val\n",
    "      self.redency_offset = torch.as_tensor(np.array(offsets_val[redency_part_slice[1]], dtype=np.int64))\n",
    "      self.unredency_offset = torch.as_tensor(np.array(offsets_val[unredency_part_slice[1]], dtype=np.int64))\n",
    "      embedding_config = embed_node_module.weight.data.shape\n",
    "      self.embed = nn.Embedding(embedding_config[0],embedding_config[1])\n",
    "      self.embed.weight.data.copy_(embed_node_module.weight.data)\n",
    "    def forward(self,x):\n",
    "      x = self.offsets  + x\n",
    "      x = self.embed(x)\n",
    "      square_of_sum = torch.sum(x, dim=1) ** 2\n",
    "      \n",
    "      \n",
    "      \n",
    "      redency_part_embed_pow = x[redency_part_slice]  ** 2\n",
    "      unredency_part_embed_pow = x[unredency_part_slice] ** 2\n",
    "      redency_embed_square_sum = torch.sum(redency_part_embed_pow ,dim=0)\n",
    "      unredency_embed_square_sum = torch.sum(unredency_part_embed_pow ,dim=1)\n",
    "      sum_of_square = redency_embed_square_sum + unredency_embed_square_sum\n",
    "      ix = square_of_sum - sum_of_square\n",
    "      ix = torch.sum(ix,dim = 1,keepdim=True)\n",
    "      return 0.5 * ix\n",
    "    \n",
    "  \n",
    "  return pattern,ReplacementClass(),_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def workload_fm_reduce_pow(dim,redundancy):\n",
    "  print(f\"now gen workload of fm with config: dim: {dim}\")\n",
    "  fm_model_ori = fm.FactorizationMachineModel([100 for i in range(100)],dim)\n",
    "  fm_model_traced_ori = symbolic_trace(fm_model_ori)\n",
    "  \n",
    "  fm_model_modify = fm.FactorizationMachineModel([100 for i in range(100)],dim)\n",
    "  fm_model_traced_modify = symbolic_trace(fm_model_modify)\n",
    "  pattern,replace,match = gen_pattern_replace_and_matcher_for_FM_pow_reduce(fm_model_traced_modify,\n",
    "                                                                      (0,slice(None,redundancy,None)),(slice(None,None,None),slice(redundancy,None,None)),\n",
    "                                                                      \"linear_offsets\")\n",
    "  matches = subgraph_rewriter.replace_pattern_with_filters(fm_model_traced_modify, pattern, replace,[match])\n",
    "  return fm_model_traced_ori,fm_model_traced_modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of fm with config: dim: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node offsets target offsets offsets of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori , modify = workload_fm_reduce_pow(128,80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.8 ms ± 4.28 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():ori(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.3 ms ± 2.51 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pattern_replace_and_matcher_for_FM_embed(traced,\n",
    "                                                  redency_part_slice,unredency_part_slice,\n",
    "                                                  target_node_name,batch = 4096,match_func = None\n",
    "                                                ):\n",
    "  from torch.fx.passes.utils.matcher_utils import SubgraphMatcher\n",
    "\n",
    "  class PatternClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.embed = torch.nn.Embedding(1, 1)\n",
    "      self.offsets = torch.as_tensor(np.array((0, *np.cumsum([10,10])[:-1]), dtype=np.int64))\n",
    "\n",
    "    def forward(self,x):\n",
    "      x = x + self.offsets\n",
    "      x = self.embed(x)\n",
    "      square_of_sum = torch.sum(x, dim=1) ** 2\n",
    "      sum_of_square = torch.sum(x ** 2, dim=1)\n",
    "      ix = square_of_sum - sum_of_square    \n",
    "      ix = torch.sum(ix, dim=1, keepdim=True)  \n",
    "      return 0.5 * ix\n",
    "  def _match(match,ori,pat):\n",
    "    return True \n",
    "  # env  = utils.get_env(traced)\n",
    "  # target_node = env[target_node_name]\n",
    "  # target_node_mod = utils.get_target_mod(traced,target_node_name,\"_\")\n",
    "  pattern = PatternClass()  \n",
    "  pattern_trace = symbolic_trace(pattern)\n",
    "  pattern_graph = pattern_trace.graph\n",
    "  original_graph = traced.graph\n",
    "  matcher =  SubgraphMatcher(pattern_graph, match_output=False, match_placeholder=False,\n",
    "                              remove_overlapping_matches=True)\n",
    "  _matches = matcher.match(original_graph)\n",
    "  match_filters = [_match if match_func is None else match_func]\n",
    "  _matches = [\n",
    "      m for m in _matches\n",
    "      if all(match_filter(m, original_graph, pattern_graph)\n",
    "              for match_filter in match_filters)\n",
    "  ]  \n",
    "  # 因为在过滤器中做了限制应该只有一个符合要求的\n",
    "  _matched = _matches[0]\n",
    "  pattern_env = utils.get_env(pattern_trace)\n",
    "  node_map = _matched.nodes_map\n",
    "  pn = pattern_env['offsets']\n",
    "  offsets_node = node_map[pn]\n",
    "  offsets_val = utils.get_target_mod(traced,offsets_node.target)\n",
    "  \n",
    "  embed_node = node_map[pattern_env['embed']]\n",
    "  embed_node_module = utils.get_target_mod(traced,embed_node.target)\n",
    "  \n",
    "\n",
    "  \n",
    "  \n",
    "  class ReplacementClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.redency_offset = torch.as_tensor(np.array(offsets_val[redency_part_slice[1]], dtype=np.int64))\n",
    "      self.unredency_offset = torch.as_tensor(np.array(offsets_val[unredency_part_slice[1]], dtype=np.int64))\n",
    "      embedding_config = embed_node_module.weight.data.shape\n",
    "      self.embed = nn.Embedding(embedding_config[0],embedding_config[1])\n",
    "      self.embed.weight.data.copy_(embed_node_module.weight.data)\n",
    "    def forward(self,x):\n",
    "      redency_part = x[redency_part_slice] + self.redency_offset\n",
    "      unredency_part = x[unredency_part_slice] + self.unredency_offset\n",
    "      redency_embed = self.embed(redency_part)\n",
    "      unredency_embed = self.embed(unredency_part)\n",
    "      redency_embed = redency_embed.repeat(batch,1,1)\n",
    "      embed = torch.concat([redency_embed, unredency_embed],dim = 1)\n",
    "      square_of_sum = torch.sum(embed, dim=1) ** 2\n",
    "      sum_of_square = torch.sum(embed ** 2,dim = 1)\n",
    "      ix = square_of_sum - sum_of_square\n",
    "      ix = torch.sum(ix,dim = 1,keepdim=True)\n",
    "      return 0.5 * ix\n",
    "    \n",
    "  \n",
    "  return pattern,ReplacementClass(),_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def workload_fm_reduce_embed(dim,redundancy):\n",
    "  print(f\"now gen workload of fm with config: dim: {dim}\")\n",
    "  fm_model_ori = fm.FactorizationMachineModel([100 for i in range(100)],dim)\n",
    "  fm_model_traced_ori = symbolic_trace(fm_model_ori)\n",
    "  \n",
    "  fm_model_modify = fm.FactorizationMachineModel([100 for i in range(100)],dim)\n",
    "  fm_model_traced_modify = symbolic_trace(fm_model_modify)\n",
    "  pattern,replace,match = gen_pattern_replace_and_matcher_for_FM_embed(fm_model_traced_modify,\n",
    "                                                                      (0,slice(None,redundancy,None)),(slice(None,None,None),slice(redundancy,None,None)),\n",
    "                                                                      \"linear_offsets\")\n",
    "  matches = subgraph_rewriter.replace_pattern_with_filters(fm_model_traced_modify, pattern, replace,[match])\n",
    "  return fm_model_traced_ori,fm_model_traced_modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of fm with config: dim: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori , modify = workload_fm_reduce_embed(128,80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.1 ms ± 3.59 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():ori(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.4 ms ± 4.87 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pattern_replace_and_matcher_for_FM_reduce_stack(traced,\n",
    "                                                  redency_part_slice,unredency_part_slice,\n",
    "                                                  target_node_name,batch = 4096,match_func = None\n",
    "                                                ):\n",
    "  from torch.fx.passes.utils.matcher_utils import SubgraphMatcher\n",
    "\n",
    "  class PatternClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.embed = torch.nn.Embedding(1, 1)\n",
    "      self.offsets = torch.as_tensor(np.array((0, *np.cumsum([10,10])[:-1]), dtype=np.int64))\n",
    "\n",
    "    def forward(self,x):\n",
    "      x = x + self.offsets\n",
    "      x = self.embed(x)\n",
    "      square_of_sum = torch.sum(x, dim=1) ** 2\n",
    "      sum_of_square = torch.sum(x ** 2, dim=1)\n",
    "      ix = square_of_sum - sum_of_square    \n",
    "      ix = torch.sum(ix, dim=1, keepdim=True)  \n",
    "      return 0.5 * ix\n",
    "  def _match(match,ori,pat):\n",
    "    return True \n",
    "  # env  = utils.get_env(traced)\n",
    "  # target_node = env[target_node_name]\n",
    "  # target_node_mod = utils.get_target_mod(traced,target_node_name,\"_\")\n",
    "  pattern = PatternClass()  \n",
    "  pattern_trace = symbolic_trace(pattern)\n",
    "  pattern_graph = pattern_trace.graph\n",
    "  original_graph = traced.graph\n",
    "  matcher =  SubgraphMatcher(pattern_graph, match_output=False, match_placeholder=False,\n",
    "                              remove_overlapping_matches=True)\n",
    "  _matches = matcher.match(original_graph)\n",
    "  match_filters = [_match if match_func is None else match_func]\n",
    "  _matches = [\n",
    "      m for m in _matches\n",
    "      if all(match_filter(m, original_graph, pattern_graph)\n",
    "              for match_filter in match_filters)\n",
    "  ]  \n",
    "  # 因为在过滤器中做了限制应该只有一个符合要求的\n",
    "  _matched = _matches[0]\n",
    "  pattern_env = utils.get_env(pattern_trace)\n",
    "  node_map = _matched.nodes_map\n",
    "  pn = pattern_env['offsets']\n",
    "  offsets_node = node_map[pn]\n",
    "  offsets_val = utils.get_target_mod(traced,offsets_node.target)\n",
    "  \n",
    "  embed_node = node_map[pattern_env['embed']]\n",
    "  embed_node_module = utils.get_target_mod(traced,embed_node.target)\n",
    "  \n",
    "\n",
    "  \n",
    "  \n",
    "  class ReplacementClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.offsets = offsets_val\n",
    "      self.redency_offset = torch.as_tensor(np.array(offsets_val[redency_part_slice[1]], dtype=np.int64))\n",
    "      self.unredency_offset = torch.as_tensor(np.array(offsets_val[unredency_part_slice[1]], dtype=np.int64))\n",
    "      embedding_config = embed_node_module.weight.data.shape\n",
    "      self.embed = nn.Embedding(embedding_config[0],embedding_config[1])\n",
    "      self.embed.weight.data.copy_(embed_node_module.weight.data)\n",
    "    def forward(self,x):\n",
    "      x = self.offsets  + x\n",
    "      x = self.embed(x)\n",
    "      square_of_sum = torch.sum(x, dim=1) ** 2\n",
    "      pow_x = x ** 2\n",
    "      \n",
    "      \n",
    "      redency_part_embed_pow = pow_x[redency_part_slice] \n",
    "      unredency_part_embed_pow = pow_x[unredency_part_slice] \n",
    "      redency_embed_square_sum = torch.sum(redency_part_embed_pow ,dim=0)\n",
    "      unredency_embed_square_sum = torch.sum(unredency_part_embed_pow ,dim=1)\n",
    "      redency_embed_square_sum = redency_embed_square_sum.repeat(batch,1)\n",
    "      sum_of_square = torch.stack([redency_embed_square_sum,unredency_embed_square_sum], dim = 1)\n",
    "      sum_of_square = torch.sum(sum_of_square, dim = 1)\n",
    "      # sum_of_square = torch.sum(embed, dim = 1)\n",
    "      ix = square_of_sum - sum_of_square\n",
    "      ix = torch.sum(ix,dim = 1,keepdim=True)\n",
    "      return 0.5 * ix\n",
    "    \n",
    "  \n",
    "  return pattern,ReplacementClass(),_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def workload_fm_reduce_stack(dim,redundancy):\n",
    "  print(f\"now gen workload of fm with config: dim: {dim}\")\n",
    "  fm_model_ori = fm.FactorizationMachineModel([100 for i in range(100)],dim)\n",
    "  fm_model_traced_ori = symbolic_trace(fm_model_ori)\n",
    "  \n",
    "  fm_model_modify = fm.FactorizationMachineModel([100 for i in range(100)],dim)\n",
    "  fm_model_traced_modify = symbolic_trace(fm_model_modify)\n",
    "  pattern,replace,match = gen_pattern_replace_and_matcher_for_FM_reduce_stack(fm_model_traced_modify,\n",
    "                                                                      (0,slice(None,redundancy,None)),(slice(None,None,None),slice(redundancy,None,None)),\n",
    "                                                                      \"linear_offsets\")\n",
    "  matches = subgraph_rewriter.replace_pattern_with_filters(fm_model_traced_modify, pattern, replace,[match])\n",
    "  return fm_model_traced_ori,fm_model_traced_modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of fm with config: dim: 128\n"
     ]
    }
   ],
   "source": [
    "ori , modify = workload_fm_reduce(128,80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.3 ms ± 3.61 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():ori(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 ms ± 567 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorizationMachineModel(\n",
       "  (linear): Module(\n",
       "    (fc): Embedding(10000, 1)\n",
       "  )\n",
       "  (embedding): Module(\n",
       "    (embedding): Embedding(10000, 128)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214 µs ± 64 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long).to(device)\n",
    "%timeit -n 1 -r 30 with torch.no_grad():ori(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorizationMachineModel(\n",
       "  (linear): Module(\n",
       "    (fc): Embedding(10000, 1)\n",
       "  )\n",
       "  (embed): Embedding(10000, 128)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modify.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297 µs ± 45.3 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long).to(device)\n",
    "%timeit -n 1 -r 30 with torch.no_grad():modify(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of fm with config: dim: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node offsets target offsets offsets of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori , modify = workload_fm_reduce_stack(128,80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.1 ms ± 3.84 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():ori(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.6 ms ± 3.66 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pattern_replace_and_matcher_for_FM_reduce_add(traced,\n",
    "                                                  redency_part_slice,unredency_part_slice,\n",
    "                                                  target_node_name,batch = 4096,match_func = None\n",
    "                                                ):\n",
    "  from torch.fx.passes.utils.matcher_utils import SubgraphMatcher\n",
    "\n",
    "  class PatternClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.embed = torch.nn.Embedding(1, 1)\n",
    "      # self.offsets = torch.as_tensor(np.array((0, *np.cumsum([10,10])[:-1]), dtype=np.int64))\n",
    "      self.offsets = nn.Parameter(torch.as_tensor(np.array((0, *np.cumsum([10,10])[:-1]), dtype=np.int64)),False)\n",
    "\n",
    "    def forward(self,x):\n",
    "      x = x + self.offsets\n",
    "      x = self.embed(x)\n",
    "      square_of_sum = torch.sum(x, dim=1) ** 2\n",
    "      sum_of_square = torch.sum(x ** 2, dim=1)\n",
    "      ix = square_of_sum - sum_of_square    \n",
    "      ix = torch.sum(ix, dim=1, keepdim=True)  \n",
    "      return 0.5 * ix\n",
    "  def _match(match,ori,pat):\n",
    "    return True \n",
    "  # env  = utils.get_env(traced)\n",
    "  # target_node = env[target_node_name]\n",
    "  # target_node_mod = utils.get_target_mod(traced,target_node_name,\"_\")\n",
    "  pattern = PatternClass()  \n",
    "  pattern_trace = symbolic_trace(pattern)\n",
    "  pattern_graph = pattern_trace.graph\n",
    "  original_graph = traced.graph\n",
    "  matcher =  SubgraphMatcher(pattern_graph, match_output=False, match_placeholder=False,\n",
    "                              remove_overlapping_matches=True)\n",
    "  _matches = matcher.match(original_graph)\n",
    "  match_filters = [_match if match_func is None else match_func]\n",
    "  _matches = [\n",
    "      m for m in _matches\n",
    "      if all(match_filter(m, original_graph, pattern_graph)\n",
    "              for match_filter in match_filters)\n",
    "  ]  \n",
    "  # 因为在过滤器中做了限制应该只有一个符合要求的\n",
    "  _matched = _matches[0]\n",
    "  pattern_env = utils.get_env(pattern_trace)\n",
    "  node_map = _matched.nodes_map\n",
    "  pn = pattern_env['offsets']\n",
    "  offsets_node = node_map[pn]\n",
    "  offsets_val = utils.get_target_mod(traced,offsets_node.target)\n",
    "  \n",
    "  embed_node = node_map[pattern_env['embed']]\n",
    "  embed_node_module = utils.get_target_mod(traced,embed_node.target)\n",
    "  \n",
    "\n",
    "  \n",
    "  \n",
    "  class ReplacementClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.offsets = offsets_val\n",
    "      self.redency_offset = torch.as_tensor(np.array(offsets_val[redency_part_slice[1]], dtype=np.int64))\n",
    "      self.unredency_offset = torch.as_tensor(np.array(offsets_val[unredency_part_slice[1]], dtype=np.int64))\n",
    "      embedding_config = embed_node_module.weight.data.shape\n",
    "      self.embed = nn.Embedding(embedding_config[0],embedding_config[1])\n",
    "      self.embed.weight.data.copy_(embed_node_module.weight.data)\n",
    "    def forward(self,x):\n",
    "      x = self.offsets  + x\n",
    "      x = self.embed(x)\n",
    "      square_of_sum = torch.sum(x, dim=1) ** 2\n",
    "      pow_x = x ** 2\n",
    "      \n",
    "      \n",
    "      redency_part_embed_pow = pow_x[redency_part_slice] \n",
    "      unredency_part_embed_pow = pow_x[unredency_part_slice] \n",
    "      redency_embed_square_sum = torch.sum(redency_part_embed_pow ,dim=0)\n",
    "      unredency_embed_square_sum = torch.sum(unredency_part_embed_pow ,dim=1)\n",
    "      redency_embed_square_sum = redency_embed_square_sum.repeat(batch,1)\n",
    "      sum_of_square = redency_embed_square_sum + unredency_embed_square_sum\n",
    "      # sum_of_square = torch.sum(embed, dim = 1)\n",
    "      ix = square_of_sum - sum_of_square\n",
    "      ix = torch.sum(ix,dim = 1,keepdim=True)\n",
    "      return 0.5 * ix\n",
    "    \n",
    "  \n",
    "  return pattern,ReplacementClass(),_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_model_ori = fm.FactorizationMachineModel([100 for i in range(100)],32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_model_traced_ori = symbolic_trace(fm_model_ori)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class FactorizationMachineModel(torch.nn.Module):\n",
      "    def forward(self, x):\n",
      "        # No stacktrace found for following nodes\n",
      "        linear_offsets = self.linear.offsets\n",
      "        add = x + linear_offsets;  linear_offsets = None\n",
      "        linear_fc = self.linear.fc(add);  add = None\n",
      "        sum_1 = torch.sum(linear_fc, dim = 1);  linear_fc = None\n",
      "        linear_bias = self.linear.bias\n",
      "        add_1 = sum_1 + linear_bias;  sum_1 = linear_bias = None\n",
      "        embedding_offsets = self.embedding.offsets\n",
      "        add_2 = x + embedding_offsets;  x = embedding_offsets = None\n",
      "        embedding_embedding = self.embedding.embedding(add_2);  add_2 = None\n",
      "        sum_2 = torch.sum(embedding_embedding, dim = 1)\n",
      "        pow_1 = sum_2 ** 2;  sum_2 = None\n",
      "        pow_2 = embedding_embedding ** 2;  embedding_embedding = None\n",
      "        sum_3 = torch.sum(pow_2, dim = 1);  pow_2 = None\n",
      "        sub = pow_1 - sum_3;  pow_1 = sum_3 = None\n",
      "        sum_4 = torch.sum(sub, dim = 1, keepdim = True);  sub = None\n",
      "        mul = 0.5 * sum_4;  sum_4 = None\n",
      "        add_3 = add_1 + mul;  add_1 = mul = None\n",
      "        squeeze = add_3.squeeze(1);  add_3 = None\n",
      "        sigmoid = torch.sigmoid(squeeze);  squeeze = None\n",
      "        return sigmoid\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'class FactorizationMachineModel(torch.nn.Module):\\n    def forward(self, x):\\n        # No stacktrace found for following nodes\\n        linear_offsets = self.linear.offsets\\n        add = x + linear_offsets;  linear_offsets = None\\n        linear_fc = self.linear.fc(add);  add = None\\n        sum_1 = torch.sum(linear_fc, dim = 1);  linear_fc = None\\n        linear_bias = self.linear.bias\\n        add_1 = sum_1 + linear_bias;  sum_1 = linear_bias = None\\n        embedding_offsets = self.embedding.offsets\\n        add_2 = x + embedding_offsets;  x = embedding_offsets = None\\n        embedding_embedding = self.embedding.embedding(add_2);  add_2 = None\\n        sum_2 = torch.sum(embedding_embedding, dim = 1)\\n        pow_1 = sum_2 ** 2;  sum_2 = None\\n        pow_2 = embedding_embedding ** 2;  embedding_embedding = None\\n        sum_3 = torch.sum(pow_2, dim = 1);  pow_2 = None\\n        sub = pow_1 - sum_3;  pow_1 = sum_3 = None\\n        sum_4 = torch.sum(sub, dim = 1, keepdim = True);  sub = None\\n        mul = 0.5 * sum_4;  sum_4 = None\\n        add_3 = add_1 + mul;  add_1 = mul = None\\n        squeeze = add_3.squeeze(1);  add_3 = None\\n        sigmoid = torch.sigmoid(squeeze);  squeeze = None\\n        return sigmoid\\n        '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_model_traced_ori.print_readable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def workload_fm_reduce_add(dim,redundancy):\n",
    "  print(f\"now gen workload of fm with config: dim: {dim}\")\n",
    "  fm_model_ori = fm.FactorizationMachineModel([100 for i in range(100)],dim)\n",
    "  fm_model_traced_ori = symbolic_trace(fm_model_ori)\n",
    "  \n",
    "  fm_model_modify = fm.FactorizationMachineModel([100 for i in range(100)],dim)\n",
    "  fm_model_traced_modify = symbolic_trace(fm_model_modify)\n",
    "  pattern,replace,match = gen_pattern_replace_and_matcher_for_FM_reduce_add(fm_model_traced_modify,\n",
    "                                                                      (0,slice(None,redundancy,None)),(slice(None,None,None),slice(redundancy,None,None)),\n",
    "                                                                      \"linear_offsets\")\n",
    "  matches = subgraph_rewriter.replace_pattern_with_filters(fm_model_traced_modify, pattern, replace,[match])\n",
    "  return fm_model_traced_ori,fm_model_traced_modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name                 target                                                      args                      kwargs\n",
      "-------------  -------------------  ----------------------------------------------------------  ------------------------  ---------------------------\n",
      "placeholder    x                    x                                                           ()                        {}\n",
      "get_attr       linear_offsets       linear.offsets                                              ()                        {}\n",
      "call_function  add                  <built-in function add>                                     (x, linear_offsets)       {}\n",
      "call_module    linear_fc            linear.fc                                                   (add,)                    {}\n",
      "call_function  sum_1                <built-in method sum of type object at 0x7f17d3680760>      (linear_fc,)              {'dim': 1}\n",
      "get_attr       linear_bias          linear.bias                                                 ()                        {}\n",
      "call_function  add_1                <built-in function add>                                     (sum_1, linear_bias)      {}\n",
      "get_attr       embedding_offsets    embedding.offsets                                           ()                        {}\n",
      "call_function  add_2                <built-in function add>                                     (x, embedding_offsets)    {}\n",
      "call_module    embedding_embedding  embedding.embedding                                         (add_2,)                  {}\n",
      "call_function  sum_2                <built-in method sum of type object at 0x7f17d3680760>      (embedding_embedding,)    {'dim': 1}\n",
      "call_function  pow_1                <built-in function pow>                                     (sum_2, 2)                {}\n",
      "call_function  pow_2                <built-in function pow>                                     (embedding_embedding, 2)  {}\n",
      "call_function  sum_3                <built-in method sum of type object at 0x7f17d3680760>      (pow_2,)                  {'dim': 1}\n",
      "call_function  sub                  <built-in function sub>                                     (pow_1, sum_3)            {}\n",
      "call_function  sum_4                <built-in method sum of type object at 0x7f17d3680760>      (sub,)                    {'dim': 1, 'keepdim': True}\n",
      "call_function  mul                  <built-in function mul>                                     (0.5, sum_4)              {}\n",
      "call_function  add_3                <built-in function add>                                     (add_1, mul)              {}\n",
      "call_method    squeeze              squeeze                                                     (add_3, 1)                {}\n",
      "call_function  sigmoid              <built-in method sigmoid of type object at 0x7f17d3680760>  (squeeze,)                {}\n",
      "output         output               output                                                      (sigmoid,)                {}\n"
     ]
    }
   ],
   "source": [
    "fm_model_traced_ori.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of fm with config: dim: 128\n"
     ]
    }
   ],
   "source": [
    "ori , modify = workload_fm_reduce_add(128,80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.2 ms ± 4.03 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():ori(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.5 ms ± 3.31 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorizationMachineModel(\n",
       "  (linear): Module(\n",
       "    (fc): Embedding(10000, 1)\n",
       "  )\n",
       "  (embedding): Module(\n",
       "    (embedding): Embedding(10000, 128)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251 µs ± 71.8 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long).to(device)\n",
    "%timeit -n 1 -r 30 with torch.no_grad():ori(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorizationMachineModel(\n",
       "  (linear): Module(\n",
       "    (fc): Embedding(10000, 1)\n",
       "  )\n",
       "  (embed): Embedding(10000, 128)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modify.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340 µs ± 96.9 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long).to(device)\n",
    "%timeit -n 1 -r 30 with torch.no_grad():modify(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of fm with config: dim: 128\n"
     ]
    }
   ],
   "source": [
    "ori , modify = workload_fm(128,80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorizationMachineModel(\n",
       "  (linear): Module(\n",
       "    (fc): Embedding(10000, 1)\n",
       "  )\n",
       "  (embed): Embedding(10000, 128)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modify.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorizationMachineModel(\n",
       "  (linear): Module(\n",
       "    (fc): Embedding(10000, 1)\n",
       "  )\n",
       "  (embedding): Module(\n",
       "    (embedding): Embedding(10000, 128)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245 µs ± 71 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long).to(device)\n",
    "%timeit -n 1 -r 30 with torch.no_grad():ori(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356 µs ± 75 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long).to(device)\n",
    "%timeit -n 1 -r 30 with torch.no_grad():modify(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of fm with config: dim: 128\n"
     ]
    }
   ],
   "source": [
    "ori , modify = workload_fm(128,80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.3 ms ± 4.26 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():ori(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.1 ms ± 1.98 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorizationMachineModel(\n",
       "  (linear): Module(\n",
       "    (fc): Embedding(10000, 1)\n",
       "  )\n",
       "  (embedding): Module(\n",
       "    (embedding): Embedding(10000, 128)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modify.to(device)\n",
    "ori.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254 µs ± 41.4 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long).to(device)\n",
    "%timeit -n 1 -r 30 with torch.no_grad():ori(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366 µs ± 63.6 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long).to(device)\n",
    "%timeit -n 1 -r 30 with torch.no_grad():modify(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total true time 0.8633136749267578 ms\n",
      "total time: 1.0313987731933594 ms\n",
      "Op type        Op                     Average runtime (ms)    Pct total runtime\n",
      "-------------  -------------------  ----------------------  -------------------\n",
      "call_module    linear_fc                        0.112295              10.8877\n",
      "call_function  add                              0.0777245              7.53583\n",
      "call_module    embedding_embedding              0.067234               6.51872\n",
      "call_function  sum_1                            0.0574589              5.57097\n",
      "call_function  pow_1                            0.0493526              4.78502\n",
      "call_function  mul                              0.0472069              4.57698\n",
      "call_function  sigmoid                          0.0460148              4.4614\n",
      "call_function  sum_2                            0.0445843              4.3227\n",
      "call_function  add_1                            0.0400543              3.8835\n",
      "call_function  sub                              0.0374317              3.62922\n",
      "call_function  sum_3                            0.0360012              3.49052\n",
      "call_function  sum_4                            0.0350475              3.39806\n",
      "call_function  pow_2                            0.0312328              3.0282\n",
      "call_function  add_3                            0.0309944              3.00509\n",
      "call_method    squeeze                          0.0300407              2.91262\n",
      "call_function  add_2                            0.0288486              2.79704\n",
      "get_attr       linear_offsets                   0.023365               2.26537\n",
      "placeholder    x                                0.0219345              2.12668\n",
      "get_attr       linear_bias                      0.0202656              1.96486\n",
      "get_attr       embedding_offsets                0.0181198              1.75682\n",
      "output         output                           0.00810623             0.785945\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long).to(device)\n",
    "\n",
    "interp = utils.ProfilingInterpreter(ori)\n",
    "interp.run(x)\n",
    "print(interp.summary(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total true time 0.9818077087402344 ms\n",
      "total time: 1.1548995971679688 ms\n",
      "Op type        Op                  Average runtime (ms)    Pct total runtime\n",
      "-------------  ----------------  ----------------------  -------------------\n",
      "call_module    linear_fc                     0.102043               8.83567\n",
      "call_function  add                           0.0693798              6.00743\n",
      "call_module    embed                         0.0555515              4.81007\n",
      "call_function  mul                           0.0510216              4.41784\n",
      "call_function  sum_1                         0.0493526              4.27333\n",
      "call_function  getitem                       0.0448227              3.88109\n",
      "call_function  pow_1                         0.0412464              3.57143\n",
      "call_module    embed_1                       0.0398159              3.44756\n",
      "call_function  sum_2                         0.0352859              3.05533\n",
      "call_function  add_1                         0.0312328              2.70438\n",
      "call_function  sigmoid                       0.0305176              2.64244\n",
      "call_function  sub                           0.0290871              2.51858\n",
      "call_function  sum_4                         0.0267029              2.31214\n",
      "call_function  sum_3                         0.026226               2.27085\n",
      "call_function  sum_5                         0.026226               2.27085\n",
      "call_function  sum_6                         0.026226               2.27085\n",
      "call_function  add_4                         0.0259876              2.25021\n",
      "call_function  add_6                         0.0252724              2.18827\n",
      "call_method    squeeze                       0.0247955              2.14699\n",
      "call_function  add_2                         0.0240803              2.08505\n",
      "call_function  pow_3                         0.0240803              2.08505\n",
      "call_function  add_5                         0.0238419              2.06441\n",
      "call_function  pow_2                         0.0228882              1.98183\n",
      "call_function  add_3                         0.0226498              1.96119\n",
      "call_function  getitem_1                     0.0214577              1.85797\n",
      "placeholder    x                             0.0190735              1.65153\n",
      "get_attr       linear_offsets                0.0181198              1.56895\n",
      "get_attr       linear_bias                   0.014782               1.27993\n",
      "get_attr       redency_offset                0.0109673              0.949628\n",
      "get_attr       unredency_offset              0.010252               0.887696\n",
      "output         output                        0.00882149             0.763832\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long).to(device)\n",
    "\n",
    "interp = utils.ProfilingInterpreter(modify)\n",
    "interp.run(x)\n",
    "print(interp.summary(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduceOp(x):\n",
    "  return torch.sum(x, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class reduceRewriteOp(nn.Module):\n",
    "    \"\"\"\n",
    "    A pytorch implementation of Factorization Machine.\n",
    "\n",
    "    Reference:\n",
    "        S Rendle, Factorization Machines, 2010.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,redundancy_slice,non_redundancy_slice):\n",
    "        super().__init__()\n",
    "        self.redundancy_slice = redundancy_slice\n",
    "        self.non_redundancy_slice = non_redundancy_slice\n",
    "\n",
    "    def forward(self, x):\n",
    "      redundancy_emb = x[self.redundancy_slice]\n",
    "      non_redundancy_emb = x[self.non_redundancy_slice]\n",
    "      return torch.sum(redundancy_emb, dim = -2) + torch.sum(non_redundancy_emb, dim = -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori = symbolic_trace(reduceOp)\n",
    "modify = symbolic_trace(reduceRewriteOp((0,slice(None,80,None)),(slice(None,None,None),slice(80,None,None))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.03 ms ± 293 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn( size=(4096,100,128))\n",
    "%timeit -n 1 -r 30 with torch.no_grad():ori(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reduceOp()"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 10.49 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "23.8 µs ± 28.6 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn( size=(4096,100,128)).to(device)\n",
    "%timeit -n 1 -r 30 with torch.no_grad():ori(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total true time 5.20014762878418 ms\n",
      "total time: 5.33294677734375 ms\n",
      "Op type        Op        Average runtime (ms)    Pct total runtime\n",
      "-------------  ------  ----------------------  -------------------\n",
      "call_function  sum_1                5.12409              96.0837\n",
      "output         output               0.04673               0.876252\n",
      "placeholder    x                    0.0293255             0.549893\n"
     ]
    }
   ],
   "source": [
    "interp = utils.ProfilingInterpreter(ori)\n",
    "interp.run(x)\n",
    "print(interp.summary(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reduceRewriteOp()"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modify.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 6.21 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "55.1 µs ± 42.2 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn( size=(4096,100,128)).to(device)\n",
    "%timeit -n 1 -r 30 with torch.no_grad():modify(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 5.44 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "381 µs ± 269 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn( size=(4096,100,128))\n",
    "%timeit -n 1 -r 30 with torch.no_grad():modify(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total true time 1.5444755554199219 ms\n",
      "total time: 1.789093017578125 ms\n",
      "Op type        Op           Average runtime (ms)    Pct total runtime\n",
      "-------------  ---------  ----------------------  -------------------\n",
      "call_function  sum_2                   0.804663              44.976\n",
      "call_function  add                     0.380993              21.2953\n",
      "call_function  getitem                 0.128031               7.15618\n",
      "call_function  sum_1                   0.0972748              5.4371\n",
      "call_function  getitem_1               0.0579357              3.23827\n",
      "placeholder    x                       0.0431538              2.41205\n",
      "output         output                  0.0324249              1.81237\n"
     ]
    }
   ],
   "source": [
    "interp = utils.ProfilingInterpreter(modify)\n",
    "interp.run(x)\n",
    "print(interp.summary(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class reduceRewriteOpComplex1(nn.Module):\n",
    "    \"\"\"\n",
    "    A pytorch implementation of Factorization Machine.\n",
    "\n",
    "    Reference:\n",
    "        S Rendle, Factorization Machines, 2010.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,redundancy_slice,non_redundancy_slice,batch=4096):\n",
    "        super().__init__()\n",
    "        self.redundancy_slice = redundancy_slice\n",
    "        self.non_redundancy_slice = non_redundancy_slice\n",
    "        self.batch = batch\n",
    "\n",
    "    def forward(self, x):\n",
    "      redundancy_emb = x[self.redundancy_slice]\n",
    "      non_redundancy_emb = x[self.non_redundancy_slice]\n",
    "      redundancy_emb = torch.sum(redundancy_emb, dim = -2)\n",
    "      redundancy_emb = redundancy_emb.repeat(self.batch, 1)\n",
    "      return redundancy_emb + torch.sum(non_redundancy_emb, dim = -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "modify1 = symbolic_trace(reduceRewriteOpComplex1((0,slice(None,80,None)),(slice(None,None,None),slice(80,None,None))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reduceRewriteOpComplex1()"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modify1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 5.59 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "74.6 µs ± 51 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn( size=(4096,100,128)).to(device)\n",
    "%timeit -n 1 -r 30 with torch.no_grad():modify1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 6.27 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "428 µs ± 285 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn( size=(4096,100,128))\n",
    "%timeit -n 1 -r 30 with torch.no_grad():modify1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total true time 1.1098384857177734 ms\n",
      "total time: 1.2545585632324219 ms\n",
      "Op type        Op           Average runtime (ms)    Pct total runtime\n",
      "-------------  ---------  ----------------------  -------------------\n",
      "call_function  sum_2                   0.523806             41.7522\n",
      "call_method    repeat                  0.275135             21.9308\n",
      "call_function  add                     0.145912             11.6306\n",
      "call_function  getitem                 0.0641346             5.11212\n",
      "call_function  sum_1                   0.0445843             3.55378\n",
      "placeholder    x                       0.0264645             2.10946\n",
      "call_function  getitem_1               0.0190735             1.52033\n",
      "output         output                  0.0107288             0.855188\n"
     ]
    }
   ],
   "source": [
    "interp = utils.ProfilingInterpreter(modify1)\n",
    "interp.run(x)\n",
    "print(interp.summary(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class reduceRewriteOpComplex2(nn.Module):\n",
    "    \"\"\"\n",
    "    A pytorch implementation of Factorization Machine.\n",
    "\n",
    "    Reference:\n",
    "        S Rendle, Factorization Machines, 2010.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,redundancy_slice,non_redundancy_slice,batch=4096):\n",
    "        super().__init__()\n",
    "        self.redundancy_slice = redundancy_slice\n",
    "        self.non_redundancy_slice = non_redundancy_slice\n",
    "        self.batch = batch\n",
    "\n",
    "    def forward(self, x):\n",
    "      redundancy_emb = x[self.redundancy_slice]\n",
    "      non_redundancy_emb = x[self.non_redundancy_slice]\n",
    "      redundancy_emb = torch.sum(redundancy_emb,dim = -2)\n",
    "      non_redundancy_emb = torch.sum(non_redundancy_emb,dim = -2)\n",
    "      redundancy_emb = redundancy_emb.repeat(self.batch, 1)\n",
    "      emb = torch.stack([redundancy_emb,non_redundancy_emb], dim = 1)\n",
    "      return torch.sum(emb, dim = -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "modify2 = symbolic_trace(reduceRewriteOpComplex2((0,slice(None,80,None)),(slice(None,None,None),slice(80,None,None))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reduceRewriteOpComplex2()"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modify2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 4.74 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "91.4 µs ± 51.3 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn( size=(4096,100,128)).to(device)\n",
    "%timeit -n 1 -r 30 with torch.no_grad():modify2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 5.34 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "671 µs ± 391 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn( size=(4096,100,128))\n",
    "%timeit -n 1 -r 30 with torch.no_grad():modify2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total true time 2.0186901092529297 ms\n",
      "total time: 4.133939743041992 ms\n",
      "Op type        Op           Average runtime (ms)    Pct total runtime\n",
      "-------------  ---------  ----------------------  -------------------\n",
      "call_function  sum_2                   0.939608             22.7291\n",
      "call_function  stack                   0.309229              7.48025\n",
      "call_function  sum_3                   0.272751              6.59784\n",
      "call_method    repeat                  0.183582              4.44086\n",
      "call_function  getitem                 0.113487              2.74526\n",
      "call_function  sum_1                   0.0874996             2.11662\n",
      "call_function  getitem_1               0.0560284             1.35533\n",
      "placeholder    x                       0.0422001             1.02082\n",
      "output         output                  0.0143051             0.346041\n"
     ]
    }
   ],
   "source": [
    "interp = utils.ProfilingInterpreter(modify2)\n",
    "interp.run(x)\n",
    "print(interp.summary(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepctr-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
