{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import pnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace success!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.fx import subgraph_rewriter, symbolic_trace\n",
    "import utils\n",
    "from torch.fx import Proxy, Graph, GraphModule\n",
    "from torch.fx.passes.utils.matcher_utils import SubgraphMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxsim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只修改了哈达玛乘的部分\n",
    "def gen_pattern_replace_and_matcher_for_loop_pnn_mul(traced,\n",
    "                                                  redundancy_part_slice,non_redundancy_part_slice,\n",
    "                                                  embed_node_name,getitem_node_names,num_field,batch = 4096,match_func = None\n",
    "                                                ):\n",
    "  from torch.fx.passes.utils.matcher_utils import SubgraphMatcher\n",
    "\n",
    "\n",
    "  def _match(match,ori,pat):\n",
    "    return True \n",
    "  env  = utils.get_env(traced)\n",
    "  target_node = env[embed_node_name]\n",
    "  target_node_mod = utils.get_target_mod(traced,target_node.target)\n",
    "  shape_info = target_node_mod.weight.data.shape\n",
    "  getitem_node_args = [env[i].args[1] for i in getitem_node_names]\n",
    "  class PatternClass(torch.nn.Module):\n",
    "      def __init__(self):\n",
    "          super().__init__()\n",
    "          self.embed = torch.nn.Embedding(1, 1)\n",
    "          self.offsets = nn.Parameter(torch.as_tensor(np.array((0, *np.cumsum([10,10])[:-1]), dtype=np.int64)),False)\n",
    "          self.embed_output_dim = shape_info[1] * num_field\n",
    "          self.mlp = nn.Linear(shape_info[0],shape_info[1])\n",
    "\n",
    "      def pn(self,x):\n",
    "         return torch.sum(x[getitem_node_args[0]] * x[getitem_node_args[1]], dim = 2)\n",
    "\n",
    "      def forward(self,x):\n",
    "          x = self.embed(x + self.offsets)\n",
    "          cross_term = self.pn(x)\n",
    "          x = torch.cat([x.view(-1, self.embed_output_dim), cross_term], dim=1)\n",
    "          return x\n",
    "  pattern = PatternClass()  \n",
    "  pattern_trace = symbolic_trace(pattern)\n",
    "  pattern_graph = pattern_trace.graph\n",
    "  original_graph = traced.graph\n",
    "  matcher =  SubgraphMatcher(pattern_graph, match_output=False, match_placeholder=False,\n",
    "                              remove_overlapping_matches=True)\n",
    "  _matches = matcher.match(original_graph)\n",
    "  match_filters = [_match if match_func is None else match_func]\n",
    "  _matches = [\n",
    "      m for m in _matches\n",
    "      if all(match_filter(m, original_graph, pattern_graph)\n",
    "              for match_filter in match_filters)\n",
    "  ]  \n",
    "  # 因为在过滤器中做了限制应该只有一个符合要求的\n",
    "  _matched = _matches[0]\n",
    "  pattern_env = utils.get_env(pattern_trace)\n",
    "  node_map = _matched.nodes_map\n",
    "  pn = pattern_env['offsets']\n",
    "  offsets_node = node_map[pn]\n",
    "  offsets_val = utils.get_target_mod(traced,offsets_node.target)\n",
    "  \n",
    "  embed_node = node_map[pattern_env['embed']]\n",
    "  embed_node_module = utils.get_target_mod(traced,embed_node.target)\n",
    "  \n",
    "#   linear_node = node_map[pattern_env['mlp']]\n",
    "#   linear_node_module = utils.get_target_mod(traced,linear_node.target)\n",
    "#   linear_node_weight = linear_node_module.weight.data\n",
    "#   linear_node_bias = linear_node_module.bias.data\n",
    "  \n",
    "  class ReplacementClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.offsets = offsets_val\n",
    "      self.num_fields = num_field\n",
    "      self.num_prefix = redundancy_part_slice[1].stop\n",
    "      # self.ori_linear_shape = linear_node_weight.shape\n",
    "      self.num_sufix = self.num_fields - self.num_prefix\n",
    "      self.total = self.num_fields * (self.num_fields - 1) // 2\n",
    "      self.redundancy_cross_part_total = self.num_prefix * (self.num_prefix - 1) // 2\n",
    "      self.non_redundancy_cross_part_total = self.num_sufix * (self.num_sufix - 1) // 2\n",
    "      self.rest_cross_part_total = self.total - self.redundancy_cross_part_total - self.non_redundancy_cross_part_total\n",
    "      # 提取对应的权重参数逻辑有些复杂，先mock\n",
    "      self.embed = embed_node_module\n",
    "      self.embed_dim = self.embed.weight.data.shape[1]\n",
    "      self.embed_output_dim = self.embed_dim * self.num_fields\n",
    "      # self.ll = nn.Linear(linear_node_weight.shape[1],linear_node_weight.shape[0],bias = True)\n",
    "\n",
    "    def pn(self,reducey_x,non_redundancy_x):\n",
    "       redundancy_x_row, reducey_x_col = list(),list()\n",
    "       non_redundancy_x_row, non_redundancy_x_col = list(), list()\n",
    "       mixed_x_row, mixed_x_col = list(), list()\n",
    "       prefix = self.num_prefix\n",
    "       sufix = self.num_sufix\n",
    "       for i in range(prefix - 1):\n",
    "          for j in range(i+1,prefix):\n",
    "             redundancy_x_row.append(i),reducey_x_col.append(j)\n",
    "       for i in range(sufix - 1):\n",
    "          for j in range(i + 1,sufix):\n",
    "             non_redundancy_x_row.append(i),non_redundancy_x_col.append(j)\n",
    "\n",
    "       for i in range(prefix):\n",
    "          for j in range(sufix):\n",
    "             mixed_x_row.append(i),mixed_x_col.append(j)\n",
    "       return reducey_x[redundancy_x_row] * reducey_x[reducey_x_col],\\\n",
    "              non_redundancy_x[:,non_redundancy_x_row] * non_redundancy_x[:,non_redundancy_x_col],\\\n",
    "              reducey_x[mixed_x_row] * non_redundancy_x[:,mixed_x_col]\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "      x = self.embed(x + self.offsets)\n",
    "      redundancy_part_embed = x[redundancy_part_slice]\n",
    "      non_redundancy_part_embed = x[non_redundancy_part_slice]\n",
    "      redundancy_part_pn, non_redundancy_part_pn, mixed_part_pn = self.pn(redundancy_part_embed,non_redundancy_part_embed)\n",
    "      redundancy_part_pn = redundancy_part_pn.repeat(batch,1,1)\n",
    "      \n",
    "      cross_term =  torch.concat([redundancy_part_pn,non_redundancy_part_pn,mixed_part_pn], dim = 1)\n",
    "      cross_term = torch.sum(cross_term, dim = 2)\n",
    "      x = torch.cat([x.view(-1, self.embed_output_dim), cross_term], dim=1)\n",
    "      return x\n",
    "    \n",
    "  \n",
    "  return pattern,ReplacementClass(),_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 联合修改了哈达玛乘和reduce部分\n",
    "def gen_pattern_replace_and_matcher_for_loop_pnn_mul_reduce(traced,\n",
    "                                                  redundancy_part_slice,non_redundancy_part_slice,\n",
    "                                                  embed_node_name,getitem_node_names,num_field,batch = 4096,match_func = None\n",
    "                                                ):\n",
    "  from torch.fx.passes.utils.matcher_utils import SubgraphMatcher\n",
    "\n",
    "\n",
    "  def _match(match,ori,pat):\n",
    "    return True \n",
    "  env  = utils.get_env(traced)\n",
    "  target_node = env[embed_node_name]\n",
    "  target_node_mod = utils.get_target_mod(traced,target_node.target)\n",
    "  shape_info = target_node_mod.weight.data.shape\n",
    "  getitem_node_args = [env[i].args[1] for i in getitem_node_names]\n",
    "  class PatternClass(torch.nn.Module):\n",
    "      def __init__(self):\n",
    "          super().__init__()\n",
    "          self.embed = torch.nn.Embedding(1, 1)\n",
    "          self.offsets = nn.Parameter(torch.as_tensor(np.array((0, *np.cumsum([10,10])[:-1]), dtype=np.int64)),False)\n",
    "          self.embed_output_dim = shape_info[1] * num_field\n",
    "          self.mlp = nn.Linear(shape_info[0],shape_info[1])\n",
    "\n",
    "      def pn(self,x):\n",
    "         return torch.sum(x[getitem_node_args[0]] * x[getitem_node_args[1]], dim = 2)\n",
    "\n",
    "      def forward(self,x):\n",
    "          x = self.embed(x + self.offsets)\n",
    "          cross_term = self.pn(x)\n",
    "          x = torch.cat([x.view(-1, self.embed_output_dim), cross_term], dim=1)\n",
    "          return x\n",
    "  pattern = PatternClass()  \n",
    "  pattern_trace = symbolic_trace(pattern)\n",
    "  pattern_graph = pattern_trace.graph\n",
    "  original_graph = traced.graph\n",
    "  matcher =  SubgraphMatcher(pattern_graph, match_output=False, match_placeholder=False,\n",
    "                              remove_overlapping_matches=True)\n",
    "  _matches = matcher.match(original_graph)\n",
    "  match_filters = [_match if match_func is None else match_func]\n",
    "  _matches = [\n",
    "      m for m in _matches\n",
    "      if all(match_filter(m, original_graph, pattern_graph)\n",
    "              for match_filter in match_filters)\n",
    "  ]  \n",
    "  # 因为在过滤器中做了限制应该只有一个符合要求的\n",
    "  _matched = _matches[0]\n",
    "  pattern_env = utils.get_env(pattern_trace)\n",
    "  node_map = _matched.nodes_map\n",
    "  pn = pattern_env['offsets']\n",
    "  offsets_node = node_map[pn]\n",
    "  offsets_val = utils.get_target_mod(traced,offsets_node.target)\n",
    "  \n",
    "  embed_node = node_map[pattern_env['embed']]\n",
    "  embed_node_module = utils.get_target_mod(traced,embed_node.target)\n",
    "  \n",
    "#   linear_node = node_map[pattern_env['mlp']]\n",
    "#   linear_node_module = utils.get_target_mod(traced,linear_node.target)\n",
    "#   linear_node_weight = linear_node_module.weight.data\n",
    "#   linear_node_bias = linear_node_module.bias.data\n",
    "  \n",
    "  class ReplacementClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.offsets = offsets_val\n",
    "      self.num_fields = num_field\n",
    "      self.num_prefix = redundancy_part_slice[1].stop\n",
    "      # self.ori_linear_shape = linear_node_weight.shape\n",
    "      self.num_sufix = self.num_fields - self.num_prefix\n",
    "      self.total = self.num_fields * (self.num_fields - 1) // 2\n",
    "      self.redundancy_cross_part_total = self.num_prefix * (self.num_prefix - 1) // 2\n",
    "      self.non_redundancy_cross_part_total = self.num_sufix * (self.num_sufix - 1) // 2\n",
    "      self.rest_cross_part_total = self.total - self.redundancy_cross_part_total - self.non_redundancy_cross_part_total\n",
    "      # 提取对应的权重参数逻辑有些复杂，先mock\n",
    "      self.embed = embed_node_module\n",
    "      self.embed_dim = self.embed.weight.data.shape[1]\n",
    "      self.embed_output_dim = self.embed_dim * self.num_fields\n",
    "      # self.ll = nn.Linear(linear_node_weight.shape[1],linear_node_weight.shape[0],bias = True)\n",
    "\n",
    "    def pn(self,reducey_x,non_redundancy_x):\n",
    "       redundancy_x_row, reducey_x_col = list(),list()\n",
    "       non_redundancy_x_row, non_redundancy_x_col = list(), list()\n",
    "       mixed_x_row, mixed_x_col = list(), list()\n",
    "       prefix = self.num_prefix\n",
    "       sufix = self.num_sufix\n",
    "       for i in range(prefix - 1):\n",
    "          for j in range(i+1,prefix):\n",
    "             redundancy_x_row.append(i),reducey_x_col.append(j)\n",
    "       for i in range(sufix - 1):\n",
    "          for j in range(i + 1,sufix):\n",
    "             non_redundancy_x_row.append(i),non_redundancy_x_col.append(j)\n",
    "\n",
    "       for i in range(prefix):\n",
    "          for j in range(sufix):\n",
    "             mixed_x_row.append(i),mixed_x_col.append(j)\n",
    "       return torch.sum((reducey_x[redundancy_x_row] * reducey_x[reducey_x_col]).unsqueeze(0),dim = -1),\\\n",
    "              torch.sum(non_redundancy_x[:,non_redundancy_x_row] * non_redundancy_x[:,non_redundancy_x_col],dim = -1),\\\n",
    "              torch.sum(reducey_x[mixed_x_row] * non_redundancy_x[:,mixed_x_col],dim = 2)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "      x = self.embed(x + self.offsets)\n",
    "      redundancy_part_embed = x[redundancy_part_slice]\n",
    "      non_redundancy_part_embed = x[non_redundancy_part_slice]\n",
    "      redundancy_part_pn, non_redundancy_part_pn, mixed_part_pn = self.pn(redundancy_part_embed,non_redundancy_part_embed)\n",
    "      redundancy_part_pn = redundancy_part_pn.repeat(batch,1)\n",
    "      \n",
    "      cross_term =  torch.concat([redundancy_part_pn,non_redundancy_part_pn,mixed_part_pn], dim = 1)\n",
    "      # cross_term = torch.sum(cross_term, dim = 1)\n",
    "      x = torch.cat([x.view(-1, self.embed_output_dim), cross_term], dim=1)\n",
    "      return x\n",
    "    \n",
    "  \n",
    "  return pattern,ReplacementClass(),_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只修改了reduce部分\n",
    "def gen_pattern_replace_and_matcher_for_loop_pnn_reduce(traced,\n",
    "                                                  redundancy_part_slice,non_redundancy_part_slice,\n",
    "                                                  embed_node_name,getitem_node_names,num_field,batch = 4096,match_func = None\n",
    "                                                ):\n",
    "  from torch.fx.passes.utils.matcher_utils import SubgraphMatcher\n",
    "\n",
    "\n",
    "  def _match(match,ori,pat):\n",
    "    return True \n",
    "  env  = utils.get_env(traced)\n",
    "  target_node = env[embed_node_name]\n",
    "  target_node_mod = utils.get_target_mod(traced,target_node.target)\n",
    "  shape_info = target_node_mod.weight.data.shape\n",
    "  getitem_node_args = [env[i].args[1] for i in getitem_node_names]\n",
    "  class PatternClass(torch.nn.Module):\n",
    "      def __init__(self):\n",
    "          super().__init__()\n",
    "          self.embed = torch.nn.Embedding(1, 1)\n",
    "          self.offsets = nn.Parameter(torch.as_tensor(np.array((0, *np.cumsum([10,10])[:-1]), dtype=np.int64)),False)\n",
    "          self.embed_output_dim = shape_info[1] * num_field\n",
    "          self.mlp = nn.Linear(shape_info[0],shape_info[1])\n",
    "\n",
    "      def pn(self,x):\n",
    "         return torch.sum(x[getitem_node_args[0]] * x[getitem_node_args[1]], dim = 2)\n",
    "\n",
    "      def forward(self,x):\n",
    "          x = self.embed(x + self.offsets)\n",
    "          cross_term = self.pn(x)\n",
    "          x = torch.cat([x.view(-1, self.embed_output_dim), cross_term], dim=1)\n",
    "          return x\n",
    "  pattern = PatternClass()  \n",
    "  pattern_trace = symbolic_trace(pattern)\n",
    "  pattern_graph = pattern_trace.graph\n",
    "  original_graph = traced.graph\n",
    "  matcher =  SubgraphMatcher(pattern_graph, match_output=False, match_placeholder=False,\n",
    "                              remove_overlapping_matches=True)\n",
    "  _matches = matcher.match(original_graph)\n",
    "  match_filters = [_match if match_func is None else match_func]\n",
    "  _matches = [\n",
    "      m for m in _matches\n",
    "      if all(match_filter(m, original_graph, pattern_graph)\n",
    "              for match_filter in match_filters)\n",
    "  ]  \n",
    "  # 因为在过滤器中做了限制应该只有一个符合要求的\n",
    "  _matched = _matches[0]\n",
    "  pattern_env = utils.get_env(pattern_trace)\n",
    "  node_map = _matched.nodes_map\n",
    "  pn = pattern_env['offsets']\n",
    "  offsets_node = node_map[pn]\n",
    "  offsets_val = utils.get_target_mod(traced,offsets_node.target)\n",
    "  \n",
    "  embed_node = node_map[pattern_env['embed']]\n",
    "  embed_node_module = utils.get_target_mod(traced,embed_node.target)\n",
    "  \n",
    "#   linear_node = node_map[pattern_env['mlp']]\n",
    "#   linear_node_module = utils.get_target_mod(traced,linear_node.target)\n",
    "#   linear_node_weight = linear_node_module.weight.data\n",
    "#   linear_node_bias = linear_node_module.bias.data\n",
    "  \n",
    "  class ReplacementClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.offsets = offsets_val\n",
    "      self.num_fields = num_field\n",
    "      self.num_prefix = redundancy_part_slice[1].stop\n",
    "      # self.ori_linear_shape = linear_node_weight.shape\n",
    "      self.num_sufix = self.num_fields - self.num_prefix\n",
    "      self.total = self.num_fields * (self.num_fields - 1) // 2\n",
    "      self.redundancy_cross_part_total = self.num_prefix * (self.num_prefix - 1) // 2\n",
    "      self.non_redundancy_cross_part_total = self.num_sufix * (self.num_sufix - 1) // 2\n",
    "      self.rest_cross_part_total = self.total - self.redundancy_cross_part_total - self.non_redundancy_cross_part_total\n",
    "      # 提取对应的权重参数逻辑有些复杂，先mock\n",
    "      self.embed = embed_node_module\n",
    "      self.embed_dim = self.embed.weight.data.shape[1]\n",
    "      self.embed_output_dim = self.embed_dim * self.num_fields\n",
    "      # self.ll = nn.Linear(linear_node_weight.shape[1],linear_node_weight.shape[0],bias = True)\n",
    "\n",
    "    def pn(self,reducey_x,non_redundancy_x):\n",
    "       redundancy_x_row, reducey_x_col = list(),list()\n",
    "       non_redundancy_x_row, non_redundancy_x_col = list(), list()\n",
    "       mixed_x_row, mixed_x_col = list(), list()\n",
    "       prefix = self.num_prefix\n",
    "       sufix = self.num_sufix\n",
    "       for i in range(prefix - 1):\n",
    "          for j in range(i+1,prefix):\n",
    "             redundancy_x_row.append(i),reducey_x_col.append(j)\n",
    "       for i in range(sufix - 1):\n",
    "          for j in range(i + 1,sufix):\n",
    "             non_redundancy_x_row.append(i),non_redundancy_x_col.append(j)\n",
    "\n",
    "       for i in range(prefix):\n",
    "          for j in range(sufix):\n",
    "             mixed_x_row.append(i),mixed_x_col.append(j)\n",
    "       return torch.sum((reducey_x[redundancy_x_row] * reducey_x[reducey_x_col]).unsqueeze(0),dim = -1),\\\n",
    "              torch.sum(non_redundancy_x[:,non_redundancy_x_row] * non_redundancy_x[:,non_redundancy_x_col],dim = -1),\\\n",
    "              torch.sum(reducey_x[mixed_x_row] * non_redundancy_x[:,mixed_x_col],dim = 2)\n",
    "\n",
    "    def pn(self,x):\n",
    "      cross_term =  x[getitem_node_args[0]] * x[getitem_node_args[1]]\n",
    "      redundancy_part_pn = cross_term[0,:self.redundancy_cross_part_total]\n",
    "      non_redundancy_part_pn = cross_term[:,self.redundancy_cross_part_total:self.redundancy_cross_part_total + self.non_redundancy_cross_part_total]\n",
    "      mixed_part_pn = cross_term[:,self.redundancy_cross_part_total + self.non_redundancy_cross_part_total:]\n",
    "      return torch.sum(redundancy_part_pn.unsqueeze(0),dim = -1),\\\n",
    "              torch.sum(non_redundancy_part_pn,dim = -1),\\\n",
    "              torch.sum(mixed_part_pn,dim = 2)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "      x = self.embed(x + self.offsets)\n",
    "      redundancy_part_pn, non_redundancy_part_pn, mixed_part_pn = self.pn(x)\n",
    "      redundancy_part_pn = redundancy_part_pn.repeat(batch,1)\n",
    "      \n",
    "      cross_term =  torch.concat([redundancy_part_pn,non_redundancy_part_pn,mixed_part_pn], dim = 1)\n",
    "      # cross_term = torch.sum(cross_term, dim = 1)\n",
    "      x = torch.cat([x.view(-1, self.embed_output_dim), cross_term], dim=1)\n",
    "      return x\n",
    "    \n",
    "  \n",
    "  return pattern,ReplacementClass(),_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全模型改写\n",
    "def gen_pattern_replace_and_matcher_for_loop_pnn(traced,\n",
    "                                                  redency_part_slice,unredency_part_slice,\n",
    "                                                  embed_node_name,getitem_node_names,num_field,match_func = None\n",
    "                                                ):\n",
    "  from torch.fx.passes.utils.matcher_utils import SubgraphMatcher\n",
    "\n",
    "\n",
    "  def _match(match,ori,pat):\n",
    "    return True \n",
    "  env  = utils.get_env(traced)\n",
    "  target_node = env[embed_node_name]\n",
    "  target_node_mod = utils.get_target_mod(traced,target_node.target)\n",
    "  shape_info = target_node_mod.weight.data.shape\n",
    "  getitem_node_args = [env[i].args[1] for i in getitem_node_names]\n",
    "  class PatternClass(torch.nn.Module):\n",
    "      def __init__(self):\n",
    "          super().__init__()\n",
    "          self.embed = torch.nn.Embedding(1, 1)\n",
    "          self.offsets = nn.Parameter(torch.as_tensor(np.array((0, *np.cumsum([10,10])[:-1]), dtype=np.int64)),False)\n",
    "          self.embed_output_dim = shape_info[1] * num_field\n",
    "          self.mlp = nn.Linear(shape_info[0],shape_info[1])\n",
    "\n",
    "      def pn(self,x):\n",
    "         return torch.sum(x[getitem_node_args[0]] * x[getitem_node_args[1]], dim = 2)\n",
    "\n",
    "      def forward(self,x):\n",
    "          x = self.embed(x + self.offsets)\n",
    "          cross_term = self.pn(x)\n",
    "          x = torch.cat([x.view(-1, self.embed_output_dim), cross_term], dim=1)\n",
    "          return self.mlp(x)\n",
    "  pattern = PatternClass()  \n",
    "  pattern_trace = symbolic_trace(pattern)\n",
    "  pattern_graph = pattern_trace.graph\n",
    "  original_graph = traced.graph\n",
    "  matcher =  SubgraphMatcher(pattern_graph, match_output=False, match_placeholder=False,\n",
    "                              remove_overlapping_matches=True)\n",
    "  _matches = matcher.match(original_graph)\n",
    "  match_filters = [_match if match_func is None else match_func]\n",
    "  _matches = [\n",
    "      m for m in _matches\n",
    "      if all(match_filter(m, original_graph, pattern_graph)\n",
    "              for match_filter in match_filters)\n",
    "  ]  \n",
    "  # 因为在过滤器中做了限制应该只有一个符合要求的\n",
    "  _matched = _matches[0]\n",
    "  pattern_env = utils.get_env(pattern_trace)\n",
    "  node_map = _matched.nodes_map\n",
    "  pn = pattern_env['offsets']\n",
    "  offsets_node = node_map[pn]\n",
    "  offsets_val = utils.get_target_mod(traced,offsets_node.target)\n",
    "  \n",
    "  embed_node = node_map[pattern_env['embed']]\n",
    "  embed_node_module = utils.get_target_mod(traced,embed_node.target)\n",
    "  \n",
    "  linear_node = node_map[pattern_env['mlp']]\n",
    "  linear_node_module = utils.get_target_mod(traced,linear_node.target)\n",
    "  linear_node_weight = linear_node_module.weight.data\n",
    "  linear_node_bias = linear_node_module.bias.data\n",
    "  \n",
    "  class ReplacementClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.redency_offset = nn.Parameter(torch.as_tensor(np.array(offsets_val[redency_part_slice[1]], dtype=np.int64)),False)\n",
    "      self.unredency_offset = nn.Parameter(torch.as_tensor(np.array(offsets_val[unredency_part_slice[1]], dtype=np.int64)),False)\n",
    "      self.num_fields = num_field\n",
    "      self.num_prefix = redency_part_slice[1].stop\n",
    "      self.ori_linear_shape = linear_node_weight.shape\n",
    "      self.num_sufix = self.num_fields - self.num_prefix\n",
    "      self.total = self.num_fields * (self.num_fields - 1) // 2\n",
    "      self.redency_cross_part_total = self.num_prefix * (self.num_prefix - 1) // 2\n",
    "      self.unredency_cross_part_total = self.num_sufix * (self.num_sufix - 1) // 2\n",
    "      self.rest_cross_part_total = self.total - self.redency_cross_part_total - self.unredency_cross_part_total\n",
    "      # 提取对应的权重参数逻辑有些复杂，先mock\n",
    "      self.embed = embed_node_module\n",
    "      self.embed_dim = self.embed.weight.data.shape[1]\n",
    "\n",
    "      # cross part linear\n",
    "      self.redency_cross_part_linear = nn.Linear(self.redency_cross_part_total,self.ori_linear_shape[0],bias = True)\n",
    "      self.unredency_cross_part_linear = nn.Linear(self.unredency_cross_part_total,self.ori_linear_shape[0],bias = False)\n",
    "      self.mixed_cross_part_linear = nn.Linear(self.rest_cross_part_total,self.ori_linear_shape[0],bias = False)\n",
    "      \n",
    "      # embed part linear \n",
    "      self.redency_linear = nn.Linear(self.embed_dim * self.num_prefix,self.ori_linear_shape[0],bias = False)\n",
    "      self.unredency_linear = nn.Linear(self.embed_dim * self.num_sufix,self.ori_linear_shape[0],bias = False)\n",
    "\n",
    "    def pn(self,reducey_x,unredency_x):\n",
    "       redency_x_row, reducey_x_col = list(),list()\n",
    "       unredency_x_row, unredency_x_col = list(), list()\n",
    "       mixed_x_row, mixed_x_col = list(), list()\n",
    "       prefix = self.num_prefix\n",
    "       sufix = self.num_sufix\n",
    "       for i in range(prefix - 1):\n",
    "          for j in range(i+1,prefix):\n",
    "             redency_x_row.append(i),reducey_x_col.append(j)\n",
    "       for i in range(sufix - 1):\n",
    "          for j in range(i + 1,sufix):\n",
    "             unredency_x_row.append(i),unredency_x_col.append(j)\n",
    "\n",
    "       for i in range(prefix):\n",
    "          for j in range(sufix):\n",
    "             mixed_x_row.append(i),mixed_x_col.append(j)\n",
    "       return torch.sum((reducey_x[redency_x_row] * reducey_x[reducey_x_col]).unsqueeze(0),dim = 2),\\\n",
    "              torch.sum(unredency_x[:,unredency_x_row] * unredency_x[:,unredency_x_col],dim = 2),\\\n",
    "              torch.sum(reducey_x[mixed_x_row] * unredency_x[:,mixed_x_col],dim = 2)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "      redency_part = x[redency_part_slice] + self.redency_offset\n",
    "      unredency_part = x[unredency_part_slice] + self.unredency_offset\n",
    "      redency_part_embed = self.embed(redency_part)\n",
    "      unredency_part_embed = self.embed(unredency_part)\n",
    "      redency_part_pn, unredency_part_pn, mixed_part_pn = self.pn(redency_part_embed,unredency_part_embed)\n",
    "      \n",
    "      \n",
    "      return self.redency_linear(self.embed(redency_part).view(-1,self.embed_dim * self.num_prefix)) + self.unredency_linear(self.embed(unredency_part).view(-1,self.embed_dim * self.num_sufix)) +\\\n",
    "         self.redency_cross_part_linear(redency_part_pn) + self.mixed_cross_part_linear(mixed_part_pn) + self.unredency_cross_part_linear(unredency_part_pn)\n",
    "      # return unredency_sum\n",
    "    \n",
    "  \n",
    "  return pattern,ReplacementClass(),_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只改写linear\n",
    "def gen_pattern_replace_and_matcher_for_loop_pnn_linear(traced,\n",
    "                                                  redency_part_slice,unredency_part_slice,\n",
    "                                                  embed_node_name,getitem_node_names,num_field,match_func = None\n",
    "                                                ):\n",
    "  from torch.fx.passes.utils.matcher_utils import SubgraphMatcher\n",
    "\n",
    "\n",
    "  def _match(match,ori,pat):\n",
    "    return True \n",
    "  env  = utils.get_env(traced)\n",
    "  target_node = env[embed_node_name]\n",
    "  target_node_mod = utils.get_target_mod(traced,target_node.target)\n",
    "  shape_info = target_node_mod.weight.data.shape\n",
    "  getitem_node_args = [env[i].args[1] for i in getitem_node_names]\n",
    "  class PatternClass(torch.nn.Module):\n",
    "      def __init__(self):\n",
    "          super().__init__()\n",
    "          self.embed = torch.nn.Embedding(1, 1)\n",
    "          self.offsets = nn.Parameter(torch.as_tensor(np.array((0, *np.cumsum([10,10])[:-1]), dtype=np.int64)),False)\n",
    "          self.embed_output_dim = shape_info[1] * num_field\n",
    "          self.mlp = nn.Linear(shape_info[0],shape_info[1])\n",
    "\n",
    "      def pn(self,x):\n",
    "         return torch.sum(x[getitem_node_args[0]] * x[getitem_node_args[1]], dim = 2)\n",
    "\n",
    "      def forward(self,x):\n",
    "          x = self.embed(x + self.offsets)\n",
    "          cross_term = self.pn(x)\n",
    "          x = torch.cat([x.view(-1, self.embed_output_dim), cross_term], dim=1)\n",
    "          return self.mlp(x)\n",
    "  pattern = PatternClass()  \n",
    "  pattern_trace = symbolic_trace(pattern)\n",
    "  pattern_graph = pattern_trace.graph\n",
    "  original_graph = traced.graph\n",
    "  matcher =  SubgraphMatcher(pattern_graph, match_output=False, match_placeholder=False,\n",
    "                              remove_overlapping_matches=True)\n",
    "  _matches = matcher.match(original_graph)\n",
    "  match_filters = [_match if match_func is None else match_func]\n",
    "  _matches = [\n",
    "      m for m in _matches\n",
    "      if all(match_filter(m, original_graph, pattern_graph)\n",
    "              for match_filter in match_filters)\n",
    "  ]  \n",
    "  # 因为在过滤器中做了限制应该只有一个符合要求的\n",
    "  _matched = _matches[0]\n",
    "  pattern_env = utils.get_env(pattern_trace)\n",
    "  node_map = _matched.nodes_map\n",
    "  pn = pattern_env['offsets']\n",
    "  offsets_node = node_map[pn]\n",
    "  offsets_val = utils.get_target_mod(traced,offsets_node.target)\n",
    "  \n",
    "  embed_node = node_map[pattern_env['embed']]\n",
    "  embed_node_module = utils.get_target_mod(traced,embed_node.target)\n",
    "  \n",
    "  linear_node = node_map[pattern_env['mlp']]\n",
    "  linear_node_module = utils.get_target_mod(traced,linear_node.target)\n",
    "  linear_node_weight = linear_node_module.weight.data\n",
    "  linear_node_bias = linear_node_module.bias.data\n",
    "  \n",
    "  class ReplacementClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.offsets = offsets_val\n",
    "      self.num_fields = num_field\n",
    "      self.num_prefix = redency_part_slice[1].stop\n",
    "      self.ori_linear_shape = linear_node_weight.shape\n",
    "      self.num_sufix = self.num_fields - self.num_prefix\n",
    "      self.total = self.num_fields * (self.num_fields - 1) // 2\n",
    "      self.redency_cross_part_total = self.num_prefix * (self.num_prefix - 1) // 2\n",
    "      self.unredency_cross_part_total = self.num_sufix * (self.num_sufix - 1) // 2\n",
    "      self.rest_cross_part_total = self.total - self.redency_cross_part_total - self.unredency_cross_part_total\n",
    "      # 提取对应的权重参数逻辑有些复杂，先mock\n",
    "      self.embed = embed_node_module\n",
    "      self.embed_dim = self.embed.weight.data.shape[1]\n",
    "      self.embed_output_dim = self.num_fields * self.embed_dim\n",
    "      # cross part linear\n",
    "      self.redency_cross_part_linear = nn.Linear(self.redency_cross_part_total,self.ori_linear_shape[0],bias = True)\n",
    "      self.unredency_cross_part_linear = nn.Linear(self.unredency_cross_part_total,self.ori_linear_shape[0],bias = False)\n",
    "      self.mixed_cross_part_linear = nn.Linear(self.rest_cross_part_total,self.ori_linear_shape[0],bias = False)\n",
    "      \n",
    "      # embed part linear \n",
    "      self.redency_linear = nn.Linear(self.embed_dim * self.num_prefix,self.ori_linear_shape[0],bias = False)\n",
    "      self.unredency_linear = nn.Linear(self.embed_dim * self.num_sufix,self.ori_linear_shape[0],bias = False)\n",
    "\n",
    "    def pn(self,x):\n",
    "         cross_term =  x[getitem_node_args[0]] * x[getitem_node_args[1]]\n",
    "         return torch.sum(cross_term, dim = 2)\n",
    "\n",
    "    def forward(self,x):\n",
    "      x = self.embed(x + self.offsets)\n",
    "      cross_term = self.pn(x)\n",
    "      x = torch.cat([x.view(-1, self.embed_output_dim), cross_term], dim=1)\n",
    "      len_redundancy = self.embed_dim * self.num_prefix\n",
    "      len_nonredundancy = len_redundancy + self.embed_dim * self.num_sufix\n",
    "      len_pn_redundancy = len_nonredundancy + self.redency_cross_part_total\n",
    "      len_pn_nonredundancy = len_pn_redundancy + self.unredency_cross_part_total\n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      return self.redency_linear(x[0,:len_redundancy]) + self.unredency_linear(x[:,len_redundancy : len_nonredundancy]) +\\\n",
    "         self.redency_cross_part_linear(x[0,len_nonredundancy:len_pn_redundancy]) +\\\n",
    "         self.mixed_cross_part_linear(x[:,len_pn_nonredundancy:]) + self.unredency_cross_part_linear(x[:,len_pn_redundancy:len_pn_nonredundancy])\n",
    "      # return unredency_sum\n",
    "    \n",
    "  \n",
    "  return pattern,ReplacementClass(),_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试对mul算子和reduce算子做单独改写的效果\n",
    "def gen_pattern_replace_and_matcher_for_loop_pnn_single_mul_reduce(traced,\n",
    "                                                  redundancy_part_slice,non_redundancy_part_slice,\n",
    "                                                  embed_node_name,getitem_node_names,num_field,batch = 4096,match_func = None\n",
    "                                                ):\n",
    "  from torch.fx.passes.utils.matcher_utils import SubgraphMatcher\n",
    "\n",
    "\n",
    "  def _match(match,ori,pat):\n",
    "    return True \n",
    "  env  = utils.get_env(traced)\n",
    "  target_node = env[embed_node_name]\n",
    "  target_node_mod = utils.get_target_mod(traced,target_node.target)\n",
    "  shape_info = target_node_mod.weight.data.shape\n",
    "  getitem_node_args = [env[i].args[1] for i in getitem_node_names]\n",
    "  class PatternClass(torch.nn.Module):\n",
    "      def __init__(self):\n",
    "          super().__init__()\n",
    "          self.embed = torch.nn.Embedding(1, 1)\n",
    "          self.offsets = nn.Parameter(torch.as_tensor(np.array((0, *np.cumsum([10,10])[:-1]), dtype=np.int64)),False)\n",
    "          self.embed_output_dim = shape_info[1] * num_field\n",
    "          self.mlp = nn.Linear(shape_info[0],shape_info[1])\n",
    "\n",
    "      def pn(self,x):\n",
    "         return torch.sum(x[getitem_node_args[0]] * x[getitem_node_args[1]], dim = 2)\n",
    "\n",
    "      def forward(self,x):\n",
    "          x = self.embed(x + self.offsets)\n",
    "          cross_term = self.pn(x)\n",
    "          x = torch.cat([x.view(-1, self.embed_output_dim), cross_term], dim=1)\n",
    "          return x\n",
    "  pattern = PatternClass()  \n",
    "  pattern_trace = symbolic_trace(pattern)\n",
    "  pattern_graph = pattern_trace.graph\n",
    "  original_graph = traced.graph\n",
    "  matcher =  SubgraphMatcher(pattern_graph, match_output=False, match_placeholder=False,\n",
    "                              remove_overlapping_matches=True)\n",
    "  _matches = matcher.match(original_graph)\n",
    "  match_filters = [_match if match_func is None else match_func]\n",
    "  _matches = [\n",
    "      m for m in _matches\n",
    "      if all(match_filter(m, original_graph, pattern_graph)\n",
    "              for match_filter in match_filters)\n",
    "  ]  \n",
    "  # 因为在过滤器中做了限制应该只有一个符合要求的\n",
    "  _matched = _matches[0]\n",
    "  pattern_env = utils.get_env(pattern_trace)\n",
    "  node_map = _matched.nodes_map\n",
    "  pn = pattern_env['offsets']\n",
    "  offsets_node = node_map[pn]\n",
    "  offsets_val = utils.get_target_mod(traced,offsets_node.target)\n",
    "  \n",
    "  embed_node = node_map[pattern_env['embed']]\n",
    "  embed_node_module = utils.get_target_mod(traced,embed_node.target)\n",
    "  \n",
    "#   linear_node = node_map[pattern_env['mlp']]\n",
    "#   linear_node_module = utils.get_target_mod(traced,linear_node.target)\n",
    "#   linear_node_weight = linear_node_module.weight.data\n",
    "#   linear_node_bias = linear_node_module.bias.data\n",
    "  \n",
    "  class ReplacementClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.offsets = offsets_val\n",
    "      self.num_fields = num_field\n",
    "      self.num_prefix = redundancy_part_slice[1].stop\n",
    "      # self.ori_linear_shape = linear_node_weight.shape\n",
    "      self.num_sufix = self.num_fields - self.num_prefix\n",
    "      self.total = self.num_fields * (self.num_fields - 1) // 2\n",
    "      self.redundancy_cross_part_total = self.num_prefix * (self.num_prefix - 1) // 2\n",
    "      self.non_redundancy_cross_part_total = self.num_sufix * (self.num_sufix - 1) // 2\n",
    "      self.rest_cross_part_total = self.total - self.redundancy_cross_part_total - self.non_redundancy_cross_part_total\n",
    "      # 提取对应的权重参数逻辑有些复杂，先mock\n",
    "      self.embed = embed_node_module\n",
    "      self.embed_dim = self.embed.weight.data.shape[1]\n",
    "      self.embed_output_dim = self.embed_dim * self.num_fields\n",
    "      # self.ll = nn.Linear(linear_node_weight.shape[1],linear_node_weight.shape[0],bias = True)\n",
    "\n",
    "    def pn1(self,reducey_x,non_redundancy_x):\n",
    "       redundancy_x_row, reducey_x_col = list(),list()\n",
    "       non_redundancy_x_row, non_redundancy_x_col = list(), list()\n",
    "       mixed_x_row, mixed_x_col = list(), list()\n",
    "       prefix = self.num_prefix\n",
    "       sufix = self.num_sufix\n",
    "       for i in range(prefix - 1):\n",
    "          for j in range(i+1,prefix):\n",
    "             redundancy_x_row.append(i),reducey_x_col.append(j)\n",
    "       for i in range(sufix - 1):\n",
    "          for j in range(i + 1,sufix):\n",
    "             non_redundancy_x_row.append(i),non_redundancy_x_col.append(j)\n",
    "\n",
    "       for i in range(prefix):\n",
    "          for j in range(sufix):\n",
    "             mixed_x_row.append(i),mixed_x_col.append(j)\n",
    "       return (reducey_x[redundancy_x_row] * reducey_x[reducey_x_col]).unsqueeze(0),\\\n",
    "              non_redundancy_x[:,non_redundancy_x_row] * non_redundancy_x[:,non_redundancy_x_col],\\\n",
    "              reducey_x[mixed_x_row] * non_redundancy_x[:,mixed_x_col]\n",
    "\n",
    "    def pn(self,x):\n",
    "      redundancy_part_embed = x[redundancy_part_slice]\n",
    "      non_redundancy_part_embed = x[non_redundancy_part_slice]\n",
    "      redundancy_part_pn, non_redundancy_part_pn, mixed_part_pn = self.pn1(redundancy_part_embed,non_redundancy_part_embed)\n",
    "      redundancy_part_pn = redundancy_part_pn.repeat(batch,1,1)\n",
    "      \n",
    "      cross_term =  torch.concat([redundancy_part_pn,non_redundancy_part_pn,mixed_part_pn], dim = 1)\n",
    "      redundancy_part_pn = cross_term[0,:self.redundancy_cross_part_total]\n",
    "      non_redundancy_part_pn = cross_term[:,self.redundancy_cross_part_total:self.redundancy_cross_part_total + self.non_redundancy_cross_part_total]\n",
    "      mixed_part_pn = cross_term[:,self.redundancy_cross_part_total + self.non_redundancy_cross_part_total:]\n",
    "      return torch.sum(redundancy_part_pn.unsqueeze(0),dim = -1),\\\n",
    "              torch.sum(non_redundancy_part_pn,dim = -1),\\\n",
    "              torch.sum(mixed_part_pn,dim = -1)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "      x = self.embed(x + self.offsets)\n",
    "      redundancy_part_pn, non_redundancy_part_pn, mixed_part_pn = self.pn(x)\n",
    "      redundancy_part_pn = redundancy_part_pn.repeat(batch,1)\n",
    "      \n",
    "      cross_term =  torch.concat([redundancy_part_pn,non_redundancy_part_pn,mixed_part_pn], dim = 1)\n",
    "      # cross_term = torch.sum(cross_term, dim = 1)\n",
    "      x = torch.cat([x.view(-1, self.embed_output_dim), cross_term], dim=1)\n",
    "      return x\n",
    "    \n",
    "  \n",
    "  return pattern,ReplacementClass(),_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用优化的方法改写\n",
    "def gen_pattern_replace_and_matcher_for_optimize(traced,\n",
    "                                                  redundancy_part_slice,non_redundancy_part_slice,\n",
    "                                                  embed_node_name,getitem_node_names,num_field,batch = 4096,match_func = None\n",
    "                                                ):\n",
    "  from torch.fx.passes.utils.matcher_utils import SubgraphMatcher\n",
    "\n",
    "\n",
    "  def _match(match,ori,pat):\n",
    "    return True \n",
    "  env  = utils.get_env(traced)\n",
    "  target_node = env[embed_node_name]\n",
    "  target_node_mod = utils.get_target_mod(traced,target_node.target)\n",
    "  shape_info = target_node_mod.weight.data.shape\n",
    "  # getitem_node_args = [env[i].args[1] for i in getitem_node_names]\n",
    "  class PatternClass(torch.nn.Module):\n",
    "      def __init__(self):\n",
    "          super().__init__()\n",
    "          self.embed = torch.nn.Embedding(1, 1)\n",
    "          self.offsets = nn.Parameter(torch.as_tensor(np.array((0, *np.cumsum([10,10])[:-1]), dtype=np.int64)),False)\n",
    "          self.embed_output_dim = shape_info[1] * num_field\n",
    "          self.interaction_units = int(num_field * (num_field - 1) / 2)\n",
    "          self.triu_mask = nn.Parameter(torch.triu(torch.ones(num_field, num_field), 1).bool(),\n",
    "                                    requires_grad=False) \n",
    "          self.mlp = nn.Linear(shape_info[0],shape_info[1])\n",
    "\n",
    "      def pn(self,feature_emb):\n",
    "        inner_product_matrix = torch.bmm(feature_emb, feature_emb.transpose(1, 2))\n",
    "        triu_values = torch.masked_select(inner_product_matrix, self.triu_mask)\n",
    "        return triu_values.view(-1, self.interaction_units)\n",
    "\n",
    "      def forward(self,x):\n",
    "          x = self.embed(x + self.offsets)\n",
    "          cross_term = self.pn(x)\n",
    "          x = torch.cat([x.view(-1, self.embed_output_dim), cross_term], dim=1)\n",
    "          return x\n",
    "  pattern = PatternClass()  \n",
    "  pattern_trace = symbolic_trace(pattern)\n",
    "  pattern_graph = pattern_trace.graph\n",
    "  original_graph = traced.graph\n",
    "  matcher =  SubgraphMatcher(pattern_graph, match_output=False, match_placeholder=False,\n",
    "                              remove_overlapping_matches=True)\n",
    "  _matches = matcher.match(original_graph)\n",
    "  match_filters = [_match if match_func is None else match_func]\n",
    "  _matches = [\n",
    "      m for m in _matches\n",
    "      if all(match_filter(m, original_graph, pattern_graph)\n",
    "              for match_filter in match_filters)\n",
    "  ]  \n",
    "  # 因为在过滤器中做了限制应该只有一个符合要求的\n",
    "  _matched = _matches[0]\n",
    "  pattern_env = utils.get_env(pattern_trace)\n",
    "  node_map = _matched.nodes_map\n",
    "  pn = pattern_env['offsets']\n",
    "  offsets_node = node_map[pn]\n",
    "  offsets_val = utils.get_target_mod(traced,offsets_node.target)\n",
    "  \n",
    "  embed_node = node_map[pattern_env['embed']]\n",
    "  embed_node_module = utils.get_target_mod(traced,embed_node.target)\n",
    "  \n",
    "#   linear_node = node_map[pattern_env['mlp']]\n",
    "#   linear_node_module = utils.get_target_mod(traced,linear_node.target)\n",
    "#   linear_node_weight = linear_node_module.weight.data\n",
    "#   linear_node_bias = linear_node_module.bias.data\n",
    "  \n",
    "  class ReplacementClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.offsets = offsets_val\n",
    "      self.num_fields = num_field\n",
    "      self.num_prefix = redundancy_part_slice[1].stop\n",
    "      # self.ori_linear_shape = linear_node_weight.shape\n",
    "      self.num_sufix = self.num_fields - self.num_prefix\n",
    "      self.total = self.num_fields * (self.num_fields - 1) // 2\n",
    "      self.redundancy_cross_part_total = self.num_prefix * (self.num_prefix - 1) // 2\n",
    "      self.non_redundancy_cross_part_total = self.num_sufix * (self.num_sufix - 1) // 2\n",
    "      self.rest_cross_part_total = self.total - self.redundancy_cross_part_total - self.non_redundancy_cross_part_total\n",
    "      # 提取对应的权重参数逻辑有些复杂，先mock\n",
    "      self.embed = embed_node_module\n",
    "      self.embed_dim = self.embed.weight.data.shape[1]\n",
    "      self.embed_output_dim = self.embed_dim * self.num_fields\n",
    "      self.redundancy_mask = nn.Parameter(torch.triu(torch.ones(self.num_prefix, self.num_prefix), 1).bool(),\n",
    "                                    requires_grad=False)\n",
    "      self.non_redundancy_mask = nn.Parameter(torch.triu(torch.ones(self.num_sufix, self.num_sufix), 1).bool(),\n",
    "                                    requires_grad=False)\n",
    "      # self.ll = nn.Linear(linear_node_weight.shape[1],linear_node_weight.shape[0],bias = True)\n",
    "\n",
    "    def pn(self,redundancy_x,non_redundancy_x):\n",
    "       redundancy_x_row, reducey_x_col = list(),list()\n",
    "       non_redundancy_x_row, non_redundancy_x_col = list(), list()\n",
    "       mixed_x_row, mixed_x_col = list(), list()\n",
    "       prefix = self.num_prefix\n",
    "       sufix = self.num_sufix\n",
    "\n",
    "       redundancy_interaction_units = int(prefix * (prefix - 1) / 2)\n",
    "       non_redundancy_interaction_units = int(sufix * (sufix - 1) / 2)\n",
    "       mixed_interaction_units = int(prefix * sufix)\n",
    "       \n",
    "       non_redundancy_x_t = non_redundancy_x.transpose(1, 2)\n",
    "       \n",
    "       redundancy_inner_product_matrix = redundancy_x @ redundancy_x.transpose(-2,-1)\n",
    "       non_redundancy_inner_product_matrix = torch.bmm(non_redundancy_x, non_redundancy_x_t)\n",
    "       \n",
    "       mixed_redundancy_inner_product_matrix = redundancy_x @ non_redundancy_x_t\n",
    "       \n",
    "       redundancy_triu_values = torch.masked_select(redundancy_inner_product_matrix, self.redundancy_mask)\n",
    "       non_redundancy_triu_values = torch.masked_select(non_redundancy_inner_product_matrix,self.non_redundancy_mask)\n",
    "       \n",
    "       return redundancy_triu_values.view(-1,redundancy_interaction_units),\\\n",
    "              non_redundancy_triu_values.view(-1,non_redundancy_interaction_units),\\\n",
    "              mixed_redundancy_inner_product_matrix.view(-1,mixed_interaction_units)\n",
    "    def forward(self,x):\n",
    "      x = self.embed(x + self.offsets)\n",
    "      redundancy_part_embed = x[redundancy_part_slice]\n",
    "      non_redundancy_part_embed = x[non_redundancy_part_slice]\n",
    "      redundancy_part_pn, non_redundancy_part_pn, mixed_part_pn = self.pn(redundancy_part_embed,non_redundancy_part_embed)\n",
    "      redundancy_part_pn = redundancy_part_pn.repeat(batch,1)\n",
    "      \n",
    "      cross_term =  torch.concat([redundancy_part_pn,non_redundancy_part_pn,mixed_part_pn], dim = 1)\n",
    "      # cross_term = torch.sum(cross_term, dim = 2)\n",
    "      x = torch.cat([x.view(-1, self.embed_output_dim), cross_term], dim=1)\n",
    "      return x\n",
    "    \n",
    "  \n",
    "  return pattern,ReplacementClass(),_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 负载生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只修改哈达玛积\n",
    "def workload_pnn_mul(dim,redundancy,mlp = [400,400,400]):\n",
    "  print(f\"now gen workload of pnn with config: dim: {dim}\")\n",
    "  pnn_loop = pnn.ProductNeuralNetworkModel([100 for i in range(100)],dim,mlp,0.1)\n",
    "\n",
    "  pnn_model_traced_ori = symbolic_trace(pnn_loop)\n",
    "  \n",
    "  pnn_model_modify = pnn.ProductNeuralNetworkModel([100 for i in range(100)],dim,mlp,0.1)\n",
    "  pnn_model_traced_modify = symbolic_trace(pnn_model_modify)\n",
    "  pattern,replace,match = gen_pattern_replace_and_matcher_for_loop_pnn_mul(pnn_model_traced_modify,\n",
    "                                                                      (0,slice(None,redundancy,None)),(slice(None,None,None),slice(redundancy,None,None)),\n",
    "                                                                      embed_node_name = \"embedding_embedding\",\n",
    "                                                                      getitem_node_names = [\"getitem\",\"getitem_1\"],num_field=100)\n",
    "  matches = subgraph_rewriter.replace_pattern_with_filters(pnn_model_traced_modify, pattern, replace,[match])\n",
    "  return pnn_model_traced_ori,pnn_model_traced_modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 联合修改哈达玛积和reduce\n",
    "def workload_pnn_mul_reduce(dim,redundancy,mlp = [400,400,400]):\n",
    "  print(f\"now gen workload of pnn with config: dim: {dim}\")\n",
    "  pnn_loop = pnn.ProductNeuralNetworkModel([100 for i in range(100)],dim,mlp,0.1)\n",
    "\n",
    "  pnn_model_traced_ori = symbolic_trace(pnn_loop)\n",
    "  \n",
    "  pnn_model_modify = pnn.ProductNeuralNetworkModel([100 for i in range(100)],dim,mlp,0.1)\n",
    "  pnn_model_traced_modify = symbolic_trace(pnn_model_modify)\n",
    "  pattern,replace,match = gen_pattern_replace_and_matcher_for_loop_pnn_mul_reduce(pnn_model_traced_modify,\n",
    "                                                                      (0,slice(None,redundancy,None)),(slice(None,None,None),slice(redundancy,None,None)),\n",
    "                                                                      embed_node_name = \"embedding_embedding\",\n",
    "                                                                      getitem_node_names = [\"getitem\",\"getitem_1\"],num_field=100)\n",
    "  matches = subgraph_rewriter.replace_pattern_with_filters(pnn_model_traced_modify, pattern, replace,[match])\n",
    "  return pnn_model_traced_ori,pnn_model_traced_modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只修改reduce\n",
    "def workload_pnn_reduce(dim,redundancy,mlp = [400,400,400]):\n",
    "  print(f\"now gen workload of pnn with config: dim: {dim}\")\n",
    "  pnn_loop = pnn.ProductNeuralNetworkModel([100 for i in range(100)],dim,mlp,0.1)\n",
    "\n",
    "  pnn_model_traced_ori = symbolic_trace(pnn_loop)\n",
    "  \n",
    "  pnn_model_modify = pnn.ProductNeuralNetworkModel([100 for i in range(100)],dim,mlp,0.1)\n",
    "  pnn_model_traced_modify = symbolic_trace(pnn_model_modify)\n",
    "  pattern,replace,match = gen_pattern_replace_and_matcher_for_loop_pnn_reduce(pnn_model_traced_modify,\n",
    "                                                                      (0,slice(None,redundancy,None)),(slice(None,None,None),slice(redundancy,None,None)),\n",
    "                                                                      embed_node_name = \"embedding_embedding\",\n",
    "                                                                      getitem_node_names = [\"getitem\",\"getitem_1\"],num_field=100)\n",
    "  matches = subgraph_rewriter.replace_pattern_with_filters(pnn_model_traced_modify, pattern, replace,[match])\n",
    "  return pnn_model_traced_ori,pnn_model_traced_modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改全部模型\n",
    "def workload_pnn(dim,redundancy,mlp = [400,400,400]):\n",
    "  print(f\"now gen workload of pnn with config: dim: {dim}\")\n",
    "  pnn_loop = pnn.ProductNeuralNetworkModel([100 for i in range(100)],dim,mlp,0.1)\n",
    "\n",
    "  pnn_model_traced_ori = symbolic_trace(pnn_loop)\n",
    "  \n",
    "  pnn_model_modify = pnn.ProductNeuralNetworkModel([100 for i in range(100)],dim,mlp,0.1)\n",
    "  pnn_model_traced_modify = symbolic_trace(pnn_model_modify)\n",
    "  pattern,replace,match = gen_pattern_replace_and_matcher_for_loop_pnn(pnn_model_traced_modify,\n",
    "                                                                      (0,slice(None,redundancy,None)),(slice(None,None,None),slice(redundancy,None,None)),\n",
    "                                                                      embed_node_name = \"embedding_embedding\",\n",
    "                                                                      getitem_node_names = [\"getitem\",\"getitem_1\"],num_field=100)\n",
    "  matches = subgraph_rewriter.replace_pattern_with_filters(pnn_model_traced_modify, pattern, replace,[match])\n",
    "  return pnn_model_traced_ori,pnn_model_traced_modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只修改linear\n",
    "def workload_pnn_linear(dim,redundancy,mlps = [400,400,400]):\n",
    "  print(f\"now gen workload of pnn with config: dim: {dim}\")\n",
    "  pnn_loop = pnn.ProductNeuralNetworkModel([100 for i in range(100)],dim,mlps,0.1)\n",
    "\n",
    "  pnn_model_traced_ori = symbolic_trace(pnn_loop)\n",
    "  \n",
    "  pnn_model_modify = pnn.ProductNeuralNetworkModel([100 for i in range(100)],dim,mlps,0.1)\n",
    "  pnn_model_traced_modify = symbolic_trace(pnn_model_modify)\n",
    "  pattern,replace,match = gen_pattern_replace_and_matcher_for_loop_pnn_linear(pnn_model_traced_modify,\n",
    "                                                                      (0,slice(None,redundancy,None)),(slice(None,None,None),slice(redundancy,None,None)),\n",
    "                                                                      embed_node_name = \"embedding_embedding\",\n",
    "                                                                      getitem_node_names = [\"getitem\",\"getitem_1\"],num_field=100)\n",
    "  matches = subgraph_rewriter.replace_pattern_with_filters(pnn_model_traced_modify, pattern, replace,[match])\n",
    "  return pnn_model_traced_ori,pnn_model_traced_modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试对mul算子和reduce算子做单独改写的效果\n",
    "\n",
    "def workload_pnn_single_mul_reduce(dim,redundancy,mlps = [400,400,400]):\n",
    "  print(f\"now gen workload of pnn with config: dim: {dim}\")\n",
    "  pnn_loop = pnn.ProductNeuralNetworkModel([100 for i in range(100)],dim,mlps,0.1)\n",
    "\n",
    "  pnn_model_traced_ori = symbolic_trace(pnn_loop)\n",
    "  \n",
    "  pnn_model_modify = pnn.ProductNeuralNetworkModel([100 for i in range(100)],dim,mlps,0.1)\n",
    "  pnn_model_traced_modify = symbolic_trace(pnn_model_modify)\n",
    "  pattern,replace,match = gen_pattern_replace_and_matcher_for_loop_pnn_single_mul_reduce(pnn_model_traced_modify,\n",
    "                                                                      (0,slice(None,redundancy,None)),(slice(None,None,None),slice(redundancy,None,None)),\n",
    "                                                                      embed_node_name = \"embedding_embedding\",\n",
    "                                                                      getitem_node_names = [\"getitem\",\"getitem_1\"],num_field=100)\n",
    "  matches = subgraph_rewriter.replace_pattern_with_filters(pnn_model_traced_modify, pattern, replace,[match])\n",
    "  return pnn_model_traced_ori,pnn_model_traced_modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试对mul算子和reduce算子做单独改写的效果\n",
    "\n",
    "def workload_pnn_optimize(dim,redundancy,mlps = [400,400,400]):\n",
    "  print(f\"now gen workload of pnn with config: dim: {dim}\")\n",
    "  pnn_loop = pnn.ProductNeuralNetworkModel([100 for i in range(100)],dim,mlps,0.1,\"innerV2\")\n",
    "\n",
    "  pnn_model_traced_ori = symbolic_trace(pnn_loop)\n",
    "  \n",
    "  pnn_model_modify = pnn.ProductNeuralNetworkModel([100 for i in range(100)],dim,mlps,0.1,\"innerV2\")\n",
    "  pnn_model_traced_modify = symbolic_trace(pnn_model_modify)\n",
    "  pattern,replace,match = gen_pattern_replace_and_matcher_for_optimize(pnn_model_traced_modify,\n",
    "                                                                      (0,slice(None,redundancy,None)),(slice(None,None,None),slice(redundancy,None,None)),\n",
    "                                                                      embed_node_name = \"embedding_embedding\",\n",
    "                                                                      getitem_node_names = [\"getitem\",\"getitem_1\"],num_field=100)\n",
    "  matches = subgraph_rewriter.replace_pattern_with_filters(pnn_model_traced_modify, pattern, replace,[match])\n",
    "  return pnn_model_traced_ori,pnn_model_traced_modify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of pnn with config: dim: 128\n"
     ]
    }
   ],
   "source": [
    "ori , modify = workload_pnn_mul(128,80,[1000,400,400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.18 s ± 78.4 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():ori(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.81 s ± 27.1 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of pnn with config: dim: 128\n"
     ]
    }
   ],
   "source": [
    "ori , modify = workload_pnn_reduce(128,80,[1000,400,400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.19 s ± 46.2 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of pnn with config: dim: 128\n"
     ]
    }
   ],
   "source": [
    "ori , modify = workload_pnn_linear(128,80,[1000,400,400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.18 s ± 39.2 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of pnn with config: dim: 128\n"
     ]
    }
   ],
   "source": [
    "ori , modify = workload_pnn_mul_reduce(128,80,[1000,400,400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 s ± 32.1 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of pnn with config: dim: 128\n"
     ]
    }
   ],
   "source": [
    "ori , modify = workload_pnn_mul_reduce(128,40,[1000,400,400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.34 s ± 47.2 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of pnn with config: dim: 128\n"
     ]
    }
   ],
   "source": [
    "ori , modify = workload_pnn_mul_reduce(128,20,[1000,400,400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.79 s ± 18.6 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of pnn with config: dim: 128\n"
     ]
    }
   ],
   "source": [
    "ori , modify = workload_pnn(128,80,[1000,400,400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994 ms ± 22.2 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of pnn with config: dim: 128\n"
     ]
    }
   ],
   "source": [
    "ori , modify = workload_pnn(128,40,[1000,400,400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3 s ± 32.3 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of pnn with config: dim: 128\n"
     ]
    }
   ],
   "source": [
    "ori , modify = workload_pnn(128,20,[1000,400,400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.76 s ± 28.5 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全部改写和只改写关键算子以后性能差距不大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProductNeuralNetworkModel(\n",
       "  (mlp): Module(\n",
       "    (mlp): Module(\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): Linear(in_features=1000, out_features=400, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=400, out_features=400, bias=True)\n",
       "      (7): ReLU()\n",
       "      (8): Dropout(p=0.1, inplace=False)\n",
       "      (9): Linear(in_features=400, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (embed): Embedding(10000, 128)\n",
       "  (redency_linear): Linear(in_features=10240, out_features=1000, bias=False)\n",
       "  (unredency_linear): Linear(in_features=2560, out_features=1000, bias=False)\n",
       "  (redency_cross_part_linear): Linear(in_features=3160, out_features=1000, bias=True)\n",
       "  (mixed_cross_part_linear): Linear(in_features=1600, out_features=1000, bias=False)\n",
       "  (unredency_cross_part_linear): Linear(in_features=190, out_features=1000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori.to(device)\n",
    "modify.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 5.82 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "102 ms ± 15.6 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long).to(device)\n",
    "%timeit -n 1 -r 30 with torch.no_grad():ori(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 4.29 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "27.2 ms ± 3.84 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long).to(device)\n",
    "%timeit -n 1 -r 30 with torch.no_grad():modify(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of pnn with config: dim: 128\n"
     ]
    }
   ],
   "source": [
    "ori , modify = workload_pnn(128,30,[1000,400,400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProductNeuralNetworkModel(\n",
       "  (mlp): Module(\n",
       "    (mlp): Module(\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): Linear(in_features=1000, out_features=400, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=400, out_features=400, bias=True)\n",
       "      (7): ReLU()\n",
       "      (8): Dropout(p=0.1, inplace=False)\n",
       "      (9): Linear(in_features=400, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (embed): Embedding(10000, 128)\n",
       "  (redency_linear): Linear(in_features=3840, out_features=1000, bias=False)\n",
       "  (unredency_linear): Linear(in_features=8960, out_features=1000, bias=False)\n",
       "  (redency_cross_part_linear): Linear(in_features=435, out_features=1000, bias=True)\n",
       "  (mixed_cross_part_linear): Linear(in_features=2100, out_features=1000, bias=False)\n",
       "  (unredency_cross_part_linear): Linear(in_features=2415, out_features=1000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori.to(device)\n",
    "modify.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 5.49 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "102 ms ± 15.3 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long).to(device)\n",
    "%timeit -n 1 -r 30 with torch.no_grad():ori(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.6 ms ± 5.71 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long).to(device)\n",
    "%timeit -n 1 -r 30 with torch.no_grad():modify(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of pnn with config: dim: 32\n"
     ]
    }
   ],
   "source": [
    "ori , modify = workload_pnn(32,30,[1000,400,400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProductNeuralNetworkModel(\n",
       "  (mlp): Module(\n",
       "    (mlp): Module(\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): Linear(in_features=1000, out_features=400, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=400, out_features=400, bias=True)\n",
       "      (7): ReLU()\n",
       "      (8): Dropout(p=0.1, inplace=False)\n",
       "      (9): Linear(in_features=400, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (embed): Embedding(10000, 32)\n",
       "  (redency_linear): Linear(in_features=960, out_features=1000, bias=False)\n",
       "  (unredency_linear): Linear(in_features=2240, out_features=1000, bias=False)\n",
       "  (redency_cross_part_linear): Linear(in_features=435, out_features=1000, bias=True)\n",
       "  (mixed_cross_part_linear): Linear(in_features=2100, out_features=1000, bias=False)\n",
       "  (unredency_cross_part_linear): Linear(in_features=2415, out_features=1000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori.to(device)\n",
    "modify.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 5.44 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "31.3 ms ± 4.7 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long).to(device)\n",
    "%timeit -n 1 -r 30 with torch.no_grad():ori(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 ms ± 1.8 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long).to(device)\n",
    "%timeit -n 1 -r 30 with torch.no_grad():modify(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of pnn with config: dim: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node offsets target offsets offsets of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori , modify = workload_pnn_single_mul_reduce(128,80,[1000,400,400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.59 s ± 21.5 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of pnn with config: dim: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node offsets target offsets offsets of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori , modify = workload_pnn_optimize(128,80,[1000,400,400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    ori,  # 模型实例\n",
    "    torch.randint(low=0, high=88, size=(4096,100)),  # 示例输入（假设是图像数据）\n",
    "    'pnn_opt_ori.onnx',  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load('pnn_opt_ori.onnx')\n",
    "model_simplified, check = onnxsim.simplify(onnx_model)\n",
    "onnx.save(model_simplified, 'pnn_opt_ori_smi.onnx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = ort.InferenceSession('pnn_opt_ori_smi.onnx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198 ms ± 25.8 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 session.run(None,{'onnx::Add_0' : np.random.randint(low=0, high=88, size=(4096, 100), dtype=np.int64)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "session = ort.InferenceSession('pnn_opt_ori')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196 ms ± 21.9 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 session.run(None,{'onnx::Add_0' : np.random.randint(low=0, high=88, size=(4096, 100), dtype=np.int64)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418 ms ± 8.76 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():ori(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    modify,  # 模型实例\n",
    "    torch.randint(low=0, high=88, size=(4096,100)),  # 示例输入（假设是图像数据）\n",
    "    'pnn_opt_modify.onnx',  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load('pnn_opt_modify.onnx')\n",
    "model_simplified, check = onnxsim.simplify(onnx_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx.save(model_simplified, 'pnn_opt_modify_sim.onnx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = ort.InferenceSession('pnn_opt_modify_sim.onnx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115 ms ± 14.7 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 session.run(None,{'onnx::Add_0' : np.random.randint(low=0, high=88, size=(4096, 100), dtype=np.int64)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154 ms ± 9.06 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad():modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(low=0, high=88, size=(4096, 100), dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186 ms ± 2.12 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 session.run(None,{'onnx::Add_0' : x})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepctr-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
