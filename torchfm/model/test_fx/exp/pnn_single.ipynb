{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../')\n",
    "import time\n",
    "import pnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.fx import subgraph_rewriter, symbolic_trace\n",
    "import utils\n",
    "from torch.fx import Proxy, Graph, GraphModule\n",
    "from torch.fx.passes.utils.matcher_utils import SubgraphMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import time\n",
    "import torch._dynamo as dynamo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_and_variance_manual(data):\n",
    "    n = len(data)\n",
    "    mean = sum(data) / n\n",
    "    variance = sum((x - mean) ** 2 for x in data) / n\n",
    "    return mean, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pattern_replace_and_matcher_for_loop_pnn(traced,\n",
    "                                                  redundancy_part_slice,non_redundancy_part_slice,\n",
    "                                                  embed_node_name,getitem_node_names,num_field,batch = 4096,match_func = None\n",
    "                                                ):\n",
    "  from torch.fx.passes.utils.matcher_utils import SubgraphMatcher\n",
    "\n",
    "\n",
    "  def _match(match,ori,pat):\n",
    "    return True \n",
    "  env  = utils.get_env(traced)\n",
    "  target_node = env[embed_node_name]\n",
    "  target_node_mod = utils.get_target_mod(traced,target_node.target)\n",
    "  shape_info = target_node_mod.weight.data.shape\n",
    "  getitem_node_args = [env[i].args[1] for i in getitem_node_names]\n",
    "  class PatternClass(torch.nn.Module):\n",
    "      def __init__(self):\n",
    "          super().__init__()\n",
    "          \n",
    "      def pn(self,x):\n",
    "         return torch.sum(x[getitem_node_args[0]] * x[getitem_node_args[1]], dim = 2)\n",
    "\n",
    "      def forward(self,x):\n",
    "          return self.pn(x)\n",
    "  pattern = PatternClass()  \n",
    "  pattern_trace = symbolic_trace(pattern)\n",
    "  pattern_graph = pattern_trace.graph\n",
    "  original_graph = traced.graph\n",
    "  matcher =  SubgraphMatcher(pattern_graph, match_output=False, match_placeholder=False,\n",
    "                              remove_overlapping_matches=True)\n",
    "  _matches = matcher.match(original_graph)\n",
    "  match_filters = [_match if match_func is None else match_func]\n",
    "  _matches = [\n",
    "      m for m in _matches\n",
    "      if all(match_filter(m, original_graph, pattern_graph)\n",
    "              for match_filter in match_filters)\n",
    "  ]  \n",
    "  # 因为在过滤器中做了限制应该只有一个符合要求的\n",
    "  _matched = _matches[0]\n",
    "  pattern_env = utils.get_env(pattern_trace)\n",
    "  node_map = _matched.nodes_map\n",
    "  \n",
    "  \n",
    "#   linear_node = node_map[pattern_env['mlp']]\n",
    "#   linear_node_module = utils.get_target_mod(traced,linear_node.target)\n",
    "#   linear_node_weight = linear_node_module.weight.data\n",
    "#   linear_node_bias = linear_node_module.bias.data\n",
    "  \n",
    "  class ReplacementClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.num_fields = num_field\n",
    "      self.num_prefix = redundancy_part_slice[1].stop\n",
    "      # self.ori_linear_shape = linear_node_weight.shape\n",
    "      self.num_sufix = self.num_fields - self.num_prefix\n",
    "      self.total = self.num_fields * (self.num_fields - 1) // 2\n",
    "      self.redundancy_cross_part_total = self.num_prefix * (self.num_prefix - 1) // 2\n",
    "      self.non_redundancy_cross_part_total = self.num_sufix * (self.num_sufix - 1) // 2\n",
    "      self.rest_cross_part_total = self.total - self.redundancy_cross_part_total - self.non_redundancy_cross_part_total\n",
    "\n",
    "    def pn(self,reducey_x,non_redundancy_x):\n",
    "       redundancy_x_row, reducey_x_col = list(),list()\n",
    "       non_redundancy_x_row, non_redundancy_x_col = list(), list()\n",
    "       mixed_x_row, mixed_x_col = list(), list()\n",
    "       prefix = self.num_prefix\n",
    "       sufix = self.num_sufix\n",
    "       for i in range(prefix - 1):\n",
    "          for j in range(i+1,prefix):\n",
    "             redundancy_x_row.append(i),reducey_x_col.append(j)\n",
    "       for i in range(sufix - 1):\n",
    "          for j in range(i + 1,sufix):\n",
    "             non_redundancy_x_row.append(i),non_redundancy_x_col.append(j)\n",
    "\n",
    "       for i in range(prefix):\n",
    "          for j in range(sufix):\n",
    "             mixed_x_row.append(i),mixed_x_col.append(j)\n",
    "       return torch.sum((reducey_x[redundancy_x_row] * reducey_x[reducey_x_col]).unsqueeze(0),dim = -1),\\\n",
    "              torch.sum(non_redundancy_x[:,non_redundancy_x_row] * non_redundancy_x[:,non_redundancy_x_col],dim= -1),\\\n",
    "              torch.sum(reducey_x[mixed_x_row] * non_redundancy_x[:,mixed_x_col],dim = -1)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "      redundancy_part_embed = x[redundancy_part_slice]\n",
    "      non_redundancy_part_embed = x[non_redundancy_part_slice]\n",
    "      redundancy_part_pn, non_redundancy_part_pn, mixed_part_pn = self.pn(redundancy_part_embed,non_redundancy_part_embed)\n",
    "      \n",
    "      redundancy_part_pn = redundancy_part_pn.repeat(batch,1)\n",
    "      cross_term =  torch.concat([redundancy_part_pn,non_redundancy_part_pn,mixed_part_pn], dim = 1)\n",
    "\n",
    "      return cross_term\n",
    "    \n",
    "  \n",
    "  return pattern,ReplacementClass(),_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只修改哈达玛积\n",
    "def workload_pnn(num_field, prefix,dim = 64,l = [1024,512,256],batch = 4096):\n",
    "  print(f\"now gen workload of PNN with config: dim: {dim}, num_field: {num_field}, prefix: {prefix}\")\n",
    "  pnn_loop = pnn.ProductNeuralNetworkModel([100 for i in range(num_field)],dim,l,0.1)\n",
    "\n",
    "  pnn_model_traced_ori = symbolic_trace(pnn_loop)\n",
    "  \n",
    "  pnn_model_modify = pnn.ProductNeuralNetworkModel([100 for i in range(num_field)],dim,l,0.1)\n",
    "  pnn_model_traced_modify = symbolic_trace(pnn_model_modify)\n",
    "  pattern,replace,match = gen_pattern_replace_and_matcher_for_loop_pnn(pnn_model_traced_modify,\n",
    "                                                                      (0,slice(None,prefix,None)),(slice(None,None,None),slice(prefix,None,None)),\n",
    "                                                                      embed_node_name = \"embedding_embedding\",\n",
    "                                                                      getitem_node_names = [\"getitem\",\"getitem_1\"],num_field=num_field,batch = batch)\n",
    "  matches = subgraph_rewriter.replace_pattern_with_filters(pnn_model_traced_modify, pattern, replace,[match])\n",
    "  return pnn_model_traced_ori,pnn_model_traced_modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_and_test(num_field = 22,prefix = 10, batch = 4096, dim = 32, workload_func = workload_pnn,l = [1024,512,256]):\n",
    "  def run(model):\n",
    "    traced_model = torch.jit.trace(model, torch.randint(low=0, high=88, size=(batch,num_field), dtype=torch.long))\n",
    "    compiled_model = torch.compile(traced_model, backend=\"inductor\")\n",
    "    compiled_model.eval()\n",
    "    total_time = []\n",
    "    t = torch.randint(low=0, high=88, size=(batch ,num_field), dtype=torch.long)\n",
    "\n",
    "    with profile(activities=[ProfilerActivity.CPU],     record_shapes=True,  # 记录操作输入和输出的形状\n",
    "    # profile_memory=True,  # 记录内存使用情况\n",
    "    # with_stack=True,  # 记录代码堆栈信息\n",
    "    # with_flops=True,  # 记录FLOPs\n",
    "    # on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')  \n",
    "    ) as prof:\n",
    "        with record_function(\"model_inference\"):\n",
    "          for i in range(1):\n",
    "              start_time = time.time()  # 开始计时\n",
    "              with torch.no_grad():\n",
    "                soutput = compiled_model(t)\n",
    "              end_time = time.time()  # 结束计时\n",
    "              \n",
    "              # 计算并打印函数执行所需的时间\n",
    "              elapsed_time = end_time - start_time\n",
    "              total_time.append(elapsed_time * 1000)\n",
    "    print(calculate_mean_and_variance_manual(total_time))\n",
    "    return prof\n",
    "  print(f\"now gen workload of DFM with config: dim: {dim}, num_field: {num_field}, prefix: {prefix}, batch :{batch}\")\n",
    "  ori, modify = workload_func(num_field,prefix,dim,l,batch = batch)\n",
    "  p1 = run(ori)\n",
    "  p2 = run(modify)\n",
    "  return p1, p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pattern_replace_and_matcher_for_loop_pnn_getItem(traced,\n",
    "                                                  redundancy_part_slice,non_redundancy_part_slice,\n",
    "                                                  embed_node_name,getitem_node_names,num_field,batch = 4096,match_func = None\n",
    "                                                ):\n",
    "  from torch.fx.passes.utils.matcher_utils import SubgraphMatcher\n",
    "\n",
    "\n",
    "  def _match(match,ori,pat):\n",
    "    return True \n",
    "  env  = utils.get_env(traced)\n",
    "  target_node = env[embed_node_name]\n",
    "  target_node_mod = utils.get_target_mod(traced,target_node.target)\n",
    "  shape_info = target_node_mod.weight.data.shape\n",
    "  getitem_node_args = [env[i].args[1] for i in getitem_node_names]\n",
    "  class PatternClass(torch.nn.Module):\n",
    "      def __init__(self):\n",
    "          super().__init__()\n",
    "\n",
    "      def pn(self,x):\n",
    "         return torch.sum(x[getitem_node_args[0]] * x[getitem_node_args[1]], dim = 2)\n",
    "\n",
    "      def forward(self,x):\n",
    "          return self.pn(x)\n",
    "  pattern = PatternClass()  \n",
    "  pattern_trace = symbolic_trace(pattern)\n",
    "  pattern_graph = pattern_trace.graph\n",
    "  original_graph = traced.graph\n",
    "  matcher =  SubgraphMatcher(pattern_graph, match_output=False, match_placeholder=False,\n",
    "                              remove_overlapping_matches=True)\n",
    "  _matches = matcher.match(original_graph)\n",
    "  match_filters = [_match if match_func is None else match_func]\n",
    "  _matches = [\n",
    "      m for m in _matches\n",
    "      if all(match_filter(m, original_graph, pattern_graph)\n",
    "              for match_filter in match_filters)\n",
    "  ]  \n",
    "  # 因为在过滤器中做了限制应该只有一个符合要求的\n",
    "  _matched = _matches[0]\n",
    "  pattern_env = utils.get_env(pattern_trace)\n",
    "  node_map = _matched.nodes_map\n",
    "  \n",
    "  \n",
    "#   linear_node = node_map[pattern_env['mlp']]\n",
    "#   linear_node_module = utils.get_target_mod(traced,linear_node.target)\n",
    "#   linear_node_weight = linear_node_module.weight.data\n",
    "#   linear_node_bias = linear_node_module.bias.data\n",
    "  \n",
    "  class ReplacementClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.num_fields = num_field\n",
    "      self.num_prefix = redundancy_part_slice[1].stop\n",
    "      # self.ori_linear_shape = linear_node_weight.shape\n",
    "      self.num_sufix = self.num_fields - self.num_prefix\n",
    "      self.total = self.num_fields * (self.num_fields - 1) // 2\n",
    "      self.redundancy_cross_part_total = self.num_prefix * (self.num_prefix - 1) // 2\n",
    "      self.non_redundancy_cross_part_total = self.num_sufix * (self.num_sufix - 1) // 2\n",
    "      self.rest_cross_part_total = self.total - self.redundancy_cross_part_total - self.non_redundancy_cross_part_total\n",
    "\n",
    "    def pn(self,embed):\n",
    "      redundancy_x_row, reducey_x_col = list(),list()\n",
    "      non_redundancy_x_row, non_redundancy_x_col = list(), list()\n",
    "      mixed_x_row, mixed_x_col = list(), list()\n",
    "      prefix = self.num_prefix\n",
    "      sufix = self.num_sufix\n",
    "      for i in range(prefix - 1):\n",
    "        for j in range(i+1,prefix):\n",
    "            redundancy_x_row.append(i),reducey_x_col.append(j)\n",
    "      for i in range(sufix - 1):\n",
    "        for j in range(i + 1,sufix):\n",
    "            non_redundancy_x_row.append(i),non_redundancy_x_col.append(j)\n",
    "      for i in range(prefix):\n",
    "        for j in range(sufix):\n",
    "            mixed_x_row.append(i),mixed_x_col.append(j)\n",
    "      redundancy_x_row = redundancy_x_row + mixed_x_row\n",
    "      \n",
    "      other_x_col = reducey_x_col + non_redundancy_x_col + mixed_x_col\n",
    "      redundancy_emb = embed[0,:]\n",
    "      redundancy_emb = redundancy_emb[redundancy_x_row]\n",
    "      other_emb = embed[:,other_x_col]\n",
    "      this_emb = torch.concat([redundancy_emb.repeat(batch,1,1),embed[:,non_redundancy_x_row]],dim = 1)\n",
    "      return torch.sum(this_emb * other_emb,dim = -1)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "      return self.pn(x)\n",
    "    \n",
    "  \n",
    "  return pattern,ReplacementClass(),_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def workload_pnn_getItem(num_field, prefix,dim = 64,l = [1024,512,256],batch = 4096):\n",
    "  print(f\"now gen workload of PNN with config: dim: {dim}, num_field: {num_field}, prefix: {prefix}\")\n",
    "  pnn_loop = pnn.ProductNeuralNetworkModel([100 for i in range(num_field)],dim,l,0.1)\n",
    "\n",
    "  pnn_model_traced_ori = symbolic_trace(pnn_loop)\n",
    "  \n",
    "  pnn_model_modify = pnn.ProductNeuralNetworkModel([100 for i in range(num_field)],dim,l,0.1)\n",
    "  pnn_model_traced_modify = symbolic_trace(pnn_model_modify)\n",
    "  pattern,replace,match = gen_pattern_replace_and_matcher_for_loop_pnn_getItem(pnn_model_traced_modify,\n",
    "                                                                      (0,slice(None,prefix,None)),(slice(None,None,None),slice(prefix,None,None)),\n",
    "                                                                      embed_node_name = \"embedding_embedding\",\n",
    "                                                                      getitem_node_names = [\"getitem\",\"getitem_1\"],num_field=num_field,batch = batch)\n",
    "  matches = subgraph_rewriter.replace_pattern_with_filters(pnn_model_traced_modify, pattern, replace,[match])\n",
    "  return pnn_model_traced_ori,pnn_model_traced_modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pattern_replace_and_matcher_for_loop_pnn_mul(traced,\n",
    "                                                  redundancy_part_slice,non_redundancy_part_slice,\n",
    "                                                  embed_node_name,getitem_node_names,num_field,batch = 4096,match_func = None\n",
    "                                                ):\n",
    "  from torch.fx.passes.utils.matcher_utils import SubgraphMatcher\n",
    "\n",
    "\n",
    "  def _match(match,ori,pat):\n",
    "    return True \n",
    "  env  = utils.get_env(traced)\n",
    "  target_node = env[embed_node_name]\n",
    "  target_node_mod = utils.get_target_mod(traced,target_node.target)\n",
    "  shape_info = target_node_mod.weight.data.shape\n",
    "  getitem_node_args = [env[i].args[1] for i in getitem_node_names]\n",
    "  class PatternClass(torch.nn.Module):\n",
    "      def __init__(self):\n",
    "          super().__init__()\n",
    "\n",
    "      def pn(self,x):\n",
    "         return torch.sum(x[getitem_node_args[0]] * x[getitem_node_args[1]], dim = 2)\n",
    "\n",
    "      def forward(self,x):\n",
    "          return self.pn(x)\n",
    "  pattern = PatternClass()  \n",
    "  pattern_trace = symbolic_trace(pattern)\n",
    "  pattern_graph = pattern_trace.graph\n",
    "  original_graph = traced.graph\n",
    "  matcher =  SubgraphMatcher(pattern_graph, match_output=False, match_placeholder=False,\n",
    "                              remove_overlapping_matches=True)\n",
    "  _matches = matcher.match(original_graph)\n",
    "  match_filters = [_match if match_func is None else match_func]\n",
    "  _matches = [\n",
    "      m for m in _matches\n",
    "      if all(match_filter(m, original_graph, pattern_graph)\n",
    "              for match_filter in match_filters)\n",
    "  ]  \n",
    "  # 因为在过滤器中做了限制应该只有一个符合要求的\n",
    "  _matched = _matches[0]\n",
    "  pattern_env = utils.get_env(pattern_trace)\n",
    "  node_map = _matched.nodes_map\n",
    "  \n",
    "  \n",
    "#   linear_node = node_map[pattern_env['mlp']]\n",
    "#   linear_node_module = utils.get_target_mod(traced,linear_node.target)\n",
    "#   linear_node_weight = linear_node_module.weight.data\n",
    "#   linear_node_bias = linear_node_module.bias.data\n",
    "  \n",
    "  class ReplacementClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.num_fields = num_field\n",
    "      self.num_prefix = redundancy_part_slice[1].stop\n",
    "      # self.ori_linear_shape = linear_node_weight.shape\n",
    "      self.num_sufix = self.num_fields - self.num_prefix\n",
    "      self.total = self.num_fields * (self.num_fields - 1) // 2\n",
    "      self.redundancy_cross_part_total = self.num_prefix * (self.num_prefix - 1) // 2\n",
    "      self.non_redundancy_cross_part_total = self.num_sufix * (self.num_sufix - 1) // 2\n",
    "      self.rest_cross_part_total = self.total - self.redundancy_cross_part_total - self.non_redundancy_cross_part_total\n",
    "\n",
    "    def pn(self,embed):\n",
    "      row_emb = embed[getitem_node_args[0]]\n",
    "      col_emb = embed[getitem_node_args[1]]\n",
    "      redundancy_x_row, reducey_x_col = list(),list()\n",
    "      non_redundancy_x_row, non_redundancy_x_col = list(), list()\n",
    "      mixed_x_row, mixed_x_col = list(), list()\n",
    "      prefix = self.num_prefix\n",
    "      sufix = self.num_sufix\n",
    "      for i in range(prefix - 1):\n",
    "        for j in range(i+1,prefix):\n",
    "            redundancy_x_row.append(i),reducey_x_col.append(j)\n",
    "      for i in range(sufix - 1):\n",
    "        for j in range(i + 1,sufix):\n",
    "            non_redundancy_x_row.append(i),non_redundancy_x_col.append(j)\n",
    "      for i in range(prefix):\n",
    "        for j in range(sufix):\n",
    "            mixed_x_row.append(i),mixed_x_col.append(j)\n",
    "      redundancy_x_product = row_emb[0,redundancy_x_row] * col_emb[0,reducey_x_col]\n",
    "      non_redundancy_x_product = row_emb[:,(non_redundancy_x_row + mixed_x_row)] * col_emb[:,(non_redundancy_x_col + mixed_x_col)]\n",
    "      pn = torch.concat([redundancy_x_product.repeat(batch,1,1),non_redundancy_x_product],dim = 1)\n",
    "      return torch.sum(pn,dim = -1)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "      return self.pn(x)\n",
    "    \n",
    "  \n",
    "  return pattern,ReplacementClass(),_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def workload_pnn_mul(num_field, prefix,dim = 64,l = [1024,512,256],batch = 4096):\n",
    "  print(f\"now gen workload of PNN with config: dim: {dim}, num_field: {num_field}, prefix: {prefix}\")\n",
    "  pnn_loop = pnn.ProductNeuralNetworkModel([100 for i in range(num_field)],dim,l,0.1)\n",
    "\n",
    "  pnn_model_traced_ori = symbolic_trace(pnn_loop)\n",
    "  \n",
    "  pnn_model_modify = pnn.ProductNeuralNetworkModel([100 for i in range(num_field)],dim,l,0.1)\n",
    "  pnn_model_traced_modify = symbolic_trace(pnn_model_modify)\n",
    "  pattern,replace,match = gen_pattern_replace_and_matcher_for_loop_pnn_mul(pnn_model_traced_modify,\n",
    "                                                                      (0,slice(None,prefix,None)),(slice(None,None,None),slice(prefix,None,None)),\n",
    "                                                                      embed_node_name = \"embedding_embedding\",\n",
    "                                                                      getitem_node_names = [\"getitem\",\"getitem_1\"],num_field=num_field,batch = batch)\n",
    "  matches = subgraph_rewriter.replace_pattern_with_filters(pnn_model_traced_modify, pattern, replace,[match])\n",
    "  return pnn_model_traced_ori,pnn_model_traced_modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pattern_replace_and_matcher_for_loop_pnn_reduce(traced,\n",
    "                                                  redundancy_part_slice,non_redundancy_part_slice,\n",
    "                                                  embed_node_name,getitem_node_names,num_field,batch = 4096,match_func = None\n",
    "                                                ):\n",
    "  from torch.fx.passes.utils.matcher_utils import SubgraphMatcher\n",
    "\n",
    "\n",
    "  def _match(match,ori,pat):\n",
    "    return True \n",
    "  env  = utils.get_env(traced)\n",
    "  target_node = env[embed_node_name]\n",
    "  target_node_mod = utils.get_target_mod(traced,target_node.target)\n",
    "  shape_info = target_node_mod.weight.data.shape\n",
    "  getitem_node_args = [env[i].args[1] for i in getitem_node_names]\n",
    "  class PatternClass(torch.nn.Module):\n",
    "      def __init__(self):\n",
    "          super().__init__()\n",
    "\n",
    "      def pn(self,x):\n",
    "         return torch.sum(x[getitem_node_args[0]] * x[getitem_node_args[1]], dim = 2)\n",
    "\n",
    "      def forward(self,x):\n",
    "          return self.pn(x)\n",
    "  pattern = PatternClass()  \n",
    "  pattern_trace = symbolic_trace(pattern)\n",
    "  pattern_graph = pattern_trace.graph\n",
    "  original_graph = traced.graph\n",
    "  matcher =  SubgraphMatcher(pattern_graph, match_output=False, match_placeholder=False,\n",
    "                              remove_overlapping_matches=True)\n",
    "  _matches = matcher.match(original_graph)\n",
    "  match_filters = [_match if match_func is None else match_func]\n",
    "  _matches = [\n",
    "      m for m in _matches\n",
    "      if all(match_filter(m, original_graph, pattern_graph)\n",
    "              for match_filter in match_filters)\n",
    "  ]  \n",
    "  # 因为在过滤器中做了限制应该只有一个符合要求的\n",
    "  _matched = _matches[0]\n",
    "  pattern_env = utils.get_env(pattern_trace)\n",
    "  node_map = _matched.nodes_map\n",
    "  \n",
    "  \n",
    "#   linear_node = node_map[pattern_env['mlp']]\n",
    "#   linear_node_module = utils.get_target_mod(traced,linear_node.target)\n",
    "#   linear_node_weight = linear_node_module.weight.data\n",
    "#   linear_node_bias = linear_node_module.bias.data\n",
    "  \n",
    "  class ReplacementClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.num_fields = num_field\n",
    "      self.num_prefix = redundancy_part_slice[1].stop\n",
    "      # self.ori_linear_shape = linear_node_weight.shape\n",
    "      self.num_sufix = self.num_fields - self.num_prefix\n",
    "      self.total = self.num_fields * (self.num_fields - 1) // 2\n",
    "      self.redundancy_cross_part_total = self.num_prefix * (self.num_prefix - 1) // 2\n",
    "      self.non_redundancy_cross_part_total = self.num_sufix * (self.num_sufix - 1) // 2\n",
    "      self.rest_cross_part_total = self.total - self.redundancy_cross_part_total - self.non_redundancy_cross_part_total\n",
    "\n",
    "    def pn(self,embed):\n",
    "      row_emb = embed[getitem_node_args[0]]\n",
    "      col_emb = embed[getitem_node_args[1]]\n",
    "      redundancy_x_row, reducey_x_col = list(),list()\n",
    "      non_redundancy_x_row, non_redundancy_x_col = list(), list()\n",
    "      mixed_x_row, mixed_x_col = list(), list()\n",
    "      prefix = self.num_prefix\n",
    "      sufix = self.num_sufix\n",
    "      for i in range(prefix - 1):\n",
    "        for j in range(i+1,prefix):\n",
    "            redundancy_x_row.append(i),reducey_x_col.append(j)\n",
    "      for i in range(sufix - 1):\n",
    "        for j in range(i + 1,sufix):\n",
    "            non_redundancy_x_row.append(i),non_redundancy_x_col.append(j)\n",
    "      for i in range(prefix):\n",
    "        for j in range(sufix):\n",
    "            mixed_x_row.append(i),mixed_x_col.append(j)\n",
    "      pn = row_emb * col_emb\n",
    "      \n",
    "      return torch.concat([torch.sum(pn[0,redundancy_x_row],dim = -1).repeat(batch,1) ,torch.sum(pn[:,(non_redundancy_x_row + mixed_x_row)],dim = -1)], dim = 1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "      return self.pn(x)\n",
    "    \n",
    "  \n",
    "  return pattern,ReplacementClass(),_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def workload_pnn_reduce(num_field, prefix,dim = 64,l = [1024,512,256],batch = 4096):\n",
    "  print(f\"now gen workload of PNN with config: dim: {dim}, num_field: {num_field}, prefix: {prefix}\")\n",
    "  pnn_loop = pnn.ProductNeuralNetworkModel([100 for i in range(num_field)],dim,l,0.1)\n",
    "\n",
    "  pnn_model_traced_ori = symbolic_trace(pnn_loop)\n",
    "  \n",
    "  pnn_model_modify = pnn.ProductNeuralNetworkModel([100 for i in range(num_field)],dim,l,0.1)\n",
    "  pnn_model_traced_modify = symbolic_trace(pnn_model_modify)\n",
    "  pattern,replace,match = gen_pattern_replace_and_matcher_for_loop_pnn_reduce(pnn_model_traced_modify,\n",
    "                                                                      (0,slice(None,prefix,None)),(slice(None,None,None),slice(prefix,None,None)),\n",
    "                                                                      embed_node_name = \"embedding_embedding\",\n",
    "                                                                      getitem_node_names = [\"getitem\",\"getitem_1\"],num_field=num_field,batch = batch)\n",
    "  matches = subgraph_rewriter.replace_pattern_with_filters(pnn_model_traced_modify, pattern, replace,[match])\n",
    "  return pnn_model_traced_ori,pnn_model_traced_modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genWorkload(num_field = 34 * 5,prefix = 29 * 5, batch = 4096, dim = 64):\n",
    "  ori_model_name = f'/home/yssun/pytorch-fm/torchfm/model/test_fx/exp/model_repo/pnn_single/pnn_single_{batch}_{num_field}_{prefix}_{dim}_ori.onnx'\n",
    "  modify_model_name = f'/home/yssun/pytorch-fm/torchfm/model/test_fx/exp/model_repo/pnn_single/pnn_single_{batch}_{num_field}_{prefix}_{dim}_modify.onnx'\n",
    "  ori, modify = workload_pnn_mul(num_field,prefix,dim,l = [1024,512,256],batch = batch)\n",
    "  torch.onnx.export(ori,               # 模型 being run\n",
    "                  torch.randint(low=0, high=20, size=(batch,num_field), dtype=torch.long),                  # 模型输入 (or a tuple for multiple inputs)\n",
    "                  ori_model_name,        # 导出文件的文件名\n",
    "                  export_params=True, # 如果设置为True，则参数也会被导出。注意某些情况下参数可能无法被导出。\n",
    "                  opset_version=10,   # ONNX版本\n",
    "                  do_constant_folding=True,  # 是否执行常量折叠以优化模型\n",
    "                  input_names = ['input'],   # 输入的名称\n",
    "                  output_names = ['output'], # 输出的名称\n",
    "                  )\n",
    "  torch.onnx.export(modify,               # 模型 being run\n",
    "                  torch.randint(low=0, high=20, size=(batch,num_field), dtype=torch.long),                  # 模型输入 (or a tuple for multiple inputs)\n",
    "                  modify_model_name,        # 导出文件的文件名\n",
    "                  export_params=True, # 如果设置为True，则参数也会被导出。注意某些情况下参数可能无法被导出。\n",
    "                  opset_version=10,   # ONNX版本\n",
    "                  do_constant_folding=True,  # 是否执行常量折叠以优化模型\n",
    "                  input_names = ['input'],   # 输入的名称\n",
    "                  output_names = ['output'], # 输出的名称\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims= [32]\n",
    "batches = [1024,2048,4096]\n",
    "num_field_and_prefixs = [(34 * 5,29*5),(22 * 5,10 * 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pattern_replace_and_matcher_for_loop_pnn_linear(traced,\n",
    "                                                  redundancy_part_slice,non_redundancy_part_slice,\n",
    "                                                  embed_node_name,getitem_node_names,num_field,batch = 4096,match_func = None\n",
    "                                                ):\n",
    "  from torch.fx.passes.utils.matcher_utils import SubgraphMatcher\n",
    "\n",
    "\n",
    "  def _match(match,ori,pat):\n",
    "    return True \n",
    "  env  = utils.get_env(traced)\n",
    "  target_node = env[embed_node_name]\n",
    "  target_node_mod = utils.get_target_mod(traced,target_node.target)\n",
    "  shape_info = target_node_mod.weight.data.shape\n",
    "  getitem_node_args = [env[i].args[1] for i in getitem_node_names]\n",
    "  class PatternClass(torch.nn.Module):\n",
    "      def __init__(self):\n",
    "          super().__init__()\n",
    "          self.mlp = nn.Linear(shape_info[0],shape_info[1])\n",
    "          self.embed = torch.nn.Embedding(1, 1)\n",
    "          self.embed_output_dim = shape_info[1]\n",
    "      def pn(self,x):\n",
    "        # num_fields = self.num_fields\n",
    "        row, col = list(), list()\n",
    "        for i in range(num_field - 1):\n",
    "            for j in range(i + 1, num_field):\n",
    "                row.append(i), col.append(j)\n",
    "        return torch.sum(x[:, row] * x[:, col], dim=2)\n",
    "\n",
    "      def forward(self,x):\n",
    "          emb = self.embed(x)\n",
    "          x = torch.cat([emb.view(-1, self.embed_output_dim * num_field), self.pn(emb)], dim=1)\n",
    "          return self.mlp(x)\n",
    "  pattern = PatternClass()  \n",
    "  pattern_trace = symbolic_trace(pattern)\n",
    "  pattern_graph = pattern_trace.graph\n",
    "  original_graph = traced.graph\n",
    "  # pattern_trace.print_readable()\n",
    "  # print(\"=====\")\n",
    "  # traced.print_readable()\n",
    "  matcher =  SubgraphMatcher(pattern_graph, match_output=False, match_placeholder=False,\n",
    "                              remove_overlapping_matches=True)\n",
    "  _matches = matcher.match(original_graph)\n",
    "  match_filters = [_match if match_func is None else match_func]\n",
    "  _matches = [\n",
    "      m for m in _matches\n",
    "      if all(match_filter(m, original_graph, pattern_graph)\n",
    "              for match_filter in match_filters)\n",
    "  ]  \n",
    "  # 因为在过滤器中做了限制应该只有一个符合要求的\n",
    "  _matched = _matches[0]\n",
    "  pattern_env = utils.get_env(pattern_trace)\n",
    "  node_map = _matched.nodes_map\n",
    "  \n",
    "  embed_node = node_map[pattern_env['embed']]\n",
    "  embed_node_module = utils.get_target_mod(traced,embed_node.target)  \n",
    "  linear_node = node_map[pattern_env['mlp']]\n",
    "  linear_node_module = utils.get_target_mod(traced,linear_node.target)\n",
    "  linear_node_weight = linear_node_module.weight.data\n",
    "  linear_node_bias = linear_node_module.bias.data\n",
    "  \n",
    "  class ReplacementClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.embed = embed_node_module\n",
    "      self.num_fields = num_field\n",
    "      self.num_prefix = redundancy_part_slice[1].stop\n",
    "      self.embed_dim = self.embed.weight.data.shape[1]\n",
    "      # self.ori_linear_shape = linear_node_weight.shape\n",
    "      self.num_sufix = self.num_fields - self.num_prefix\n",
    "      self.total = self.num_fields * (self.num_fields - 1) // 2\n",
    "      self.redundancy_cross_part_total = self.num_prefix * (self.num_prefix - 1) // 2\n",
    "      self.non_redundancy_cross_part_total = self.num_sufix * (self.num_sufix - 1) // 2\n",
    "      self.rest_cross_part_total = self.total - self.redundancy_cross_part_total - self.non_redundancy_cross_part_total\n",
    "      self.redency_weight_len = self.embed_dim * redundancy_part_slice[1].stop + self.num_prefix\n",
    "      redency_weight = linear_node_weight[:,:self.redency_weight_len]\n",
    "      unredency_weight = linear_node_weight[:,self.redency_weight_len:]\n",
    "      self.redency_linear = nn.Linear(redency_weight.shape[1],redency_weight.shape[0])\n",
    "      self.redency_linear.weight.data.copy_(redency_weight)\n",
    "      self.redency_linear.bias.data.copy_(linear_node_bias)\n",
    "      self.unredency_linear = nn.Linear(unredency_weight.shape[1],unredency_weight.shape[0],bias=False)\n",
    "      self.unredency_linear.weight.data.copy_(unredency_weight)\n",
    "\n",
    "    def pn(self,x):\n",
    "        return torch.sum(x[getitem_node_args[0]] * x[getitem_node_args[1]], dim = 2)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "      emb = self.embed(x)\n",
    "      pn = self.pn(emb)\n",
    "      re_emb = emb[0,:self.num_prefix].view(-1,self.num_prefix * self.embed_dim)\n",
    "      un_emb = emb[:,self.num_prefix:].view(-1,(num_field - self.num_prefix) * self.embed_dim)\n",
    "      re_pn = pn[0,:self.num_prefix]\n",
    "      un_pn = pn[:,self.num_prefix:]\n",
    "      return (self.redency_linear(torch.concat([re_emb,re_pn.unsqueeze(0)],dim = -1))) + self.unredency_linear(torch.concat([un_emb,un_pn],dim = -1))\n",
    "    \n",
    "  \n",
    "  return pattern,ReplacementClass(),_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def workload_pnn_linear(num_field, prefix,dim = 64,l = [1024,512,256],batch = 4096):\n",
    "  print(f\"now gen workload of PNN with config: dim: {dim}, num_field: {num_field}, prefix: {prefix}\")\n",
    "  pnn_loop = pnn.ProductNeuralNetworkModel([100 for i in range(num_field)],dim,l,0.1)\n",
    "\n",
    "  pnn_model_traced_ori = symbolic_trace(pnn_loop)\n",
    "  \n",
    "  pnn_model_modify = pnn.ProductNeuralNetworkModel([100 for i in range(num_field)],dim,l,0.1)\n",
    "  pnn_model_traced_modify = symbolic_trace(pnn_model_modify)\n",
    "  pattern,replace,match = gen_pattern_replace_and_matcher_for_loop_pnn_linear(pnn_model_traced_modify,\n",
    "                                                                      (0,slice(None,prefix,None)),(slice(None,None,None),slice(prefix,None,None)),\n",
    "                                                                      embed_node_name = \"embedding_embedding\",\n",
    "                                                                      getitem_node_names = [\"getitem\",\"getitem_1\"],num_field=num_field,batch = batch)\n",
    "  matches = subgraph_rewriter.replace_pattern_with_filters(pnn_model_traced_modify, pattern, replace,[match])\n",
    "  return pnn_model_traced_ori,pnn_model_traced_modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genWorkload(num_field = 34 * 5,prefix = 29 * 5, batch = 4096, dim = 64):\n",
    "  ori_model_name = f'/home/yssun/pytorch-fm/torchfm/model/test_fx/exp/model_repo/pnn_linear/pnn_single_{batch}_{num_field}_{prefix}_{dim}_ori.onnx'\n",
    "  modify_model_name = f'/home/yssun/pytorch-fm/torchfm/model/test_fx/exp/model_repo/pnn_linear/pnn_single_{batch}_{num_field}_{prefix}_{dim}_modify.onnx'\n",
    "  ori, modify = workload_pnn_linear(num_field,prefix,dim,l = [1024,512,256],batch = batch)\n",
    "  # torch.onnx.export(ori,               # 模型 being run\n",
    "  #                 torch.randint(low=0, high=20, size=(batch,num_field), dtype=torch.long),                  # 模型输入 (or a tuple for multiple inputs)\n",
    "  #                 ori_model_name,        # 导出文件的文件名\n",
    "  #                 export_params=True, # 如果设置为True，则参数也会被导出。注意某些情况下参数可能无法被导出。\n",
    "  #                 opset_version=10,   # ONNX版本\n",
    "  #                 do_constant_folding=True,  # 是否执行常量折叠以优化模型\n",
    "  #                 input_names = ['input'],   # 输入的名称\n",
    "  #                 output_names = ['output'], # 输出的名称\n",
    "  #                 )\n",
    "  torch.onnx.export(modify,               # 模型 being run\n",
    "                  torch.randint(low=0, high=20, size=(batch,num_field), dtype=torch.long),                  # 模型输入 (or a tuple for multiple inputs)\n",
    "                  modify_model_name,        # 导出文件的文件名\n",
    "                  export_params=True, # 如果设置为True，则参数也会被导出。注意某些情况下参数可能无法被导出。\n",
    "                  opset_version=10,   # ONNX版本\n",
    "                  do_constant_folding=True,  # 是否执行常量折叠以优化模型\n",
    "                  input_names = ['input'],   # 输入的名称\n",
    "                  output_names = ['output'], # 输出的名称\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims= [32]\n",
    "batches = [1024,2048,4096]\n",
    "num_field_and_prefixs = [(34 * 5,29*5),(22 * 5,10 * 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of PNN with config: dim: 32, num_field: 170, prefix: 145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/tensorrt/lib/python3.9/site-packages/torch/onnx/symbolic_opset9.py:1173: UserWarning: This model contains a squeeze operation on dimension 1. If the model is intended to be used with dynamic input shapes, please use opset version 11 to export the model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of PNN with config: dim: 32, num_field: 110, prefix: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/tensorrt/lib/python3.9/site-packages/torch/onnx/symbolic_opset9.py:1173: UserWarning: This model contains a squeeze operation on dimension 1. If the model is intended to be used with dynamic input shapes, please use opset version 11 to export the model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of PNN with config: dim: 32, num_field: 170, prefix: 145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/tensorrt/lib/python3.9/site-packages/torch/onnx/symbolic_opset9.py:1173: UserWarning: This model contains a squeeze operation on dimension 1. If the model is intended to be used with dynamic input shapes, please use opset version 11 to export the model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of PNN with config: dim: 32, num_field: 110, prefix: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/tensorrt/lib/python3.9/site-packages/torch/onnx/symbolic_opset9.py:1173: UserWarning: This model contains a squeeze operation on dimension 1. If the model is intended to be used with dynamic input shapes, please use opset version 11 to export the model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of PNN with config: dim: 32, num_field: 170, prefix: 145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/tensorrt/lib/python3.9/site-packages/torch/onnx/symbolic_opset9.py:1173: UserWarning: This model contains a squeeze operation on dimension 1. If the model is intended to be used with dynamic input shapes, please use opset version 11 to export the model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of PNN with config: dim: 32, num_field: 110, prefix: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/tensorrt/lib/python3.9/site-packages/torch/onnx/symbolic_opset9.py:1173: UserWarning: This model contains a squeeze operation on dimension 1. If the model is intended to be used with dynamic input shapes, please use opset version 11 to export the model.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for dim in dims:\n",
    "  for batch in batches:\n",
    "    for num_field,prefix in num_field_and_prefixs:\n",
    "      genWorkload(num_field=num_field,prefix=prefix,batch=batch,dim=dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorrt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
