{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../')\n",
    "import time\n",
    "import FwFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace success!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.fx import subgraph_rewriter, symbolic_trace\n",
    "import utils\n",
    "from torch.fx import Proxy, Graph, GraphModule\n",
    "from torch.fx.passes.utils.matcher_utils import SubgraphMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import time\n",
    "import torch._dynamo as dynamo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_and_variance_manual(data):\n",
    "    n = len(data)\n",
    "    mean = sum(data) / n\n",
    "    variance = sum((x - mean) ** 2 for x in data) / n\n",
    "    return mean, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pattern_replace_and_matcher_for_FwFM(traced,\n",
    "                                                  redency_part_slice,unredency_part_slice,\n",
    "                                                  key_node_name,getitem_node_names,num_field,batch,match_func = None\n",
    "                                                ):\n",
    "  from torch.fx.passes.utils.matcher_utils import SubgraphMatcher\n",
    "\n",
    "\n",
    "  def _match(match,ori,pat):\n",
    "    return True \n",
    "  env  = utils.get_env(traced)\n",
    "  target_node = env[key_node_name]\n",
    "  target_node_mod = utils.get_target_mod(traced,target_node.target)\n",
    "  shape_info = target_node_mod.weight.data.shape\n",
    "  getitem_node_args = [env[i].args[1] for i in getitem_node_names]\n",
    "  print(shape_info)\n",
    "  class PatternClass(torch.nn.Module):\n",
    "      def __init__(self):\n",
    "          super().__init__()\n",
    "          self.embed = torch.nn.Embedding(1, 1)\n",
    "          self.embed_output_dim = shape_info[1]\n",
    "          self.mlp = nn.Linear(shape_info[0],shape_info[1])\n",
    "\n",
    "      def pn(self,x):\n",
    "         return torch.sum(x[getitem_node_args[0]] * x[getitem_node_args[1]], dim = 2)\n",
    "      def forward(self,x):\n",
    "        x = self.embed(x)\n",
    "        cross_term = self.pn(x)\n",
    "        # x = x.view(-1,self.embed_output_dim)\n",
    "        return self.mlp(x.view(-1,self.embed_output_dim)), cross_term\n",
    "  pattern = PatternClass()  \n",
    "  pattern_trace = symbolic_trace(pattern)\n",
    "  pattern_graph = pattern_trace.graph\n",
    "  original_graph = traced.graph\n",
    "  matcher =  SubgraphMatcher(pattern_graph, match_output=False, match_placeholder=False,\n",
    "                              remove_overlapping_matches=True)\n",
    "  _matches = matcher.match(original_graph)\n",
    "  match_filters = [_match if match_func is None else match_func]\n",
    "  _matches = [\n",
    "      m for m in _matches\n",
    "      if all(match_filter(m, original_graph, pattern_graph)\n",
    "              for match_filter in match_filters)\n",
    "  ]  \n",
    "  # 因为在过滤器中做了限制应该只有一个符合要求的\n",
    "  _matched = _matches[0]\n",
    "  pattern_env = utils.get_env(pattern_trace)\n",
    "  node_map = _matched.nodes_map\n",
    "  \n",
    "  embed_node = node_map[pattern_env['embed']]\n",
    "  embed_node_module = utils.get_target_mod(traced,embed_node.target)\n",
    "  \n",
    "  linear_node = node_map[pattern_env['mlp']]\n",
    "  linear_node_module = utils.get_target_mod(traced,linear_node.target)\n",
    "  linear_node_weight = linear_node_module.weight.data\n",
    "  if linear_node_module.bias is None:\n",
    "      linear_node_bias = None\n",
    "  else:\n",
    "      linear_node_bias = linear_node_module.bias.data\n",
    "  \n",
    "  class ReplacementClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.embed = embed_node_module\n",
    "      self.embed_dim = self.embed.weight.data.shape[1]\n",
    "      self.redency_weight_len = self.embed_dim * redency_part_slice[1].stop\n",
    "      redency_weight = linear_node_weight[:,:self.redency_weight_len]\n",
    "      unredency_weight = linear_node_weight[:,self.redency_weight_len:]\n",
    "      self.unredency_weight_len = unredency_weight.shape[1]\n",
    "      if linear_node_bias is not None:\n",
    "         self.redency_linear = nn.Linear(redency_weight.shape[1],redency_weight.shape[0])\n",
    "         self.redency_linear.bias.data.copy_(linear_node_bias)\n",
    "      else:\n",
    "         self.redency_linear = nn.Linear(redency_weight.shape[1],redency_weight.shape[0],False)\n",
    "      self.redency_linear.weight.data.copy_(redency_weight)\n",
    "      \n",
    "\n",
    "      self.unredency_linear = nn.Linear(unredency_weight.shape[1],unredency_weight.shape[0],bias=False)\n",
    "      self.unredency_linear.weight.data.copy_(unredency_weight)\n",
    "      self.num_fields = num_field\n",
    "      self.num_prefix = redency_part_slice[1].stop\n",
    "      self.num_sufix = self.num_fields - self.num_prefix\n",
    "      self.total = self.num_fields * (self.num_fields - 1) // 2\n",
    "      self.redundancy_cross_part_total = self.num_prefix * (self.num_prefix - 1) // 2\n",
    "      self.non_redundancy_cross_part_total = self.num_sufix * (self.num_sufix - 1) // 2\n",
    "      self.rest_cross_part_total = self.total - self.redundancy_cross_part_total - self.non_redundancy_cross_part_total   \n",
    "         \n",
    "    def pn(self,reducey_x,non_redundancy_x):\n",
    "       redundancy_x_row, reducey_x_col = list(),list()\n",
    "       non_redundancy_x_row, non_redundancy_x_col = list(), list()\n",
    "       mixed_x_row, mixed_x_col = list(), list()\n",
    "       prefix = self.num_prefix\n",
    "       sufix = self.num_sufix\n",
    "       for i in range(prefix - 1):\n",
    "          for j in range(i+1,prefix):\n",
    "             redundancy_x_row.append(i),reducey_x_col.append(j)\n",
    "       for i in range(sufix - 1):\n",
    "          for j in range(i + 1,sufix):\n",
    "             non_redundancy_x_row.append(i),non_redundancy_x_col.append(j)\n",
    "\n",
    "       for i in range(prefix):\n",
    "          for j in range(sufix):\n",
    "             mixed_x_row.append(i),mixed_x_col.append(j)\n",
    "       return torch.sum((reducey_x[redundancy_x_row] * reducey_x[reducey_x_col]).unsqueeze(0),dim = -1),\\\n",
    "              torch.sum(non_redundancy_x[:,non_redundancy_x_row] * non_redundancy_x[:,non_redundancy_x_col],dim= -1),\\\n",
    "              torch.sum(reducey_x[mixed_x_row] * non_redundancy_x[:,mixed_x_col],dim = -1)\n",
    "    def forward(self,x):\n",
    "      redency_part = x[redency_part_slice] \n",
    "      unredency_part = x[unredency_part_slice] \n",
    "      redency_embed = self.embed(redency_part)\n",
    "      unredency_embed = self.embed(unredency_part)\n",
    "      redundancy_part_pn, non_redundancy_part_pn, mixed_part_pn = self.pn(redency_embed,unredency_embed)\n",
    "      redundancy_part_pn = redundancy_part_pn.repeat(batch,1)\n",
    "      \n",
    "      cross_term =  torch.concat([redundancy_part_pn,non_redundancy_part_pn,mixed_part_pn], dim = 1)\n",
    "      return self.redency_linear(redency_embed.view(-1,self.redency_weight_len)) + self.unredency_linear(unredency_embed.view(-1,self.unredency_weight_len)), cross_term\n",
    "      # return unredency_sum\n",
    "    \n",
    "  \n",
    "  return pattern,ReplacementClass(),_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def workload_FwFM(num_field, batch,prefix,dim = 64):\n",
    "  print(f\"now gen workload of FwFM with config: dim: {dim}, num_field: {num_field}, prefix: {prefix}\")\n",
    "  FwFM_model_ori = FwFM.FwFM([100 for i in range(num_field)],dim)\n",
    "  ori_traced = symbolic_trace(FwFM_model_ori)\n",
    "  \n",
    "  FwFM_model_modify = FwFM.FwFM([100 for i in range(num_field)],dim)\n",
    "  modify_traced = symbolic_trace(FwFM_model_modify)\n",
    "  pattern,replace,match = gen_pattern_replace_and_matcher_for_FwFM(modify_traced,\n",
    "                                                                      (0,slice(None,prefix,None)),(slice(None,None,None),slice(prefix,None,None)),\n",
    "                                                                      key_node_name = \"linear_weight_layer\",\n",
    "                                                                      getitem_node_names = [\"getitem\",\"getitem_1\"],num_field=num_field, batch = batch)\n",
    "  matches = subgraph_rewriter.replace_pattern_with_filters(modify_traced, pattern, replace,[match])\n",
    "  return ori_traced,modify_traced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genWorkload(num_field = 34 * 5,prefix = 29 * 5, batch = 4096, dim = 64):\n",
    "  ori_model_name = f'/home/yssun/pytorch-fm/torchfm/model/test_fx/exp/model_repo/FwFM/FwFM_{batch}_{num_field}_{prefix}_{dim}_ori.onnx'\n",
    "  modify_model_name = f'/home/yssun/pytorch-fm/torchfm/model/test_fx/exp/model_repo/FwFM/FwFM_{batch}_{num_field}_{prefix}_{dim}_modify.onnx'\n",
    "  ori, modify = workload_FwFM(num_field = num_field,prefix = prefix,  dim = dim,batch=batch)\n",
    "  torch.onnx.export(ori,               # 模型 being run\n",
    "                  torch.randint(low=0, high=20, size=(batch,num_field), dtype=torch.long),                  # 模型输入 (or a tuple for multiple inputs)\n",
    "                  ori_model_name,        # 导出文件的文件名\n",
    "                  export_params=True, # 如果设置为True，则参数也会被导出。注意某些情况下参数可能无法被导出。\n",
    "                  opset_version=10,   # ONNX版本\n",
    "                  do_constant_folding=True,  # 是否执行常量折叠以优化模型\n",
    "                  input_names = ['input'],   # 输入的名称\n",
    "                  output_names = ['output'], # 输出的名称\n",
    "                  )\n",
    "  torch.onnx.export(modify,               # 模型 being run\n",
    "                  torch.randint(low=0, high=20, size=(batch,num_field), dtype=torch.long),                  # 模型输入 (or a tuple for multiple inputs)\n",
    "                  modify_model_name,        # 导出文件的文件名\n",
    "                  export_params=True, # 如果设置为True，则参数也会被导出。注意某些情况下参数可能无法被导出。\n",
    "                  opset_version=10,   # ONNX版本\n",
    "                  do_constant_folding=True,  # 是否执行常量折叠以优化模型\n",
    "                  input_names = ['input'],   # 输入的名称\n",
    "                  output_names = ['output'], # 输出的名称\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims= [32]\n",
    "batches = [1024,2048,4096]\n",
    "num_field_and_prefixs = [(34 * 5,29*5),(22 * 5,10 * 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pattern_replace_and_matcher_for_FwFM(traced,\n",
    "                                                  redency_part_slice,unredency_part_slice,\n",
    "                                                  key_node_name,getitem_node_names,num_field,batch,match_func = None\n",
    "                                                ):\n",
    "  from torch.fx.passes.utils.matcher_utils import SubgraphMatcher\n",
    "\n",
    "\n",
    "  def _match(match,ori,pat):\n",
    "    return True \n",
    "  env  = utils.get_env(traced)\n",
    "  target_node = env[key_node_name]\n",
    "  target_node_mod = utils.get_target_mod(traced,target_node.target)\n",
    "  shape_info = target_node_mod.weight.data.shape\n",
    "  getitem_node_args = [env[i].args[1] for i in getitem_node_names]\n",
    "  print(shape_info)\n",
    "  class PatternClass(torch.nn.Module):\n",
    "      def __init__(self):\n",
    "          super().__init__()\n",
    "          self.embed = torch.nn.Embedding(1, 1)\n",
    "          self.embed_output_dim = shape_info[1]\n",
    "          self.mlp = nn.Linear(shape_info[0],shape_info[1])\n",
    "\n",
    "      def pn(self,x):\n",
    "         return torch.sum(x[getitem_node_args[0]] * x[getitem_node_args[1]], dim = 2)\n",
    "      def forward(self,x):\n",
    "        x = self.embed(x)\n",
    "        cross_term = self.pn(x)\n",
    "        # x = x.view(-1,self.embed_output_dim)\n",
    "        return self.mlp(x.view(-1,self.embed_output_dim)), cross_term\n",
    "  pattern = PatternClass()  \n",
    "  pattern_trace = symbolic_trace(pattern)\n",
    "  pattern_graph = pattern_trace.graph\n",
    "  original_graph = traced.graph\n",
    "  matcher =  SubgraphMatcher(pattern_graph, match_output=False, match_placeholder=False,\n",
    "                              remove_overlapping_matches=True)\n",
    "  _matches = matcher.match(original_graph)\n",
    "  match_filters = [_match if match_func is None else match_func]\n",
    "  _matches = [\n",
    "      m for m in _matches\n",
    "      if all(match_filter(m, original_graph, pattern_graph)\n",
    "              for match_filter in match_filters)\n",
    "  ]  \n",
    "  # 因为在过滤器中做了限制应该只有一个符合要求的\n",
    "  _matched = _matches[0]\n",
    "  pattern_env = utils.get_env(pattern_trace)\n",
    "  node_map = _matched.nodes_map\n",
    "  \n",
    "  embed_node = node_map[pattern_env['embed']]\n",
    "  embed_node_module = utils.get_target_mod(traced,embed_node.target)\n",
    "  \n",
    "  linear_node = node_map[pattern_env['mlp']]\n",
    "  linear_node_module = utils.get_target_mod(traced,linear_node.target)\n",
    "  linear_node_weight = linear_node_module.weight.data\n",
    "  if linear_node_module.bias is None:\n",
    "      linear_node_bias = None\n",
    "  else:\n",
    "      linear_node_bias = linear_node_module.bias.data\n",
    "  \n",
    "  class ReplacementClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.embed = embed_node_module\n",
    "      self.embed_dim = self.embed.weight.data.shape[1]\n",
    "      self.redency_weight_len = self.embed_dim * redency_part_slice[1].stop\n",
    "      redency_weight = linear_node_weight[:,:self.redency_weight_len]\n",
    "      unredency_weight = linear_node_weight[:,self.redency_weight_len:]\n",
    "      self.unredency_weight_len = unredency_weight.shape[1]\n",
    "      if linear_node_bias is not None:\n",
    "         self.redency_linear = nn.Linear(redency_weight.shape[1],redency_weight.shape[0])\n",
    "         self.redency_linear.bias.data.copy_(linear_node_bias)\n",
    "      else:\n",
    "         self.redency_linear = nn.Linear(redency_weight.shape[1],redency_weight.shape[0],False)\n",
    "      self.redency_linear.weight.data.copy_(redency_weight)\n",
    "      \n",
    "\n",
    "      self.unredency_linear = nn.Linear(unredency_weight.shape[1],unredency_weight.shape[0],bias=False)\n",
    "      self.unredency_linear.weight.data.copy_(unredency_weight)\n",
    "      self.num_fields = num_field\n",
    "      self.num_prefix = redency_part_slice[1].stop\n",
    "      self.num_sufix = self.num_fields - self.num_prefix\n",
    "      self.total = self.num_fields * (self.num_fields - 1) // 2\n",
    "      self.redundancy_cross_part_total = self.num_prefix * (self.num_prefix - 1) // 2\n",
    "      self.non_redundancy_cross_part_total = self.num_sufix * (self.num_sufix - 1) // 2\n",
    "      self.rest_cross_part_total = self.total - self.redundancy_cross_part_total - self.non_redundancy_cross_part_total   \n",
    "         \n",
    "    def pn(self,reducey_x,non_redundancy_x):\n",
    "       redundancy_x_row, reducey_x_col = list(),list()\n",
    "       non_redundancy_x_row, non_redundancy_x_col = list(), list()\n",
    "       mixed_x_row, mixed_x_col = list(), list()\n",
    "       prefix = self.num_prefix\n",
    "       sufix = self.num_sufix\n",
    "       for i in range(prefix - 1):\n",
    "          for j in range(i+1,prefix):\n",
    "             redundancy_x_row.append(i),reducey_x_col.append(j)\n",
    "       for i in range(sufix - 1):\n",
    "          for j in range(i + 1,sufix):\n",
    "             non_redundancy_x_row.append(i),non_redundancy_x_col.append(j)\n",
    "\n",
    "       for i in range(prefix):\n",
    "          for j in range(sufix):\n",
    "             mixed_x_row.append(i),mixed_x_col.append(j)\n",
    "       return torch.sum((reducey_x[redundancy_x_row] * reducey_x[reducey_x_col]).unsqueeze(0),dim = -1),\\\n",
    "              torch.sum(non_redundancy_x[:,non_redundancy_x_row] * non_redundancy_x[:,non_redundancy_x_col],dim= -1),\\\n",
    "              torch.sum(reducey_x[mixed_x_row] * non_redundancy_x[:,mixed_x_col],dim = -1)\n",
    "    def forward(self,x):\n",
    "      redency_part = x[redency_part_slice] \n",
    "      unredency_part = x[unredency_part_slice] \n",
    "      redency_embed = self.embed(redency_part)\n",
    "      unredency_embed = self.embed(unredency_part)\n",
    "      redundancy_part_pn, non_redundancy_part_pn, mixed_part_pn = self.pn(redency_embed,unredency_embed)\n",
    "      redundancy_part_pn = redundancy_part_pn.repeat(batch,1)\n",
    "      \n",
    "      cross_term =  torch.concat([redundancy_part_pn,non_redundancy_part_pn,mixed_part_pn], dim = 1)\n",
    "      return self.redency_linear(redency_embed.view(-1,self.redency_weight_len)) + self.unredency_linear(unredency_embed.view(-1,self.unredency_weight_len)), cross_term\n",
    "      # return unredency_sum\n",
    "    \n",
    "  \n",
    "  return pattern,ReplacementClass(),_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pattern_replace_and_matcher_for_FwFM_reduce(traced,\n",
    "                                                  redundancy_part_slice,non_redundancy_part_slice,\n",
    "                                                  key_node_name,getitem_node_names,num_field,batch = 4096,match_func = None\n",
    "                                                ):\n",
    "  from torch.fx.passes.utils.matcher_utils import SubgraphMatcher\n",
    "\n",
    "\n",
    "  def _match(match,ori,pat):\n",
    "    return True \n",
    "  env  = utils.get_env(traced)\n",
    "  target_node = env[key_node_name]\n",
    "  target_node_mod = utils.get_target_mod(traced,target_node.target)\n",
    "  shape_info = target_node_mod.weight.data.shape\n",
    "  getitem_node_args = [env[i].args[1] for i in getitem_node_names]\n",
    "  class PatternClass(torch.nn.Module):\n",
    "      def __init__(self):\n",
    "          super().__init__()\n",
    "\n",
    "      def pn(self,x):\n",
    "         return torch.sum(x[getitem_node_args[0]] * x[getitem_node_args[1]], dim = 2)\n",
    "\n",
    "      def forward(self,x):\n",
    "          return self.pn(x)\n",
    "  pattern = PatternClass()  \n",
    "  pattern_trace = symbolic_trace(pattern)\n",
    "  pattern_graph = pattern_trace.graph\n",
    "  original_graph = traced.graph\n",
    "  matcher =  SubgraphMatcher(pattern_graph, match_output=False, match_placeholder=False,\n",
    "                              remove_overlapping_matches=True)\n",
    "  _matches = matcher.match(original_graph)\n",
    "  match_filters = [_match if match_func is None else match_func]\n",
    "  _matches = [\n",
    "      m for m in _matches\n",
    "      if all(match_filter(m, original_graph, pattern_graph)\n",
    "              for match_filter in match_filters)\n",
    "  ]  \n",
    "  # 因为在过滤器中做了限制应该只有一个符合要求的\n",
    "  _matched = _matches[0]\n",
    "  pattern_env = utils.get_env(pattern_trace)\n",
    "  node_map = _matched.nodes_map\n",
    "  \n",
    "  \n",
    "#   linear_node = node_map[pattern_env['mlp']]\n",
    "#   linear_node_module = utils.get_target_mod(traced,linear_node.target)\n",
    "#   linear_node_weight = linear_node_module.weight.data\n",
    "#   linear_node_bias = linear_node_module.bias.data\n",
    "  \n",
    "  class ReplacementClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.num_fields = num_field\n",
    "      self.num_prefix = redundancy_part_slice[1].stop\n",
    "      # self.ori_linear_shape = linear_node_weight.shape\n",
    "      self.num_sufix = self.num_fields - self.num_prefix\n",
    "      self.total = self.num_fields * (self.num_fields - 1) // 2\n",
    "      self.redundancy_cross_part_total = self.num_prefix * (self.num_prefix - 1) // 2\n",
    "      self.non_redundancy_cross_part_total = self.num_sufix * (self.num_sufix - 1) // 2\n",
    "      self.rest_cross_part_total = self.total - self.redundancy_cross_part_total - self.non_redundancy_cross_part_total\n",
    "\n",
    "    def pn(self,embed):\n",
    "      row_emb = embed[getitem_node_args[0]]\n",
    "      col_emb = embed[getitem_node_args[1]]\n",
    "      pn = row_emb * col_emb\n",
    "      # prefix_pn = pn[0,self.num_prefix:]\n",
    "      \n",
    "      return torch.concat([torch.sum(pn[0,:self.num_prefix],dim = -1).repeat(batch,1) ,torch.sum(pn[:,self.num_prefix:],dim = -1)], dim = 1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "      return self.pn(x)\n",
    "    \n",
    "  \n",
    "  return pattern,ReplacementClass(),_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def workload_FwFM(num_field, batch,prefix,dim = 64):\n",
    "  print(f\"now gen workload of FwFM with config: dim: {dim}, num_field: {num_field}, prefix: {prefix}\")\n",
    "  FwFM_model_ori = FwFM.FwFM([100 for i in range(num_field)],dim)\n",
    "  ori_traced = symbolic_trace(FwFM_model_ori)\n",
    "  \n",
    "  FwFM_model_modify = FwFM.FwFM([100 for i in range(num_field)],dim)\n",
    "  modify_traced = symbolic_trace(FwFM_model_modify)\n",
    "  pattern,replace,match = gen_pattern_replace_and_matcher_for_FwFM_reduce(modify_traced,\n",
    "                                                                      (0,slice(None,prefix,None)),(slice(None,None,None),slice(prefix,None,None)),\n",
    "                                                                      key_node_name = \"linear_weight_layer\",\n",
    "                                                                      getitem_node_names = [\"getitem\",\"getitem_1\"],num_field=num_field, batch = batch)\n",
    "  matches = subgraph_rewriter.replace_pattern_with_filters(modify_traced, pattern, replace,[match])\n",
    "  return ori_traced,modify_traced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genWorkload(num_field = 34 * 5,prefix = 29 * 5, batch = 4096, dim = 64):\n",
    "  ori_model_name = f'/home/yssun/pytorch-fm/torchfm/model/test_fx/exp/model_repo/FwFM_reduce/FwFM_{batch}_{num_field}_{prefix}_{dim}_ori.onnx'\n",
    "  modify_model_name = f'/home/yssun/pytorch-fm/torchfm/model/test_fx/exp/model_repo/FwFM_reduce/FwFM_{batch}_{num_field}_{prefix}_{dim}_modify.onnx'\n",
    "  ori, modify = workload_FwFM(num_field = num_field,prefix = prefix,  dim = dim,batch=batch)\n",
    "  # torch.onnx.export(ori,               # 模型 being run\n",
    "  #                 torch.randint(low=0, high=20, size=(batch,num_field), dtype=torch.long),                  # 模型输入 (or a tuple for multiple inputs)\n",
    "  #                 ori_model_name,        # 导出文件的文件名\n",
    "  #                 export_params=True, # 如果设置为True，则参数也会被导出。注意某些情况下参数可能无法被导出。\n",
    "  #                 opset_version=10,   # ONNX版本\n",
    "  #                 do_constant_folding=True,  # 是否执行常量折叠以优化模型\n",
    "  #                 input_names = ['input'],   # 输入的名称\n",
    "  #                 output_names = ['output'], # 输出的名称\n",
    "  #                 )\n",
    "  torch.onnx.export(modify,               # 模型 being run\n",
    "                  torch.randint(low=0, high=20, size=(batch,num_field), dtype=torch.long),                  # 模型输入 (or a tuple for multiple inputs)\n",
    "                  modify_model_name,        # 导出文件的文件名\n",
    "                  export_params=True, # 如果设置为True，则参数也会被导出。注意某些情况下参数可能无法被导出。\n",
    "                  opset_version=10,   # ONNX版本\n",
    "                  do_constant_folding=True,  # 是否执行常量折叠以优化模型\n",
    "                  input_names = ['input'],   # 输入的名称\n",
    "                  output_names = ['output'], # 输出的名称\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of FwFM with config: dim: 32, num_field: 34, prefix: 29\n",
      "now gen workload of FwFM with config: dim: 32, num_field: 22, prefix: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/tensorrt/lib/python3.9/site-packages/torch/onnx/symbolic_opset9.py:1173: UserWarning: This model contains a squeeze operation on dimension 1. If the model is intended to be used with dynamic input shapes, please use opset version 11 to export the model.\n",
      "  warnings.warn(\n",
      "/home/yssun/miniconda3/envs/tensorrt/lib/python3.9/site-packages/torch/onnx/symbolic_opset9.py:1173: UserWarning: This model contains a squeeze operation on dimension 1. If the model is intended to be used with dynamic input shapes, please use opset version 11 to export the model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of FwFM with config: dim: 32, num_field: 170, prefix: 145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/tensorrt/lib/python3.9/site-packages/torch/onnx/symbolic_opset9.py:1173: UserWarning: This model contains a squeeze operation on dimension 1. If the model is intended to be used with dynamic input shapes, please use opset version 11 to export the model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of FwFM with config: dim: 32, num_field: 110, prefix: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/tensorrt/lib/python3.9/site-packages/torch/onnx/symbolic_opset9.py:1173: UserWarning: This model contains a squeeze operation on dimension 1. If the model is intended to be used with dynamic input shapes, please use opset version 11 to export the model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of FwFM with config: dim: 32, num_field: 34, prefix: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/tensorrt/lib/python3.9/site-packages/torch/onnx/symbolic_opset9.py:1173: UserWarning: This model contains a squeeze operation on dimension 1. If the model is intended to be used with dynamic input shapes, please use opset version 11 to export the model.\n",
      "  warnings.warn(\n",
      "/home/yssun/miniconda3/envs/tensorrt/lib/python3.9/site-packages/torch/onnx/symbolic_opset9.py:1173: UserWarning: This model contains a squeeze operation on dimension 1. If the model is intended to be used with dynamic input shapes, please use opset version 11 to export the model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of FwFM with config: dim: 32, num_field: 22, prefix: 10\n",
      "now gen workload of FwFM with config: dim: 32, num_field: 170, prefix: 145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/tensorrt/lib/python3.9/site-packages/torch/onnx/symbolic_opset9.py:1173: UserWarning: This model contains a squeeze operation on dimension 1. If the model is intended to be used with dynamic input shapes, please use opset version 11 to export the model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of FwFM with config: dim: 32, num_field: 110, prefix: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/tensorrt/lib/python3.9/site-packages/torch/onnx/symbolic_opset9.py:1173: UserWarning: This model contains a squeeze operation on dimension 1. If the model is intended to be used with dynamic input shapes, please use opset version 11 to export the model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of FwFM with config: dim: 32, num_field: 34, prefix: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/tensorrt/lib/python3.9/site-packages/torch/onnx/symbolic_opset9.py:1173: UserWarning: This model contains a squeeze operation on dimension 1. If the model is intended to be used with dynamic input shapes, please use opset version 11 to export the model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of FwFM with config: dim: 32, num_field: 22, prefix: 10\n",
      "now gen workload of FwFM with config: dim: 32, num_field: 170, prefix: 145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/tensorrt/lib/python3.9/site-packages/torch/onnx/symbolic_opset9.py:1173: UserWarning: This model contains a squeeze operation on dimension 1. If the model is intended to be used with dynamic input shapes, please use opset version 11 to export the model.\n",
      "  warnings.warn(\n",
      "/home/yssun/miniconda3/envs/tensorrt/lib/python3.9/site-packages/torch/onnx/symbolic_opset9.py:1173: UserWarning: This model contains a squeeze operation on dimension 1. If the model is intended to be used with dynamic input shapes, please use opset version 11 to export the model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of FwFM with config: dim: 32, num_field: 110, prefix: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/tensorrt/lib/python3.9/site-packages/torch/onnx/symbolic_opset9.py:1173: UserWarning: This model contains a squeeze operation on dimension 1. If the model is intended to be used with dynamic input shapes, please use opset version 11 to export the model.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for dim in dims:\n",
    "  for batch in batches:\n",
    "    for num_field,prefix in num_field_and_prefixs:\n",
    "      genWorkload(num_field=num_field,prefix=prefix,batch=batch,dim=dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorrt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
