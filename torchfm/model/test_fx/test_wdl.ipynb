{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace success!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.fx import subgraph_rewriter, symbolic_trace\n",
    "import utils\n",
    "from torch.fx import Proxy, Graph, GraphModule\n",
    "from torch.fx.passes.utils.matcher_utils import SubgraphMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wdl_model = wd.WideAndDeepModel([100 for i in range(22)],8,[400,400,400],0.1)\n",
    "traced = symbolic_trace(wdl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pattern_replace_and_matcher_for_MLP(traced,\n",
    "                                                  redency_part_slice,unredency_part_slice,\n",
    "                                                  key_node_name,match_func = None\n",
    "                                                ):\n",
    "  from torch.fx.passes.utils.matcher_utils import SubgraphMatcher\n",
    "\n",
    "\n",
    "  def _match(match,ori,pat):\n",
    "    return True \n",
    "  env  = utils.get_env(traced)\n",
    "  target_node = env[key_node_name]\n",
    "  target_node_mod = utils.get_target_mod(traced,target_node.target)\n",
    "  shape_info = target_node_mod.weight.data.shape\n",
    "  class PatternClass(torch.nn.Module):\n",
    "      def __init__(self):\n",
    "          super().__init__()\n",
    "          self.embed = torch.nn.Embedding(1, 1)\n",
    "          self.offsets = nn.Parameter(torch.as_tensor(np.array((0, *np.cumsum([10,10])[:-1]), dtype=np.int64)),False)\n",
    "          self.embed_output_dim = shape_info[1]\n",
    "          self.mlp = nn.Linear(shape_info[0],shape_info[1])\n",
    "\n",
    "\n",
    "      def forward(self,x):\n",
    "          x = x + self.offsets\n",
    "          x = self.embed(x).view(-1,self.embed_output_dim)\n",
    "          return self.mlp(x)\n",
    "  pattern = PatternClass()  \n",
    "  pattern_trace = symbolic_trace(pattern)\n",
    "  pattern_graph = pattern_trace.graph\n",
    "  original_graph = traced.graph\n",
    "  matcher =  SubgraphMatcher(pattern_graph, match_output=False, match_placeholder=False,\n",
    "                              remove_overlapping_matches=True)\n",
    "  _matches = matcher.match(original_graph)\n",
    "  match_filters = [_match if match_func is None else match_func]\n",
    "  _matches = [\n",
    "      m for m in _matches\n",
    "      if all(match_filter(m, original_graph, pattern_graph)\n",
    "              for match_filter in match_filters)\n",
    "  ]  \n",
    "  # 因为在过滤器中做了限制应该只有一个符合要求的\n",
    "  _matched = _matches[0]\n",
    "  pattern_env = utils.get_env(pattern_trace)\n",
    "  node_map = _matched.nodes_map\n",
    "  pn = pattern_env['offsets']\n",
    "  offsets_node = node_map[pn]\n",
    "  offsets_val = utils.get_target_mod(traced,offsets_node.target)\n",
    "  \n",
    "  embed_node = node_map[pattern_env['embed']]\n",
    "  embed_node_module = utils.get_target_mod(traced,embed_node.target)\n",
    "  \n",
    "  linear_node = node_map[pattern_env['mlp']]\n",
    "  linear_node_module = utils.get_target_mod(traced,linear_node.target)\n",
    "  linear_node_weight = linear_node_module.weight.data\n",
    "  linear_node_bias = linear_node_module.bias.data\n",
    "  \n",
    "  class ReplacementClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.redency_offset = torch.as_tensor(np.array(offsets_val[redency_part_slice[1]], dtype=np.int64))\n",
    "      self.unredency_offset = torch.as_tensor(np.array(offsets_val[unredency_part_slice[1]], dtype=np.int64))\n",
    "      self.embed = embed_node_module\n",
    "      self.embed_dim = self.embed.weight.data.shape[1]\n",
    "      self.redency_weight_len = self.embed_dim * redency_part_slice[1].stop\n",
    "      redency_weight = linear_node_weight[:,:self.redency_weight_len]\n",
    "      unredency_weight = linear_node_weight[:,self.redency_weight_len:]\n",
    "      self.unredency_weight_len = unredency_weight.shape[1]\n",
    "      self.redency_linear = nn.Linear(redency_weight.shape[1],redency_weight.shape[0])\n",
    "      self.redency_linear.weight.data.copy_(redency_weight)\n",
    "      self.redency_linear.bias.data.copy_(linear_node_bias)\n",
    "\n",
    "      self.unredency_linear = nn.Linear(unredency_weight.shape[1],unredency_weight.shape[0],bias=False)\n",
    "      self.unredency_linear.weight.data.copy_(unredency_weight)\n",
    "\n",
    "      \n",
    "\n",
    "    def forward(self,x):\n",
    "      redency_part = x[redency_part_slice] + self.redency_offset\n",
    "      unredency_part = x[unredency_part_slice] + self.unredency_offset\n",
    "      return self.redency_linear(self.embed(redency_part).view(-1,self.redency_weight_len)) + self.unredency_linear(self.embed(unredency_part).view(-1,self.unredency_weight_len))\n",
    "      # return unredency_sum\n",
    "    \n",
    "  \n",
    "  return pattern,ReplacementClass(),_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pattern_replace_and_matcher_for_MLP2(traced,\n",
    "                                                  redency_part_slice,unredency_part_slice,\n",
    "                                                  key_node_name,match_func = None\n",
    "                                                ):\n",
    "  from torch.fx.passes.utils.matcher_utils import SubgraphMatcher\n",
    "\n",
    "\n",
    "  def _match(match,ori,pat):\n",
    "    return True \n",
    "  env  = utils.get_env(traced)\n",
    "  target_node = env[key_node_name]\n",
    "  target_node_mod = utils.get_target_mod(traced,target_node.target)\n",
    "  shape_info = target_node_mod.weight.data.shape\n",
    "  class PatternClass(torch.nn.Module):\n",
    "      def __init__(self):\n",
    "          super().__init__()\n",
    "          self.embed = torch.nn.Embedding(1, 1)\n",
    "          self.offsets = torch.as_tensor(np.array((0, *np.cumsum([10,10])[:-1]), dtype=np.int64))\n",
    "          self.embed_output_dim = shape_info[1]\n",
    "          self.mlp = nn.Linear(shape_info[0],shape_info[1])\n",
    "\n",
    "\n",
    "      def forward(self,x):\n",
    "          x = x + self.offsets\n",
    "          x = self.embed(x).view(-1,self.embed_output_dim)\n",
    "          return self.mlp(x)\n",
    "  pattern = PatternClass()  \n",
    "  pattern_trace = symbolic_trace(pattern)\n",
    "  pattern_graph = pattern_trace.graph\n",
    "  original_graph = traced.graph\n",
    "  matcher =  SubgraphMatcher(pattern_graph, match_output=False, match_placeholder=False,\n",
    "                              remove_overlapping_matches=True)\n",
    "  _matches = matcher.match(original_graph)\n",
    "  match_filters = [_match if match_func is None else match_func]\n",
    "  _matches = [\n",
    "      m for m in _matches\n",
    "      if all(match_filter(m, original_graph, pattern_graph)\n",
    "              for match_filter in match_filters)\n",
    "  ]  \n",
    "  # 因为在过滤器中做了限制应该只有一个符合要求的\n",
    "  _matched = _matches[0]\n",
    "  pattern_env = utils.get_env(pattern_trace)\n",
    "  node_map = _matched.nodes_map\n",
    "  pn = pattern_env['offsets']\n",
    "  offsets_node = node_map[pn]\n",
    "  offsets_val = utils.get_target_mod(traced,offsets_node.target)\n",
    "  \n",
    "  embed_node = node_map[pattern_env['embed']]\n",
    "  embed_node_module = utils.get_target_mod(traced,embed_node.target)\n",
    "  \n",
    "  linear_node = node_map[pattern_env['mlp']]\n",
    "  linear_node_module = utils.get_target_mod(traced,linear_node.target)\n",
    "  linear_node_weight = linear_node_module.weight.data\n",
    "  linear_node_bias = linear_node_module.bias.data\n",
    "  \n",
    "  class ReplacementClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.offsets =  torch.as_tensor(np.array(offsets_val, dtype=np.int64))\n",
    "      self.redency_offset = torch.as_tensor(np.array(offsets_val[redency_part_slice[1]], dtype=np.int64))\n",
    "      self.unredency_offset = torch.as_tensor(np.array(offsets_val[unredency_part_slice[1]], dtype=np.int64))\n",
    "      self.embed = embed_node_module\n",
    "      self.embed_dim = self.embed.weight.data.shape[1]\n",
    "      self.redency_weight_len = self.embed_dim * redency_part_slice[1].stop\n",
    "      redency_weight = linear_node_weight[:,:self.redency_weight_len]\n",
    "      unredency_weight = linear_node_weight[:,self.redency_weight_len:]\n",
    "      self.unredency_weight_len = unredency_weight.shape[1]\n",
    "      self.redency_linear = nn.Linear(redency_weight.shape[1],redency_weight.shape[0])\n",
    "      self.redency_linear.weight.data.copy_(redency_weight)\n",
    "      self.redency_linear.bias.data.copy_(linear_node_bias)\n",
    "\n",
    "      self.unredency_linear = nn.Linear(unredency_weight.shape[1],unredency_weight.shape[0])\n",
    "      self.unredency_linear.weight.data.copy_(unredency_weight)\n",
    "\n",
    "      \n",
    "\n",
    "    def forward(self,x):\n",
    "      x = self.embed(x + self.offsets)\n",
    "      return self.redency_linear(x[redency_part_slice].view(-1,self.redency_weight_len)) + self.unredency_linear(x[unredency_part_slice].view(-1,self.unredency_weight_len))\n",
    "      # return unredency_sum\n",
    "    \n",
    "  \n",
    "  return pattern,ReplacementClass(),_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wdl_model = wd.WideAndDeepModel([100 for i in range(22)],128,[400,400,400],0.1)\n",
    "traced = symbolic_trace(wdl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def workload_wdl(dim,l = [400,400,400]):\n",
    "  print(f\"now gen workload of wdl with config: dim: {dim}\")\n",
    "  wdl_model_ori = wd.WideAndDeepModel([100 for i in range(22)],dim,l,0.1)\n",
    "  ori_traced = symbolic_trace(wdl_model_ori)\n",
    "  \n",
    "  wdl_model_modify = wd.WideAndDeepModel([100 for i in range(22)],dim,l,0.1)\n",
    "  modify_traced = symbolic_trace(wdl_model_modify)\n",
    "  pattern,replace,match = gen_pattern_replace_and_matcher_for_MLP(modify_traced,\n",
    "                                                                      (0,slice(None,10,None)),(slice(None,None,None),slice(10,None,None)),\n",
    "                                                                      \"mlp_mlp_0\")\n",
    "  matches = subgraph_rewriter.replace_pattern_with_filters(modify_traced, pattern, replace,[match])\n",
    "  return ori_traced,modify_traced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# l = [400,400,400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dim = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori, modify = workload_wdl(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.71 ms ± 544 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.87 ms ± 2.16 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.6 ms ± 1.5 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(4096,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.2 ms ± 2.57 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(4096,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori, modify = workload_wdl(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.27 ms ± 883 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5 ms ± 925 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.9 ms ± 1.04 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(4096,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.68 ms ± 1.12 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(4096,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori, modify = workload_wdl(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.43 ms ± 720 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.34 ms ± 718 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.02 ms ± 1.24 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(4096,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.18 ms ± 1.72 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(4096,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori, modify = workload_wdl(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.12 ms ± 936 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.74 ms ± 912 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.9 ms ± 1.98 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(4096,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.89 ms ± 1.28 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(4096,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def workload_wdl_script(dim,l = [400,400,400]):\n",
    "  print(f\"now gen workload of wdl with config: dim: {dim}\")\n",
    "  wdl_model_ori = wd.WideAndDeepModel([100 for i in range(22)],dim,l,0.1)\n",
    "  ori_traced = symbolic_trace(wdl_model_ori)\n",
    "  \n",
    "  wdl_model_modify = wd.WideAndDeepModel([100 for i in range(22)],dim,l,0.1)\n",
    "  modify_traced = symbolic_trace(wdl_model_modify)\n",
    "  pattern,replace,match = gen_pattern_replace_and_matcher_for_MLP(modify_traced,\n",
    "                                                                      (0,slice(None,10,None)),(slice(None,None,None),slice(10,None,None)),\n",
    "                                                                      \"mlp_mlp_0\")\n",
    "  matches = subgraph_rewriter.replace_pattern_with_filters(modify_traced, pattern, replace,[match])\n",
    "  ori = torch.jit.script(ori_traced)\n",
    "  modify = torch.jit.script(wdl_model_modify)\n",
    "  ori.eval()\n",
    "  modify.eval()\n",
    "  return ori,modify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dim = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ori, modify = workload_wdl_script(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12 ms ± 428 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12 ms ± 439 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.32 ms ± 1.27 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(4096,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.19 ms ± 740 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(4096,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ori, modify = workload_wdl_script(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.31 ms ± 481 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.29 ms ± 474 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.38 ms ± 1.77 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(4096,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.16 ms ± 1.03 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(4096,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ori, modify = workload_wdl_script(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.73 ms ± 622 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.77 ms ± 607 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.48 ms ± 1.65 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(4096,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.43 ms ± 1.25 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(4096,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ori, modify = workload_wdl_script(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.56 ms ± 910 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.58 ms ± 691 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.9 ms ± 1.94 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(4096,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.9 ms ± 1.75 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(4096,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# l = [1024,512,256]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dim = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori, modify = workload_wdl(8,[1024,512,256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.68 ms ± 683 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.45 ms ± 2.19 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.8 ms ± 2.26 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(4096,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.6 ms ± 2.6 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(4096,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori, modify = workload_wdl(32,[1024,512,256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.02 ms ± 717 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8 ms ± 714 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 ms ± 1.51 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(4096,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.7 ms ± 2.45 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(4096,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori, modify = workload_wdl(64,[1024,512,256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.58 ms ± 871 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.11 ms ± 828 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.3 ms ± 2.99 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(4096,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.9 ms ± 2.29 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(4096,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori, modify = workload_wdl(128,[1024,512,256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.41 ms ± 1.25 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.19 ms ± 861 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.9 ms ± 2.89 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(4096,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.1 ms ± 2.54 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(4096,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custom_workload 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_workload_wdl(dim,nums_fields,prefix,l = [400,400,400]):\n",
    "  print(f\"now gen workload of wdl with config: dim: {dim}\")\n",
    "  wdl_model_ori = wd.WideAndDeepModel([100 for i in range(nums_fields)],dim,l,0.1)\n",
    "  ori_traced = symbolic_trace(wdl_model_ori)\n",
    "  \n",
    "  wdl_model_modify = wd.WideAndDeepModel([100 for i in range(nums_fields)],dim,l,0.1)\n",
    "  modify_traced = symbolic_trace(wdl_model_modify)\n",
    "  pattern,replace,match = gen_pattern_replace_and_matcher_for_MLP(modify_traced,\n",
    "                                                                      (0,slice(None,prefix,None)),(slice(None,None,None),slice(prefix,None,None)),\n",
    "                                                                      \"mlp_mlp_0\")\n",
    "  matches = subgraph_rewriter.replace_pattern_with_filters(modify_traced, pattern, replace,[match])\n",
    "  return ori_traced,modify_traced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dim = 8, 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori,modify = custom_workload_wdl(8,100,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.07 ms ± 784 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(1024,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.84 ms ± 762 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(1024,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.5 ms ± 1.37 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.6 ms ± 1.56 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dim = 32, 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori,modify = custom_workload_wdl(32,100,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.83 ms ± 1.05 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(1024,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.41 ms ± 1.02 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(1024,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.4 ms ± 3.88 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.7 ms ± 2.14 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total true time 24.42026138305664 ms\n",
      "total time: 24.78790283203125 ms\n",
      "Op type        Op                     Average runtime (ms)    Pct total runtime\n",
      "-------------  -------------------  ----------------------  -------------------\n",
      "call_module    mlp_mlp_0                         6.31738             25.4857\n",
      "call_module    embedding_embedding               5.7826              23.3283\n",
      "call_module    mlp_mlp_2                         1.98817              8.02074\n",
      "call_module    mlp_mlp_6                         1.87969              7.5831\n",
      "call_module    mlp_mlp_3                         1.83821              7.41574\n",
      "call_module    mlp_mlp_8                         1.13082              4.56198\n",
      "call_module    mlp_mlp_1                         1.10626              4.46291\n",
      "call_module    mlp_mlp_4                         1.02139              4.1205\n",
      "call_module    linear_fc                         0.836134             3.37315\n",
      "call_function  add                               0.722647             2.91532\n",
      "call_module    mlp_mlp_5                         0.567675             2.29013\n",
      "call_module    mlp_mlp_7                         0.385523             1.55529\n",
      "call_function  add_1                             0.199795             0.806017\n",
      "call_function  sum_1                             0.169754             0.684826\n",
      "call_module    mlp_mlp_9                         0.154495             0.623269\n",
      "call_function  add_3                             0.0486374            0.196214\n",
      "get_attr       linear_offsets                    0.0376701            0.15197\n",
      "call_function  sigmoid                           0.0362396            0.146199\n",
      "call_method    view                              0.0360012            0.145237\n",
      "call_method    squeeze                           0.0352859            0.142351\n",
      "call_function  add_2                             0.0345707            0.139466\n",
      "placeholder    x                                 0.0321865            0.129848\n",
      "get_attr       linear_bias                       0.0212193            0.0856033\n",
      "get_attr       embedding_offsets                 0.0200272            0.0807941\n",
      "output         output                            0.0178814            0.0721376\n"
     ]
    }
   ],
   "source": [
    "interp = utils.ProfilingInterpreter(ori)\n",
    "interp.run(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))\n",
    "print(interp.summary(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total true time 15.438079833984375 ms\n",
      "total time: 15.74087142944336 ms\n",
      "Op type        Op                  Average runtime (ms)    Pct total runtime\n",
      "-------------  ----------------  ----------------------  -------------------\n",
      "call_module    unredency_linear               5.65958             35.9547\n",
      "call_module    mlp_mlp_6                      1.40572              8.93036\n",
      "call_module    mlp_mlp_2                      1.38831              8.81979\n",
      "call_module    mlp_mlp_3                      1.37973              8.76526\n",
      "call_module    embed_1                        0.806332             5.12253\n",
      "call_module    mlp_mlp_5                      0.690699             4.38793\n",
      "call_module    linear_fc                      0.607967             3.86235\n",
      "call_module    mlp_mlp_8                      0.51713              3.28527\n",
      "call_module    mlp_mlp_7                      0.50807              3.22771\n",
      "call_function  add                            0.375271             2.38405\n",
      "call_function  add_3                          0.310421             1.97207\n",
      "call_module    mlp_mlp_4                      0.309706             1.96753\n",
      "call_function  add_4                          0.286579             1.82061\n",
      "call_module    mlp_mlp_1                      0.23675              1.50404\n",
      "call_module    redency_linear                 0.198364             1.26019\n",
      "call_function  sum_1                          0.14019              0.890612\n",
      "call_module    mlp_mlp_9                      0.117779             0.748235\n",
      "call_module    embed                          0.071764             0.455909\n",
      "call_function  getitem                        0.0555515            0.352913\n",
      "call_function  add_5                          0.0443459            0.281724\n",
      "call_method    squeeze                        0.0383854            0.243858\n",
      "call_function  sigmoid                        0.0374317            0.2378\n",
      "call_method    view                           0.0355244            0.225682\n",
      "call_function  add_1                          0.0348091            0.221138\n",
      "call_method    view_1                         0.0340939            0.216594\n",
      "placeholder    x                              0.0288486            0.183272\n",
      "call_function  getitem_1                      0.0224113            0.142377\n",
      "get_attr       linear_offsets                 0.0219345            0.139347\n",
      "get_attr       linear_bias                    0.0202656            0.128745\n",
      "call_function  add_2                          0.0169277            0.10754\n",
      "get_attr       redency_offset                 0.0128746            0.0817909\n",
      "output         output                         0.0128746            0.0817909\n",
      "get_attr       unredency_offset               0.0114441            0.072703\n"
     ]
    }
   ],
   "source": [
    "interp = utils.ProfilingInterpreter(modify)\n",
    "interp.run(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))\n",
    "print(interp.summary(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dim = 64, 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori,modify = custom_workload_wdl(64,100,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.68 ms ± 2.45 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(1024,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.81 ms ± 1.7 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(1024,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.7 ms ± 3.48 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.7 ms ± 2.58 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total true time 26.803255081176758 ms\n",
      "total time: 27.068376541137695 ms\n",
      "Op type        Op                     Average runtime (ms)    Pct total runtime\n",
      "-------------  -------------------  ----------------------  -------------------\n",
      "call_module    embedding_embedding              13.222               48.8466\n",
      "call_module    mlp_mlp_0                         8.39233             31.0042\n",
      "call_module    mlp_mlp_3                         0.895977             3.31005\n",
      "call_module    mlp_mlp_6                         0.84281              3.11363\n",
      "call_module    linear_fc                         0.511408             1.88932\n",
      "call_module    mlp_mlp_2                         0.427961             1.58104\n",
      "call_function  add                               0.387192             1.43042\n",
      "call_module    mlp_mlp_8                         0.369072             1.36348\n",
      "call_module    mlp_mlp_5                         0.365734             1.35115\n",
      "call_module    mlp_mlp_1                         0.24271              0.896656\n",
      "call_module    mlp_mlp_7                         0.225544             0.833238\n",
      "call_module    mlp_mlp_4                         0.225067             0.831476\n",
      "call_function  add_1                             0.171423             0.633296\n",
      "call_module    mlp_mlp_9                         0.107765             0.398122\n",
      "call_function  sum_1                             0.104189             0.38491\n",
      "get_attr       linear_offsets                    0.0474453            0.175279\n",
      "call_function  add_3                             0.0457764            0.169114\n",
      "call_function  sigmoid                           0.0369549            0.136524\n",
      "call_method    squeeze                           0.0350475            0.129478\n",
      "call_function  add_2                             0.0329018            0.121551\n",
      "get_attr       linear_bias                       0.0309944            0.114504\n",
      "call_method    view                              0.0302792            0.111862\n",
      "placeholder    x                                 0.023365             0.0863185\n",
      "get_attr       embedding_offsets                 0.017643             0.0651793\n",
      "output         output                            0.0116825            0.0431593\n"
     ]
    }
   ],
   "source": [
    "interp = utils.ProfilingInterpreter(ori)\n",
    "interp.run(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))\n",
    "print(interp.summary(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total true time 21.889686584472656 ms\n",
      "total time: 22.223234176635742 ms\n",
      "Op type        Op                  Average runtime (ms)    Pct total runtime\n",
      "-------------  ----------------  ----------------------  -------------------\n",
      "call_module    embed_1                        8.32009             37.4387\n",
      "call_module    unredency_linear               7.00045             31.5006\n",
      "call_module    mlp_mlp_3                      1.06359              4.78592\n",
      "call_module    mlp_mlp_6                      1.00708              4.53165\n",
      "call_module    linear_fc                      0.609636             2.74324\n",
      "call_module    mlp_mlp_2                      0.547886             2.46537\n",
      "call_module    mlp_mlp_5                      0.433207             1.94934\n",
      "call_function  add                            0.401258             1.80558\n",
      "call_module    mlp_mlp_8                      0.362635             1.63178\n",
      "call_module    mlp_mlp_4                      0.287771             1.29491\n",
      "call_module    mlp_mlp_7                      0.252485             1.13613\n",
      "call_function  add_4                          0.238419             1.07283\n",
      "call_module    mlp_mlp_1                      0.20647              0.929075\n",
      "call_module    redency_linear                 0.173807             0.782097\n",
      "call_function  add_3                          0.164509             0.740256\n",
      "call_function  sum_1                          0.149965             0.674813\n",
      "call_module    mlp_mlp_9                      0.1266               0.569675\n",
      "call_module    embed                          0.0789165            0.355108\n",
      "call_function  getitem                        0.0576973            0.259626\n",
      "call_function  add_1                          0.0565052            0.254262\n",
      "call_method    view_1                         0.0500679            0.225295\n",
      "call_function  add_5                          0.0448227            0.201693\n",
      "call_function  sigmoid                        0.0376701            0.169508\n",
      "call_method    squeeze                        0.0367165            0.165217\n",
      "call_method    view                           0.0312328            0.140541\n",
      "placeholder    x                              0.0264645            0.119085\n",
      "call_function  getitem_1                      0.026226             0.118012\n",
      "get_attr       linear_offsets                 0.020504             0.0922638\n",
      "get_attr       linear_bias                    0.0197887            0.0890453\n",
      "call_function  add_2                          0.0197887            0.0890453\n",
      "get_attr       redency_offset                 0.0138283            0.0622244\n",
      "output         output                         0.0119209            0.0536417\n",
      "get_attr       unredency_offset               0.0116825            0.0525689\n"
     ]
    }
   ],
   "source": [
    "interp = utils.ProfilingInterpreter(modify)\n",
    "interp.run(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))\n",
    "print(interp.summary(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dim = 128, 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori,modify = custom_workload_wdl(128,100,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.4 ms ± 3.22 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(1024,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.12 ms ± 1.56 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(1024,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.9 ms ± 2.95 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.8 ms ± 2.52 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total true time 42.16194152832031 ms\n",
      "total time: 42.474985122680664 ms\n",
      "Op type        Op                     Average runtime (ms)    Pct total runtime\n",
      "-------------  -------------------  ----------------------  -------------------\n",
      "call_module    embedding_embedding              19.5169              45.9493\n",
      "call_module    mlp_mlp_0                        15.9841              37.6317\n",
      "call_module    mlp_mlp_8                         1.08004              2.54276\n",
      "call_module    mlp_mlp_3                         0.96035              2.26098\n",
      "call_module    mlp_mlp_6                         0.896931             2.11167\n",
      "call_module    mlp_mlp_7                         0.598431             1.4089\n",
      "call_module    linear_fc                         0.567675             1.33649\n",
      "call_module    mlp_mlp_2                         0.478268             1.126\n",
      "call_module    mlp_mlp_5                         0.37837              0.890807\n",
      "call_function  add                               0.358582             0.844218\n",
      "call_module    mlp_mlp_1                         0.330925             0.779106\n",
      "call_module    mlp_mlp_4                         0.230551             0.542792\n",
      "call_function  add_1                             0.183105             0.43109\n",
      "call_module    mlp_mlp_9                         0.130892             0.308162\n",
      "call_function  sum_1                             0.105143             0.24754\n",
      "get_attr       linear_offsets                    0.0727177            0.171201\n",
      "call_function  add_3                             0.0476837            0.112263\n",
      "call_method    squeeze                           0.0391006            0.0920557\n",
      "call_method    view                              0.0374317            0.0881265\n",
      "call_function  sigmoid                           0.0374317            0.0881265\n",
      "get_attr       linear_bias                       0.0340939            0.0802681\n",
      "call_function  add_2                             0.0338554            0.0797068\n",
      "placeholder    x                                 0.0255108            0.0600607\n",
      "get_attr       embedding_offsets                 0.0193119            0.0454665\n",
      "output         output                            0.0145435            0.0342402\n"
     ]
    }
   ],
   "source": [
    "interp = utils.ProfilingInterpreter(ori)\n",
    "interp.run(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))\n",
    "print(interp.summary(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total true time 29.746532440185547 ms\n",
      "total time: 30.073881149291992 ms\n",
      "Op type        Op                  Average runtime (ms)    Pct total runtime\n",
      "-------------  ----------------  ----------------------  -------------------\n",
      "call_module    embed_1                       14.1785              47.1456\n",
      "call_module    unredency_linear               9.78136             32.5244\n",
      "call_module    mlp_mlp_3                      0.946522             3.14732\n",
      "call_module    mlp_mlp_6                      0.891447             2.96419\n",
      "call_module    linear_fc                      0.585794             1.94785\n",
      "call_module    mlp_mlp_2                      0.469923             1.56256\n",
      "call_function  add                            0.356674             1.18599\n",
      "call_module    mlp_mlp_5                      0.317812             1.05677\n",
      "call_function  add_4                          0.253201             0.841928\n",
      "call_module    mlp_mlp_8                      0.252247             0.838757\n",
      "call_module    mlp_mlp_4                      0.236034             0.784848\n",
      "call_module    mlp_mlp_1                      0.200033             0.665139\n",
      "call_module    redency_linear                 0.185013             0.615194\n",
      "call_module    mlp_mlp_7                      0.165462             0.550187\n",
      "call_function  add_3                          0.145435             0.483593\n",
      "call_module    mlp_mlp_9                      0.12207              0.405901\n",
      "call_function  sum_1                          0.0975132            0.324245\n",
      "call_module    embed                          0.0708103            0.235455\n",
      "call_method    view_1                         0.0700951            0.233076\n",
      "call_function  getitem                        0.0619888            0.206122\n",
      "call_function  add_5                          0.0422001            0.140321\n",
      "placeholder    x                              0.0407696            0.135565\n",
      "call_function  sigmoid                        0.0376701            0.125259\n",
      "call_method    squeeze                        0.0362396            0.120502\n",
      "call_function  add_1                          0.0317097            0.105439\n",
      "call_method    view                           0.0302792            0.100683\n",
      "get_attr       linear_bias                    0.0286102            0.0951331\n",
      "call_function  getitem_1                      0.0267029            0.0887909\n",
      "call_function  add_2                          0.0200272            0.0665932\n",
      "get_attr       linear_offsets                 0.0195503            0.0650077\n",
      "get_attr       redency_offset                 0.015974             0.053116\n",
      "output         output                         0.014782             0.0491521\n",
      "get_attr       unredency_offset               0.0140667            0.0467738\n"
     ]
    }
   ],
   "source": [
    "interp = utils.ProfilingInterpreter(modify)\n",
    "interp.run(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))\n",
    "print(interp.summary(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_workload_wdl_torchscript(dim,nums_fields,prefix,l = [400,400,400]):\n",
    "  print(f\"now gen workload of wdl with config: dim: {dim}\")\n",
    "  wdl_model_ori = wd.WideAndDeepModel([100 for i in range(nums_fields)],dim,l,0.1)\n",
    "  ori_traced = symbolic_trace(wdl_model_ori)\n",
    "  \n",
    "  wdl_model_modify = wd.WideAndDeepModel([100 for i in range(nums_fields)],dim,l,0.1)\n",
    "  modify_traced = symbolic_trace(wdl_model_modify)\n",
    "  pattern,replace,match = gen_pattern_replace_and_matcher_for_MLP(modify_traced,\n",
    "                                                                      (0,slice(None,prefix,None)),(slice(None,None,None),slice(prefix,None,None)),\n",
    "                                                                      \"mlp_mlp_0\")\n",
    "  matches = subgraph_rewriter.replace_pattern_with_filters(modify_traced, pattern, replace,[match])\n",
    "  ori = torch.jit.script(ori_traced)\n",
    "  modify = torch.jit.script(modify_traced)\n",
    "  ori.eval()\n",
    "  modify.eval()\n",
    "  return ori,modify\n",
    "  # return ori_traced,modify_traced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dim = 8, 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ori, modify = custom_workload_wdl_torchscript(8,100,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.12 ms ± 2.13 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(1024,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.98 ms ± 963 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(1024,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.74 ms ± 1.88 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.35 ms ± 1.21 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dim = 32,40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ori, modify = custom_workload_wdl_torchscript(32,100,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.28 ms ± 2.04 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(1024,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.65 ms ± 2.12 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(1024,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.5 ms ± 2.3 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.6 ms ± 2.08 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dim = 64,40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ori, modify = custom_workload_wdl_torchscript(64,100,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.03 ms ± 1.22 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(1024,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.98 ms ± 1.11 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(1024,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.7 ms ± 2.14 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 ms ± 1.9 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dim = 128,40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ori, modify = custom_workload_wdl_torchscript(128,100,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.8 ms ± 3.12 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(1024,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.55 ms ± 3.03 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(1024,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.2 ms ± 2.72 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.6 ms ± 2.71 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori,modify = custom_workload_wdl(64,100,70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.63 ms ± 1.66 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(1024,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.02 ms ± 1.17 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(1024,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.3 ms ± 3.12 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.6 ms ± 1.87 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(4096,100), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custom_workload 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori,modify = custom_workload_wdl(32,50,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.11 ms ± 878 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(1024,50), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.85 ms ± 802 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(1024,50), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.46 ms ± 1.69 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(4096,50), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.35 ms ± 1.17 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(4096,50), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori,modify = custom_workload_wdl(32,50,20,l = [1024,512,256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.46 ms ± 1.4 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(1024,50), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.78 ms ± 1.02 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(1024,50), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.7 ms ± 1.91 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(4096,50), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.6 ms ± 1.71 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(4096,50), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori,modify = custom_workload_wdl(64,50,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.12 ms ± 919 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(1024,50), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.53 ms ± 933 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(1024,50), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.4 ms ± 2.29 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(4096,50), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.9 ms ± 1.89 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(4096,50), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori,modify = custom_workload_wdl(64,50,20,l=[1024,512,256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.83 ms ± 1.38 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(1024,50), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.76 ms ± 1.24 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(1024,50), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.4 ms ± 2.35 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(4096,50), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.2 ms ± 2.17 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(4096,50), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def workload_wdl2(dim):\n",
    "  print(f\"now gen workload of wdl with config: dim: {dim}\")\n",
    "  wdl_model_ori = wd.WideAndDeepModel([100 for i in range(22)],dim,[1024,512,256],0.1)\n",
    "  ori_traced = symbolic_trace(wdl_model_ori)\n",
    "  \n",
    "  wdl_model_modify = wd.WideAndDeepModel([100 for i in range(22)],dim,[400,400,400],0.1)\n",
    "  modify_traced = symbolic_trace(wdl_model_modify)\n",
    "  pattern,replace,match = gen_pattern_replace_and_matcher_for_MLP2(modify_traced,\n",
    "                                                                      (0,slice(None,10,None)),(slice(None,None,None),slice(10,None,None)),\n",
    "                                                                      \"mlp_mlp_0\")\n",
    "  matches = subgraph_rewriter.replace_pattern_with_filters(modify_traced, pattern, replace,[match])\n",
    "  return ori_traced,modify_traced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node offsets target offsets offsets of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori , modify = workload_wdl2(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.13 ms ± 1.25 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4 ms ± 702 µs per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.4 ms ± 4.78 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = ori(torch.randint(low=0, high=88, size=(4096,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.68 ms ± 1.33 ms per loop (mean ± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 30 with torch.no_grad(): output = modify(torch.randint(low=0, high=88, size=(4096,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time: 4.895687103271484 ms\n",
      "Op type        Op                     Average runtime (ms)    Pct total runtime\n",
      "-------------  -------------------  ----------------------  -------------------\n",
      "call_module    mlp_mlp_3                         0.578403             11.8146\n",
      "call_module    mlp_mlp_6                         0.54884              11.2107\n",
      "call_module    linear_fc                         0.42057               8.59063\n",
      "call_module    mlp_mlp_0                         0.409126              8.35687\n",
      "call_module    mlp_mlp_5                         0.401735              8.2059\n",
      "call_module    mlp_mlp_2                         0.380516              7.77247\n",
      "call_module    mlp_mlp_8                         0.347376              7.09555\n",
      "call_module    embedding_embedding               0.342369              6.99328\n",
      "call_module    mlp_mlp_9                         0.158548              3.23853\n",
      "call_module    mlp_mlp_7                         0.146866              2.9999\n",
      "call_function  add_1                             0.138998              2.83919\n",
      "call_module    mlp_mlp_4                         0.138998              2.83919\n",
      "call_module    mlp_mlp_1                         0.136852              2.79536\n",
      "call_function  sum_1                             0.0793934             1.6217\n",
      "call_function  add                               0.0762939             1.55839\n",
      "get_attr       linear_offsets                    0.0598431             1.22236\n",
      "call_method    view                              0.0457764             0.935035\n",
      "call_function  add_3                             0.0352859             0.720756\n",
      "call_method    squeeze                           0.0348091             0.711016\n",
      "call_function  add_2                             0.0331402             0.676926\n",
      "call_function  sigmoid                           0.0321865             0.657446\n",
      "placeholder    x                                 0.0286102             0.584397\n",
      "get_attr       embedding_offsets                 0.027895              0.569787\n",
      "get_attr       linear_bias                       0.0252724             0.516217\n",
      "output         output                            0.0152588             0.311678\n"
     ]
    }
   ],
   "source": [
    "interp = utils.ProfilingInterpreter(ori)\n",
    "interp.run(torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long))\n",
    "print(interp.summary(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scripted_model_ori = torch.jit.script(ori)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=WideAndDeepModel\n",
       "  (embedding): RecursiveScriptModule(\n",
       "    original_name=Module\n",
       "    (embedding): RecursiveScriptModule(original_name=Embedding)\n",
       "  )\n",
       "  (linear): RecursiveScriptModule(\n",
       "    original_name=Module\n",
       "    (fc): RecursiveScriptModule(original_name=Embedding)\n",
       "  )\n",
       "  (mlp): RecursiveScriptModule(\n",
       "    original_name=Module\n",
       "    (mlp): RecursiveScriptModule(\n",
       "      original_name=Module\n",
       "      (0): RecursiveScriptModule(original_name=Linear)\n",
       "      (1): RecursiveScriptModule(original_name=ReLU)\n",
       "      (2): RecursiveScriptModule(original_name=Dropout)\n",
       "      (3): RecursiveScriptModule(original_name=Linear)\n",
       "      (4): RecursiveScriptModule(original_name=ReLU)\n",
       "      (5): RecursiveScriptModule(original_name=Dropout)\n",
       "      (6): RecursiveScriptModule(original_name=Linear)\n",
       "      (7): RecursiveScriptModule(original_name=ReLU)\n",
       "      (8): RecursiveScriptModule(original_name=Dropout)\n",
       "      (9): RecursiveScriptModule(original_name=Linear)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scripted_model_ori.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.07 ms ± 367 µs per loop (mean ± std. dev. of 3 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 -r 3 with torch.no_grad(): output = scripted_model_ori(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = torch.ones((4096,22), dtype=torch.long)\n",
    "\n",
    "\n",
    "torch.onnx.export(ori,sample,f'ori.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_model = torch.jit.script(modify)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=WideAndDeepModel\n",
       "  (linear): RecursiveScriptModule(\n",
       "    original_name=Module\n",
       "    (fc): RecursiveScriptModule(original_name=Embedding)\n",
       "  )\n",
       "  (mlp): RecursiveScriptModule(\n",
       "    original_name=Module\n",
       "    (mlp): RecursiveScriptModule(\n",
       "      original_name=Module\n",
       "      (1): RecursiveScriptModule(original_name=ReLU)\n",
       "      (2): RecursiveScriptModule(original_name=Dropout)\n",
       "      (3): RecursiveScriptModule(original_name=Linear)\n",
       "      (4): RecursiveScriptModule(original_name=ReLU)\n",
       "      (5): RecursiveScriptModule(original_name=Dropout)\n",
       "      (6): RecursiveScriptModule(original_name=Linear)\n",
       "      (7): RecursiveScriptModule(original_name=ReLU)\n",
       "      (8): RecursiveScriptModule(original_name=Dropout)\n",
       "      (9): RecursiveScriptModule(original_name=Linear)\n",
       "    )\n",
       "  )\n",
       "  (embed): RecursiveScriptModule(original_name=Embedding)\n",
       "  (redency_linear): RecursiveScriptModule(original_name=Linear)\n",
       "  (unredency_linear): RecursiveScriptModule(original_name=Linear)\n",
       ")"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scripted_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.28 ms ± 348 µs per loop (mean ± std. dev. of 3 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 -r 3 with torch.no_grad(): output = scripted_model( torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time: 14.651298522949219 ms\n",
      "Op type        Op                  Average runtime (ms)    Pct total runtime\n",
      "-------------  ----------------  ----------------------  -------------------\n",
      "call_module    unredency_linear               8.31437             56.7484\n",
      "call_module    mlp_mlp_2                      0.823259             5.61902\n",
      "call_module    mlp_mlp_3                      0.820398             5.59949\n",
      "call_module    mlp_mlp_6                      0.421047             2.87379\n",
      "call_module    mlp_mlp_5                      0.388145             2.64922\n",
      "call_module    embed                          0.346899             2.3677\n",
      "call_module    mlp_mlp_8                      0.329494             2.24891\n",
      "call_module    linear_fc                      0.293732             2.00482\n",
      "call_module    mlp_mlp_1                      0.286341             1.95437\n",
      "call_module    redency_linear                 0.262737             1.79327\n",
      "call_function  getitem_1                      0.256062             1.74771\n",
      "call_function  add_3                          0.237942             1.62403\n",
      "call_module    mlp_mlp_4                      0.218153             1.48897\n",
      "call_function  add                            0.176668             1.20582\n",
      "call_module    mlp_mlp_7                      0.151634             1.03495\n",
      "call_module    mlp_mlp_9                      0.150442             1.02682\n",
      "call_method    view_1                         0.138044             0.942199\n",
      "call_function  sum_1                          0.102997             0.702988\n",
      "call_function  getitem                        0.0970364            0.662306\n",
      "call_function  add_2                          0.0674725            0.460522\n",
      "call_method    view                           0.0665188            0.454013\n",
      "call_method    squeeze                        0.0426769            0.291284\n",
      "call_function  add_1                          0.0412464            0.281521\n",
      "placeholder    x                              0.0395775            0.27013\n",
      "get_attr       linear_offsets                 0.0343323            0.234329\n",
      "call_function  add_4                          0.0338554            0.231075\n",
      "call_function  sigmoid                        0.0326633            0.222938\n",
      "get_attr       linear_bias                    0.0257492            0.175747\n",
      "get_attr       offsets                        0.0188351            0.128556\n",
      "output         output                         0.0138283            0.0943826\n"
     ]
    }
   ],
   "source": [
    "interp = utils.ProfilingInterpreter(modify)\n",
    "interp.run(torch.randint(low=0, high=88, size=(1024,22), dtype=torch.long))\n",
    "print(interp.summary(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = torch.ones((4096,22), dtype=torch.long)\n",
    "\n",
    "\n",
    "torch.onnx.export(modify,sample,f'modify.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori,modify = custom_workload_wdl(64,100,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.4598649342854815"
      ]
     },
     "execution_count": 624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "\n",
    "for _ in range(30):\n",
    "  input_data = torch.randint(low=0, high=88, size=(1024,100), dtype=torch.long)\n",
    "  start = time.time()\n",
    "  with torch.no_grad():ori(input_data)\n",
    "  end = time.time()\n",
    "  sum += (end - start) * 1000\n",
    "sum / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.596138000488281"
      ]
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "\n",
    "for _ in range(30):\n",
    "  input_data = torch.randint(low=0, high=88, size=(1024,100), dtype=torch.long)\n",
    "  start = time.time()\n",
    "  with torch.no_grad():modify(input_data)\n",
    "  end = time.time()\n",
    "  sum += (end - start) * 1000\n",
    "sum / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.384627978007"
      ]
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "ori.to(device)\n",
    "for _ in range(30):\n",
    "  input_data = torch.randint(low=0, high=88, size=(1024,100), dtype=torch.long).to(device)\n",
    "  start = time.time()\n",
    "  with torch.no_grad():ori(input_data)\n",
    "  end = time.time()\n",
    "  sum += (end - start) * 1000\n",
    "sum / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph_module.py\", line 304, in __call__\n",
      "    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"<eval_with_key>.490 from /home/yssun/pytorch-fm/torchfm/model/test_fx/../wd.py:22 in forward\", line 13, in forward\n",
      "    add_4 = getitem + redency_offset;  getitem = redency_offset = None\n",
      "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n",
      "\n",
      "Call using an FX-traced Module, line 13 of the traced Module's generated forward function:\n",
      "    redency_offset = self.redency_offset\n",
      "    add_4 = getitem + redency_offset;  getitem = redency_offset = None\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n",
      "    getitem_1 = x[(slice(None, None, None), slice(40, None, None))];  x = None\n",
      "\n",
      "    unredency_offset = self.unredency_offset\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[627], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m input_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m88\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1024\u001b[39m,\u001b[38;5;241m100\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():modify(input_data)\n\u001b[1;32m      7\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28msum\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (end \u001b[38;5;241m-\u001b[39m start) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph_module.py:737\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph_module.py:315\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_with_key\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m topmost_framesummary\u001b[38;5;241m.\u001b[39mfilename:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    312\u001b[0m         _WrappedCall\u001b[38;5;241m.\u001b[39m_generate_error_message(topmost_framesummary),\n\u001b[1;32m    313\u001b[0m         file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m    314\u001b[0m     )\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# noqa: TRY200\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "modify.to(device)\n",
    "for _ in range(30):\n",
    "  input_data = torch.randint(low=0, high=88, size=(1024,100), dtype=torch.long).to(device)\n",
    "  start = time.time()\n",
    "  with torch.no_grad():modify(input_data)\n",
    "  end = time.time()\n",
    "  sum += (end - start) * 1000\n",
    "sum / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori,modify = custom_workload_wdl(64,100,70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.074372927347819"
      ]
     },
     "execution_count": 621,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "\n",
    "for _ in range(30):\n",
    "  input_data = torch.randint(low=0, high=88, size=(1024,100), dtype=torch.long)\n",
    "  start = time.time()\n",
    "  with torch.no_grad():ori(input_data)\n",
    "  end = time.time()\n",
    "  sum += (end - start) * 1000\n",
    "sum / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7813663482666016"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "\n",
    "for _ in range(30):\n",
    "  input_data = torch.randint(low=0, high=88, size=(1024,100), dtype=torch.long)\n",
    "  start = time.time()\n",
    "  with torch.no_grad():modify(input_data)\n",
    "  end = time.time()\n",
    "  sum += (end - start) * 1000\n",
    "sum / 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frappe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dim = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori,modify = custom_workload_wdl(8,10,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.389383316040039"
      ]
     },
     "execution_count": 647,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "\n",
    "for _ in range(30):\n",
    "  input_data = torch.randint(low=0, high=88, size=(1024,10), dtype=torch.long)\n",
    "  start = time.time()\n",
    "  with torch.no_grad():ori(input_data)\n",
    "  end = time.time()\n",
    "  sum += (end - start) * 1000\n",
    "sum / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.751773834228516"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "\n",
    "for _ in range(30):\n",
    "  input_data = torch.randint(low=0, high=88, size=(1024,10), dtype=torch.long)\n",
    "  start = time.time()\n",
    "  with torch.no_grad():modify(input_data)\n",
    "  end = time.time()\n",
    "  sum += (end - start) * 1000\n",
    "sum / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.392918904622396"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "\n",
    "for _ in range(30):\n",
    "  input_data = torch.randint(low=0, high=88, size=(4096,10), dtype=torch.long)\n",
    "  start = time.time()\n",
    "  with torch.no_grad():ori(input_data)\n",
    "  end = time.time()\n",
    "  sum += (end - start) * 1000\n",
    "sum / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.2283935546875"
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "\n",
    "for _ in range(30):\n",
    "  input_data = torch.randint(low=0, high=88, size=(4096,10), dtype=torch.long)\n",
    "  start = time.time()\n",
    "  with torch.no_grad():modify(input_data)\n",
    "  end = time.time()\n",
    "  sum += (end - start) * 1000\n",
    "sum / 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori,modify = custom_workload_wdl(32,10,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.416865030924479"
      ]
     },
     "execution_count": 653,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "\n",
    "for _ in range(30):\n",
    "  input_data = torch.randint(low=0, high=88, size=(1024,10), dtype=torch.long)\n",
    "  start = time.time()\n",
    "  with torch.no_grad():ori(input_data)\n",
    "  end = time.time()\n",
    "  sum += (end - start) * 1000\n",
    "sum / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0974547068277993"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "\n",
    "for _ in range(30):\n",
    "  input_data = torch.randint(low=0, high=88, size=(1024,10), dtype=torch.long)\n",
    "  start = time.time()\n",
    "  with torch.no_grad():modify(input_data)\n",
    "  end = time.time()\n",
    "  sum += (end - start) * 1000\n",
    "sum / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4563541412353516"
      ]
     },
     "execution_count": 655,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "\n",
    "for _ in range(30):\n",
    "  input_data = torch.randint(low=0, high=88, size=(4096,10), dtype=torch.long)\n",
    "  start = time.time()\n",
    "  with torch.no_grad():ori(input_data)\n",
    "  end = time.time()\n",
    "  sum += (end - start) * 1000\n",
    "sum / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.63411808013916"
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "\n",
    "for _ in range(30):\n",
    "  input_data = torch.randint(low=0, high=88, size=(4096,10), dtype=torch.long)\n",
    "  start = time.time()\n",
    "  with torch.no_grad():modify(input_data)\n",
    "  end = time.time()\n",
    "  sum += (end - start) * 1000\n",
    "sum / 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori,modify = custom_workload_wdl(64,10,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5571584701538086"
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "\n",
    "for _ in range(30):\n",
    "  input_data = torch.randint(low=0, high=88, size=(1024,10), dtype=torch.long)\n",
    "  start = time.time()\n",
    "  with torch.no_grad():ori(input_data)\n",
    "  end = time.time()\n",
    "  sum += (end - start) * 1000\n",
    "sum / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2270520528157554"
      ]
     },
     "execution_count": 659,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "\n",
    "for _ in range(30):\n",
    "  input_data = torch.randint(low=0, high=88, size=(1024,10), dtype=torch.long)\n",
    "  start = time.time()\n",
    "  with torch.no_grad():modify(input_data)\n",
    "  end = time.time()\n",
    "  sum += (end - start) * 1000\n",
    "sum / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.056811332702637"
      ]
     },
     "execution_count": 660,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "\n",
    "for _ in range(30):\n",
    "  input_data = torch.randint(low=0, high=88, size=(4096,10), dtype=torch.long)\n",
    "  start = time.time()\n",
    "  with torch.no_grad():ori(input_data)\n",
    "  end = time.time()\n",
    "  sum += (end - start) * 1000\n",
    "sum / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9038031895955405"
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "\n",
    "for _ in range(30):\n",
    "  input_data = torch.randint(low=0, high=88, size=(4096,10), dtype=torch.long)\n",
    "  start = time.time()\n",
    "  with torch.no_grad():modify(input_data)\n",
    "  end = time.time()\n",
    "  sum += (end - start) * 1000\n",
    "sum / 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now gen workload of wdl with config: dim: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node redency_offset target redency_offset redency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/yssun/miniconda3/envs/deepctr-torch/lib/python3.9/site-packages/torch/fx/graph.py:1460: UserWarning: Node unredency_offset target unredency_offset unredency_offset of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    }
   ],
   "source": [
    "ori,modify = custom_workload_wdl(128,10,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9439528783162436"
      ]
     },
     "execution_count": 663,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "\n",
    "for _ in range(30):\n",
    "  input_data = torch.randint(low=0, high=88, size=(1024,10), dtype=torch.long)\n",
    "  start = time.time()\n",
    "  with torch.no_grad():ori(input_data)\n",
    "  end = time.time()\n",
    "  sum += (end - start) * 1000\n",
    "sum / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.367417017618815"
      ]
     },
     "execution_count": 664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "\n",
    "for _ in range(30):\n",
    "  input_data = torch.randint(low=0, high=88, size=(1024,10), dtype=torch.long)\n",
    "  start = time.time()\n",
    "  with torch.no_grad():modify(input_data)\n",
    "  end = time.time()\n",
    "  sum += (end - start) * 1000\n",
    "sum / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.027413368225098"
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "\n",
    "for _ in range(30):\n",
    "  input_data = torch.randint(low=0, high=88, size=(4096,10), dtype=torch.long)\n",
    "  start = time.time()\n",
    "  with torch.no_grad():ori(input_data)\n",
    "  end = time.time()\n",
    "  sum += (end - start) * 1000\n",
    "sum / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.300762176513672"
      ]
     },
     "execution_count": 666,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "\n",
    "for _ in range(30):\n",
    "  input_data = torch.randint(low=0, high=88, size=(4096,10), dtype=torch.long)\n",
    "  start = time.time()\n",
    "  with torch.no_grad():modify(input_data)\n",
    "  end = time.time()\n",
    "  sum += (end - start) * 1000\n",
    "sum / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union, Any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class redundancy_shape_info:\n",
    "  def __init__(self, free_dim: int,\n",
    "               slice_list: tuple[int | slice | list[int], ...]):\n",
    "    self.free_dim = free_dim\n",
    "    self.slice_list = slice_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class redundancy_shape_info:\n",
    "  def __init__(self, orientation_dim: int,\n",
    "               subscript_list: tuple[list[int], ...]):\n",
    "    self.orientation_dim = orientation_dim\n",
    "    self.subscript_list = subscript_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundancy_shape_info(free_dim = 0, slice_list = (0, slice(None,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundancy_shape_info(free_dim = 0, slice_list = [3, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundancy_shape_info(free_dim = 0, slice_list = [0, 1, 3, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepctr-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
