{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.fx import subgraph_rewriter, symbolic_trace\n",
    "from torch.fx import Proxy, Graph, GraphModule\n",
    "from torch.fx.passes.utils.matcher_utils import SubgraphMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReFeaturesLinear(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, field_dims, prefix,batch,output_dim=1):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Embedding(sum(field_dims), output_dim)\n",
    "        self.bias = torch.nn.Parameter(torch.zeros((output_dim,)))\n",
    "        self.offsets = torch.as_tensor(np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.int64))\n",
    "        self.prefix = prefix\n",
    "        self.batch = batch\n",
    "        self.prefix_offsets = self.offsets[:self.prefix]\n",
    "        self.rest_offsets = self.offsets[self.prefix:]\n",
    "        self.prefix_slice = [0,slice(None,prefix)]\n",
    "        self.rest_slice = [slice(None,None),slice(prefix,None)]\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: tuple of tensor ``(prefix_index, rest_index)``\n",
    "        prefix_index ``(prefix_field)``\n",
    "        rest_index  ``(batch_szie,rest_field)``\n",
    "        \"\"\"\n",
    "        prefix_index, rest_index = x[self.prefix_slice], x[self.rest_slice]\n",
    "        prefix_index = prefix_index + self.prefix_offsets\n",
    "        rest_index = rest_index + self.rest_offsets\n",
    "        prefix_index = prefix_index.repeat(self.batch,1)\n",
    "        index = torch.concat([prefix_index,rest_index],dim = 1)\n",
    "        prefix_index, rest_index = index[self.prefix_slice], index[self.rest_slice]\n",
    "        \n",
    "        prefix_fc = self.fc(prefix_index) + self.bias\n",
    "        prefix_fc = prefix_fc.repeat(self.batch,1,1)\n",
    "        rest_fc = self.fc(rest_index)\n",
    "        fc_result = torch.concat([prefix_fc,rest_fc],dim = 1)\n",
    "        prefix_fc = fc_result[self.prefix_slice]\n",
    "        rest_fc = fc_result[self.rest_slice]\n",
    "        prefix_sum = torch.sum(prefix_fc,dim = 0)\n",
    "        rest_sum = torch.sum(rest_fc,dim=1)\n",
    "        return prefix_sum + rest_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace success!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/yssun/pytorch-fm/torchfm/')\n",
    "sys.path.append('/home/yssun/pytorch-fm/torchfm/model/')\n",
    "sys.path.append('/home/yssun/pytorch-fm/torchfm/model/test_fx')\n",
    "import utils\n",
    "from layer import FeaturesLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReFeaturesEmbedding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, field_dims, embed_dim,prefix,batch):\n",
    "        super().__init__()\n",
    "        self.prefix = prefix\n",
    "        self.embedding = torch.nn.Embedding(sum(field_dims), embed_dim)\n",
    "        self.offsets = torch.as_tensor(np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.int64))\n",
    "        self.prefix_offsets = self.offsets[:self.prefix]\n",
    "        self.rest_offsets = self.offsets[self.prefix:]\n",
    "        self.batch = batch\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
    "        self.prefix_slice = [0,slice(None,prefix)]\n",
    "        self.rest_slice = [slice(None,None),slice(prefix,None)]\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: tuple of tensor ``(prefix_index, rest_index)``\n",
    "        prefix_index ``(prefix_field)``\n",
    "        rest_index  ``(batch_szie,rest_field)``\n",
    "        \"\"\"\n",
    "        prefix_index, rest_index = x[self.prefix_slice],x[self.rest_slice]\n",
    "        prefix_index = prefix_index + self.prefix_offsets\n",
    "        rest_index = rest_index + self.rest_offsets\n",
    "        prefix_index = prefix_index.repeat(self.batch,1)\n",
    "        index = torch.concat([prefix_index,rest_index],dim = 1)\n",
    "        prefix_index, rest_index = index[self.prefix_slice], index[self.rest_slice]\n",
    "        return torch.concat([self.embedding(prefix_index).repeat(self.batch,1,1),self.embedding(rest_index)],dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReFactorizationMachine(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,prefix,batch, reduce_sum=True):\n",
    "        super().__init__()\n",
    "        self.reduce_sum = reduce_sum\n",
    "        self.batch = batch\n",
    "        self.prefix_slice = [0,slice(None,prefix)]\n",
    "        self.rest_slice = [slice(None,None),slice(prefix,None)]\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
    "        \"\"\"\n",
    "        prefix_embed, rest_embed = x[self.prefix_slice],x[self.rest_slice]\n",
    "        # square_of_sum = torch.sum(x, dim=1) ** 2\n",
    "        square_of_sum = (torch.sum(rest_embed,dim = 1) + torch.sum(prefix_embed,dim = 0)) ** 2\n",
    "        prefix_embed, rest_embed = prefix_embed ** 2, rest_embed ** 2\n",
    "        pow_embed = torch.concat([prefix_embed.repeat(self.batch,1,1),rest_embed],dim = 1)\n",
    "        pow_prefix_embed, pow_rest_embed = pow_embed[self.prefix_slice],pow_embed[self.rest_slice]\n",
    "        sum_of_square = torch.sum(pow_rest_embed, dim=1) + torch.sum(pow_prefix_embed, dim = 0)\n",
    "        ix = square_of_sum - sum_of_square\n",
    "        if self.reduce_sum:\n",
    "            ix = torch.sum(ix, dim=1, keepdim=True)\n",
    "        return 0.5 * ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReFactorizationMachineModel(torch.nn.Module):\n",
    "  def __init__(self, field_dims, embed_dim,prefix,batch):\n",
    "    super().__init__()\n",
    "    self.embedding = ReFeaturesEmbedding(field_dims, embed_dim,prefix,batch)\n",
    "    self.linear = ReFeaturesLinear(field_dims,prefix,batch)\n",
    "    self.fm = ReFactorizationMachine(prefix,batch,reduce_sum=True)\n",
    "\n",
    "  def forward(self, x):\n",
    "      \"\"\"\n",
    "      :param x: Long tensor of size ``(batch_size, num_fields)``\n",
    "      \"\"\"\n",
    "      x = self.linear(x) + self.fm(self.embedding(x))\n",
    "      return torch.sigmoid(x.squeeze(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_model = ReFactorizationMachineModel([100 for _ in range(100)],32,30,4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_ori_model = fm.FactorizationMachineModel([100 for _ in range(100)],64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_ori_model_traced = symbolic_trace(fm_ori_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_model = ReFactorizationMachineModel([100 for _ in range(100)],64,50,4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_modify_model_traced = symbolic_trace(fm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total true time 23.090600967407227 ms\n",
      "total time: 33.370018005371094 ms\n",
      "Op type        Op                     Average runtime (ms)    Pct total runtime\n",
      "-------------  -------------------  ----------------------  -------------------\n",
      "call_module    embedding_embedding               8.65006             25.9217\n",
      "call_function  pow_2                             6.76298             20.2666\n",
      "call_function  sum_2                             2.67673              8.02135\n",
      "call_function  sum_3                             2.20108              6.59598\n",
      "call_module    linear_fc                         0.791311             2.37132\n",
      "call_function  add_2                             0.470877             1.41108\n",
      "call_function  sub                               0.423908             1.27033\n",
      "call_function  pow_1                             0.338316             1.01383\n",
      "call_function  add                               0.255108             0.764482\n",
      "call_function  sum_1                             0.104427             0.312938\n",
      "call_function  sum_4                             0.0920296            0.275785\n",
      "call_function  mul                               0.079155             0.237204\n",
      "call_method    squeeze                           0.0379086            0.113601\n",
      "call_function  sigmoid                           0.0371933            0.111457\n",
      "call_function  add_1                             0.0329018            0.0985968\n",
      "call_function  add_3                             0.0305176            0.0914521\n",
      "placeholder    x                                 0.0274181            0.082164\n",
      "get_attr       linear_bias                       0.0240803            0.0721614\n",
      "get_attr       linear_offsets                    0.0238419            0.0714469\n",
      "get_attr       embedding_offsets                 0.0178814            0.0535852\n",
      "output         output                            0.0128746            0.0385813\n"
     ]
    }
   ],
   "source": [
    "interp = utils.ProfilingInterpreter(fm_ori_model_traced)\n",
    "input_data = torch.zeros((4096,100),dtype=torch.long)\n",
    "interp.run(input_data)\n",
    "print(interp.summary(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ReFactorizationMachineModel(torch.nn.Module):\n",
      "    def forward(self, x):\n",
      "        # No stacktrace found for following nodes\n",
      "        getitem = x[[0, slice(None, 50, None)]]\n",
      "        getitem_1 = x[[slice(None, None, None), slice(50, None, None)]]\n",
      "        linear_prefix_offsets = self.linear.prefix_offsets\n",
      "        add = getitem + linear_prefix_offsets;  getitem = linear_prefix_offsets = None\n",
      "        linear_rest_offsets = self.linear.rest_offsets\n",
      "        add_1 = getitem_1 + linear_rest_offsets;  getitem_1 = linear_rest_offsets = None\n",
      "        repeat = add.repeat(4096, 1);  add = None\n",
      "        concat = torch.concat([repeat, add_1], dim = 1);  repeat = add_1 = None\n",
      "        getitem_2 = concat[[0, slice(None, 50, None)]]\n",
      "        getitem_3 = concat[[slice(None, None, None), slice(50, None, None)]];  concat = None\n",
      "        linear_fc = self.linear.fc(getitem_2);  getitem_2 = None\n",
      "        linear_bias = self.linear.bias\n",
      "        add_2 = linear_fc + linear_bias;  linear_fc = linear_bias = None\n",
      "        repeat_1 = add_2.repeat(4096, 1, 1);  add_2 = None\n",
      "        linear_fc_1 = self.linear.fc(getitem_3);  getitem_3 = None\n",
      "        concat_1 = torch.concat([repeat_1, linear_fc_1], dim = 1);  repeat_1 = linear_fc_1 = None\n",
      "        getitem_4 = concat_1[[0, slice(None, 50, None)]]\n",
      "        getitem_5 = concat_1[[slice(None, None, None), slice(50, None, None)]];  concat_1 = None\n",
      "        sum_1 = torch.sum(getitem_4, dim = 0);  getitem_4 = None\n",
      "        sum_2 = torch.sum(getitem_5, dim = 1);  getitem_5 = None\n",
      "        add_3 = sum_1 + sum_2;  sum_1 = sum_2 = None\n",
      "        getitem_6 = x[[0, slice(None, 50, None)]]\n",
      "        getitem_7 = x[[slice(None, None, None), slice(50, None, None)]];  x = None\n",
      "        embedding_prefix_offsets = self.embedding.prefix_offsets\n",
      "        add_4 = getitem_6 + embedding_prefix_offsets;  getitem_6 = embedding_prefix_offsets = None\n",
      "        embedding_rest_offsets = self.embedding.rest_offsets\n",
      "        add_5 = getitem_7 + embedding_rest_offsets;  getitem_7 = embedding_rest_offsets = None\n",
      "        repeat_2 = add_4.repeat(4096, 1);  add_4 = None\n",
      "        concat_2 = torch.concat([repeat_2, add_5], dim = 1);  repeat_2 = add_5 = None\n",
      "        getitem_8 = concat_2[[0, slice(None, 50, None)]]\n",
      "        getitem_9 = concat_2[[slice(None, None, None), slice(50, None, None)]];  concat_2 = None\n",
      "        embedding_embedding = self.embedding.embedding(getitem_8);  getitem_8 = None\n",
      "        repeat_3 = embedding_embedding.repeat(4096, 1, 1);  embedding_embedding = None\n",
      "        embedding_embedding_1 = self.embedding.embedding(getitem_9);  getitem_9 = None\n",
      "        concat_3 = torch.concat([repeat_3, embedding_embedding_1], dim = 1);  repeat_3 = embedding_embedding_1 = None\n",
      "        getitem_10 = concat_3[[0, slice(None, 50, None)]]\n",
      "        getitem_11 = concat_3[[slice(None, None, None), slice(50, None, None)]];  concat_3 = None\n",
      "        sum_3 = torch.sum(getitem_11, dim = 1)\n",
      "        sum_4 = torch.sum(getitem_10, dim = 0)\n",
      "        add_6 = sum_3 + sum_4;  sum_3 = sum_4 = None\n",
      "        pow_1 = add_6 ** 2;  add_6 = None\n",
      "        pow_2 = getitem_10 ** 2;  getitem_10 = None\n",
      "        pow_3 = getitem_11 ** 2;  getitem_11 = None\n",
      "        repeat_4 = pow_2.repeat(4096, 1, 1);  pow_2 = None\n",
      "        concat_4 = torch.concat([repeat_4, pow_3], dim = 1);  repeat_4 = pow_3 = None\n",
      "        getitem_12 = concat_4[[0, slice(None, 50, None)]]\n",
      "        getitem_13 = concat_4[[slice(None, None, None), slice(50, None, None)]];  concat_4 = None\n",
      "        sum_5 = torch.sum(getitem_13, dim = 1);  getitem_13 = None\n",
      "        sum_6 = torch.sum(getitem_12, dim = 0);  getitem_12 = None\n",
      "        add_7 = sum_5 + sum_6;  sum_5 = sum_6 = None\n",
      "        sub = pow_1 - add_7;  pow_1 = add_7 = None\n",
      "        sum_7 = torch.sum(sub, dim = 1, keepdim = True);  sub = None\n",
      "        mul = 0.5 * sum_7;  sum_7 = None\n",
      "        add_8 = add_3 + mul;  add_3 = mul = None\n",
      "        squeeze = add_8.squeeze(1);  add_8 = None\n",
      "        sigmoid = torch.sigmoid(squeeze);  squeeze = None\n",
      "        return sigmoid\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'class ReFactorizationMachineModel(torch.nn.Module):\\n    def forward(self, x):\\n        # No stacktrace found for following nodes\\n        getitem = x[[0, slice(None, 50, None)]]\\n        getitem_1 = x[[slice(None, None, None), slice(50, None, None)]]\\n        linear_prefix_offsets = self.linear.prefix_offsets\\n        add = getitem + linear_prefix_offsets;  getitem = linear_prefix_offsets = None\\n        linear_rest_offsets = self.linear.rest_offsets\\n        add_1 = getitem_1 + linear_rest_offsets;  getitem_1 = linear_rest_offsets = None\\n        repeat = add.repeat(4096, 1);  add = None\\n        concat = torch.concat([repeat, add_1], dim = 1);  repeat = add_1 = None\\n        getitem_2 = concat[[0, slice(None, 50, None)]]\\n        getitem_3 = concat[[slice(None, None, None), slice(50, None, None)]];  concat = None\\n        linear_fc = self.linear.fc(getitem_2);  getitem_2 = None\\n        linear_bias = self.linear.bias\\n        add_2 = linear_fc + linear_bias;  linear_fc = linear_bias = None\\n        repeat_1 = add_2.repeat(4096, 1, 1);  add_2 = None\\n        linear_fc_1 = self.linear.fc(getitem_3);  getitem_3 = None\\n        concat_1 = torch.concat([repeat_1, linear_fc_1], dim = 1);  repeat_1 = linear_fc_1 = None\\n        getitem_4 = concat_1[[0, slice(None, 50, None)]]\\n        getitem_5 = concat_1[[slice(None, None, None), slice(50, None, None)]];  concat_1 = None\\n        sum_1 = torch.sum(getitem_4, dim = 0);  getitem_4 = None\\n        sum_2 = torch.sum(getitem_5, dim = 1);  getitem_5 = None\\n        add_3 = sum_1 + sum_2;  sum_1 = sum_2 = None\\n        getitem_6 = x[[0, slice(None, 50, None)]]\\n        getitem_7 = x[[slice(None, None, None), slice(50, None, None)]];  x = None\\n        embedding_prefix_offsets = self.embedding.prefix_offsets\\n        add_4 = getitem_6 + embedding_prefix_offsets;  getitem_6 = embedding_prefix_offsets = None\\n        embedding_rest_offsets = self.embedding.rest_offsets\\n        add_5 = getitem_7 + embedding_rest_offsets;  getitem_7 = embedding_rest_offsets = None\\n        repeat_2 = add_4.repeat(4096, 1);  add_4 = None\\n        concat_2 = torch.concat([repeat_2, add_5], dim = 1);  repeat_2 = add_5 = None\\n        getitem_8 = concat_2[[0, slice(None, 50, None)]]\\n        getitem_9 = concat_2[[slice(None, None, None), slice(50, None, None)]];  concat_2 = None\\n        embedding_embedding = self.embedding.embedding(getitem_8);  getitem_8 = None\\n        repeat_3 = embedding_embedding.repeat(4096, 1, 1);  embedding_embedding = None\\n        embedding_embedding_1 = self.embedding.embedding(getitem_9);  getitem_9 = None\\n        concat_3 = torch.concat([repeat_3, embedding_embedding_1], dim = 1);  repeat_3 = embedding_embedding_1 = None\\n        getitem_10 = concat_3[[0, slice(None, 50, None)]]\\n        getitem_11 = concat_3[[slice(None, None, None), slice(50, None, None)]];  concat_3 = None\\n        sum_3 = torch.sum(getitem_11, dim = 1)\\n        sum_4 = torch.sum(getitem_10, dim = 0)\\n        add_6 = sum_3 + sum_4;  sum_3 = sum_4 = None\\n        pow_1 = add_6 ** 2;  add_6 = None\\n        pow_2 = getitem_10 ** 2;  getitem_10 = None\\n        pow_3 = getitem_11 ** 2;  getitem_11 = None\\n        repeat_4 = pow_2.repeat(4096, 1, 1);  pow_2 = None\\n        concat_4 = torch.concat([repeat_4, pow_3], dim = 1);  repeat_4 = pow_3 = None\\n        getitem_12 = concat_4[[0, slice(None, 50, None)]]\\n        getitem_13 = concat_4[[slice(None, None, None), slice(50, None, None)]];  concat_4 = None\\n        sum_5 = torch.sum(getitem_13, dim = 1);  getitem_13 = None\\n        sum_6 = torch.sum(getitem_12, dim = 0);  getitem_12 = None\\n        add_7 = sum_5 + sum_6;  sum_5 = sum_6 = None\\n        sub = pow_1 - add_7;  pow_1 = add_7 = None\\n        sum_7 = torch.sum(sub, dim = 1, keepdim = True);  sub = None\\n        mul = 0.5 * sum_7;  sum_7 = None\\n        add_8 = add_3 + mul;  add_3 = mul = None\\n        squeeze = add_8.squeeze(1);  add_8 = None\\n        sigmoid = torch.sigmoid(squeeze);  squeeze = None\\n        return sigmoid\\n        '"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_modify_model_traced.print_readable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total true time 35.35819053649902 ms\n",
      "total time: 63.648223876953125 ms\n",
      "Op type        Op                          Average runtime (ms)    Pct total runtime\n",
      "-------------  ------------------------  ----------------------  -------------------\n",
      "call_function  concat_3                               6.55317             10.2959\n",
      "call_function  concat_4                               5.97405              9.38605\n",
      "call_method    repeat_3                               5.29456              8.31847\n",
      "call_module    embedding_embedding_1                  4.0741               6.40096\n",
      "call_function  pow_3                                  2.9192               4.58645\n",
      "call_method    repeat_4                               2.7144               4.26468\n",
      "call_function  sum_3                                  1.13702              1.78641\n",
      "call_function  sum_5                                  1.13702              1.78641\n",
      "call_function  concat                                 0.916958             1.44067\n",
      "call_function  concat_2                               0.763416             1.19943\n",
      "call_module    linear_fc_1                            0.598907             0.940965\n",
      "call_function  add_7                                  0.384331             0.603836\n",
      "call_method    repeat                                 0.275135             0.432274\n",
      "call_function  add_1                                  0.195742             0.307537\n",
      "call_function  add_5                                  0.157595             0.247603\n",
      "call_function  getitem_10                             0.14472              0.227375\n",
      "call_function  getitem_12                             0.12517              0.196659\n",
      "call_function  concat_1                               0.124454             0.195535\n",
      "call_module    linear_fc                              0.115871             0.18205\n",
      "call_function  getitem                                0.109673             0.17231\n",
      "call_method    repeat_2                               0.103712             0.162946\n",
      "call_method    repeat_1                               0.0941753            0.147962\n",
      "call_function  sum_2                                  0.0870228            0.136725\n",
      "call_function  sum_6                                  0.0803471            0.126236\n",
      "call_function  add_6                                  0.0801086            0.125862\n",
      "call_function  sum_4                                  0.0798702            0.125487\n",
      "call_module    embedding_embedding                    0.0734329            0.115373\n",
      "call_function  mul                                    0.0710487            0.111627\n",
      "call_function  sub                                    0.0693798            0.109005\n",
      "call_function  pow_1                                  0.0648499            0.101888\n",
      "call_function  getitem_2                              0.0646114            0.101513\n",
      "call_function  sum_7                                  0.0627041            0.0985166\n",
      "call_function  getitem_8                              0.0452995            0.0711717\n",
      "call_function  getitem_4                              0.0441074            0.0692988\n",
      "call_function  add_3                                  0.0391006            0.0614324\n",
      "call_function  add                                    0.0379086            0.0595595\n",
      "call_function  sum_1                                  0.0360012            0.0565628\n",
      "call_function  sigmoid                                0.0352859            0.055439\n",
      "call_method    squeeze                                0.0324249            0.050944\n",
      "call_function  add_2                                  0.0317097            0.0498202\n",
      "placeholder    x                                      0.0293255            0.0460743\n",
      "call_function  getitem_6                              0.0281334            0.0442014\n",
      "call_function  getitem_5                              0.0267029            0.0419539\n",
      "call_function  getitem_1                              0.0259876            0.0408301\n",
      "call_function  getitem_11                             0.0252724            0.0397063\n",
      "get_attr       linear_prefix_offsets                  0.025034             0.0393317\n",
      "call_function  getitem_7                              0.025034             0.0393317\n",
      "get_attr       embedding_prefix_offsets               0.025034             0.0393317\n",
      "call_function  pow_2                                  0.0243187            0.038208\n",
      "call_function  add_8                                  0.0236034            0.0370842\n",
      "call_function  getitem_3                              0.0231266            0.036335\n",
      "call_function  getitem_9                              0.0226498            0.0355859\n",
      "call_function  getitem_13                             0.0216961            0.0340875\n",
      "call_function  add_4                                  0.0193119            0.0303416\n",
      "get_attr       embedding_rest_offsets                 0.017643             0.0277195\n",
      "get_attr       linear_bias                            0.0169277            0.0265957\n",
      "get_attr       linear_rest_offsets                    0.0162125            0.025472\n",
      "output         output                                 0.0135899            0.0213515\n"
     ]
    }
   ],
   "source": [
    "interp = utils.ProfilingInterpreter(fm_modify_model_traced)\n",
    "input_data = torch.zeros((4096,100),dtype=torch.long)\n",
    "interp.run(input_data)\n",
    "print(interp.summary(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReFactorizationMachine_pow_sum(torch.nn.Module):\n",
    "    # 对 sum(x ** 2)进行改写后的模块\n",
    "    def __init__(self,prefix,batch, reduce_sum=True):\n",
    "        super().__init__()\n",
    "        self.reduce_sum = reduce_sum\n",
    "        self.batch = batch\n",
    "        self.prefix_slice = [0,slice(None,prefix)]\n",
    "        self.rest_slice = [slice(None,None),slice(prefix,None)]\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
    "        \"\"\"\n",
    "        prefix_embed, rest_embed = x[self.prefix_slice],x[self.rest_slice]\n",
    "        # square_of_sum = torch.sum(x, dim=1) ** 2\n",
    "        square_of_sum = (torch.sum(rest_embed,dim = 1) + torch.sum(prefix_embed,dim = 0)) ** 2\n",
    "        prefix_embed, rest_embed = prefix_embed ** 2, rest_embed ** 2\n",
    "        # pow_embed = torch.concat([prefix_embed.repeat(self.batch,1,1),rest_embed],dim = 1)\n",
    "        # pow_prefix_embed, pow_rest_embed = pow_embed[self.prefix_slice],pow_embed[self.rest_slice]\n",
    "        sum_of_square = torch.sum(rest_embed, dim=1) + torch.sum(prefix_embed, dim = 0)\n",
    "        ix = square_of_sum - sum_of_square\n",
    "        if self.reduce_sum:\n",
    "            ix = torch.sum(ix, dim=1, keepdim=True)\n",
    "        return 0.5 * ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReFactorizationMachineModel_pow_sum(torch.nn.Module):\n",
    "  # 对 sum(x ** 2)进行改写后的FM\n",
    "  def __init__(self, field_dims, embed_dim,prefix,batch):\n",
    "    super().__init__()\n",
    "    self.embedding = ReFeaturesEmbedding(field_dims, embed_dim,prefix,batch)\n",
    "    self.linear = ReFeaturesLinear(field_dims,prefix,batch)\n",
    "    self.fm = ReFactorizationMachine_pow_sum(prefix,batch,reduce_sum=True)\n",
    "\n",
    "  def forward(self, x):\n",
    "      \"\"\"\n",
    "      :param x: Long tensor of size ``(batch_size, num_fields)``\n",
    "      \"\"\"\n",
    "      x = self.linear(x) + self.fm(self.embedding(x))\n",
    "      return torch.sigmoid(x.squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_model_pow_sum = ReFactorizationMachineModel_pow_sum([100 for _ in range(100)],64,50,4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_modify_model_pow_sum_traced = symbolic_trace(fm_model_pow_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total true time 23.701190948486328 ms\n",
      "total time: 37.69659996032715 ms\n",
      "Op type        Op                          Average runtime (ms)    Pct total runtime\n",
      "-------------  ------------------------  ----------------------  -------------------\n",
      "call_function  concat_3                               6.93965             18.4092\n",
      "call_method    repeat_3                               4.57954             12.1484\n",
      "call_module    embedding_embedding_1                  4.15516             11.0226\n",
      "call_function  pow_3                                  2.63143              6.98054\n",
      "call_function  sum_3                                  0.971794             2.57794\n",
      "call_function  sum_5                                  0.807047             2.1409\n",
      "call_module    linear_fc_1                            0.622034             1.65011\n",
      "call_function  concat                                 0.262976             0.697611\n",
      "call_function  concat_2                               0.213146             0.565426\n",
      "call_function  concat_1                               0.169277             0.449052\n",
      "call_function  add_1                                  0.166178             0.44083\n",
      "call_function  add_5                                  0.152111             0.403514\n",
      "call_method    repeat                                 0.122547             0.325088\n",
      "call_function  getitem_10                             0.121355             0.321926\n",
      "call_function  mul                                    0.102282             0.271328\n",
      "call_method    repeat_2                               0.100136             0.265636\n",
      "call_function  add_7                                  0.0956059            0.253619\n",
      "call_function  sum_2                                  0.0948906            0.251722\n",
      "call_module    linear_fc                              0.0867844            0.230218\n",
      "call_function  sum_6                                  0.0784397            0.208082\n",
      "call_method    repeat_1                               0.077486             0.205552\n",
      "call_function  sum_4                                  0.0736713            0.195432\n",
      "call_function  add_6                                  0.0698566            0.185313\n",
      "call_module    embedding_embedding                    0.0667572            0.177091\n",
      "call_function  getitem                                0.0658035            0.174561\n",
      "call_function  pow_1                                  0.0600815            0.159382\n",
      "call_function  sum_7                                  0.0600815            0.159382\n",
      "call_function  sub                                    0.0522137            0.13851\n",
      "call_function  getitem_8                              0.0510216            0.135348\n",
      "call_function  getitem_2                              0.0431538            0.114477\n",
      "call_function  getitem_4                              0.0431538            0.114477\n",
      "call_function  sigmoid                                0.0350475            0.0929727\n",
      "call_function  getitem_9                              0.0338554            0.0898103\n",
      "call_function  sum_1                                  0.0321865            0.0853831\n",
      "call_method    squeeze                                0.0312328            0.0828532\n",
      "call_function  pow_2                                  0.0295639            0.0784259\n",
      "call_function  add                                    0.0288486            0.0765285\n",
      "call_function  add_2                                  0.027895             0.0739986\n",
      "call_function  getitem_6                              0.0271797            0.0721012\n",
      "call_function  add_3                                  0.0269413            0.0714688\n",
      "call_function  getitem_1                              0.0252724            0.0670415\n",
      "call_function  getitem_5                              0.0252724            0.0670415\n",
      "call_function  getitem_7                              0.0245571            0.0651441\n",
      "placeholder    x                                      0.0243187            0.0645116\n",
      "call_function  add_8                                  0.0238419            0.0632467\n",
      "call_function  getitem_11                             0.0236034            0.0626142\n",
      "get_attr       embedding_prefix_offsets               0.0228882            0.0607168\n",
      "get_attr       linear_prefix_offsets                  0.0219345            0.058187\n",
      "call_function  getitem_3                              0.0219345            0.058187\n",
      "call_function  add_4                                  0.0181198            0.0480675\n",
      "get_attr       embedding_rest_offsets                 0.0171661            0.0455376\n",
      "get_attr       linear_bias                            0.0162125            0.0430078\n",
      "get_attr       linear_rest_offsets                    0.0152588            0.0404779\n",
      "output         output                                 0.0123978            0.0328883\n"
     ]
    }
   ],
   "source": [
    "interp = utils.ProfilingInterpreter(fm_modify_model_pow_sum_traced)\n",
    "input_data = torch.zeros((4096,100),dtype=torch.long)\n",
    "interp.run(input_data)\n",
    "print(interp.summary(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer import FeaturesEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReFactorizationMachineModel_pow_sum_embed(torch.nn.Module):\n",
    "  # 对 sum(x ** 2)和Embedding没有改写进行改写后的FM\n",
    "  def __init__(self, field_dims, embed_dim,prefix,batch):\n",
    "    super().__init__()\n",
    "    self.embedding = FeaturesEmbedding(field_dims, embed_dim)\n",
    "    self.linear = ReFeaturesLinear(field_dims,prefix,batch)\n",
    "    self.fm = ReFactorizationMachine_pow_sum(prefix,batch,reduce_sum=True)\n",
    "\n",
    "  def forward(self, x):\n",
    "      \"\"\"\n",
    "      :param x: Long tensor of size ``(batch_size, num_fields)``\n",
    "      \"\"\"\n",
    "      x = self.linear(x) + self.fm(self.embedding(x))\n",
    "      return torch.sigmoid(x.squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_model_pow_sum_embed = ReFactorizationMachineModel_pow_sum_embed([100 for _ in range(100)],64,50,4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_modify_model_pow_sum_embed_traced = symbolic_trace(fm_model_pow_sum_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total true time 19.382238388061523 ms\n",
      "total time: 25.19083023071289 ms\n",
      "Op type        Op                       Average runtime (ms)    Pct total runtime\n",
      "-------------  ---------------------  ----------------------  -------------------\n",
      "call_module    embedding_embedding                 9.34649             37.1027\n",
      "call_function  pow_3                               3.6881              14.6406\n",
      "call_function  sum_3                               1.40762              5.58784\n",
      "call_function  sum_5                               1.07837              4.28079\n",
      "call_module    linear_fc_1                         0.644207             2.55731\n",
      "call_function  add_4                               0.507593             2.01499\n",
      "call_function  concat                              0.336409             1.33544\n",
      "call_function  add_6                               0.252247             1.00134\n",
      "call_function  add_1                               0.203609             0.808268\n",
      "call_method    repeat                              0.159502             0.633175\n",
      "call_function  concat_1                            0.144958             0.575442\n",
      "call_module    linear_fc                           0.10252              0.406973\n",
      "call_function  getitem_6                           0.0984669            0.390884\n",
      "call_function  add_5                               0.0977516            0.388044\n",
      "call_method    repeat_1                            0.0960827            0.381419\n",
      "call_function  sum_2                               0.0858307            0.340722\n",
      "call_function  sub                                 0.0851154            0.337883\n",
      "call_function  sum_6                               0.0839233            0.33315\n",
      "call_function  sum_7                               0.0810623            0.321793\n",
      "call_function  getitem                             0.0789165            0.313275\n",
      "call_function  mul                                 0.0760555            0.301918\n",
      "call_function  sum_4                               0.0703335            0.279203\n",
      "call_function  pow_1                               0.0700951            0.278256\n",
      "call_function  getitem_2                           0.0548363            0.217683\n",
      "call_function  getitem_4                           0.0460148            0.182665\n",
      "call_function  sum_1                               0.0400543            0.159004\n",
      "call_function  add_3                               0.0391006            0.155218\n",
      "call_function  add                                 0.0360012            0.142914\n",
      "call_function  sigmoid                             0.0355244            0.141021\n",
      "call_method    squeeze                             0.0352859            0.140075\n",
      "call_function  add_2                               0.0309944            0.123038\n",
      "placeholder    x                                   0.0300407            0.119253\n",
      "call_function  getitem_7                           0.0274181            0.108842\n",
      "call_function  getitem_5                           0.0269413            0.106949\n",
      "call_function  getitem_1                           0.0247955            0.0984308\n",
      "call_function  add_7                               0.0243187            0.0965379\n",
      "get_attr       embedding_offsets                   0.0238419            0.094645\n",
      "call_function  pow_2                               0.0236034            0.0936985\n",
      "get_attr       linear_prefix_offsets               0.0228882            0.0908592\n",
      "call_function  getitem_3                           0.0219345            0.0870734\n",
      "get_attr       linear_bias                         0.0162125            0.0643586\n",
      "get_attr       linear_rest_offsets                 0.0152588            0.0605728\n",
      "output         output                              0.0119209            0.0473225\n"
     ]
    }
   ],
   "source": [
    "interp = utils.ProfilingInterpreter(fm_modify_model_pow_sum_embed_traced)\n",
    "input_data = torch.zeros((4096,100),dtype=torch.long)\n",
    "interp.run(input_data)\n",
    "print(interp.summary(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReFeaturesEmbedding_embed(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, field_dims, embed_dim,prefix,batch):\n",
    "        super().__init__()\n",
    "        self.prefix = prefix\n",
    "        self.embedding = torch.nn.Embedding(sum(field_dims), embed_dim)\n",
    "        self.offsets = torch.as_tensor(np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.int64))\n",
    "        self.prefix_offsets = self.offsets[:self.prefix]\n",
    "        self.rest_offsets = self.offsets[self.prefix:]\n",
    "        self.batch = batch\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
    "        self.prefix_slice = [0,slice(None,prefix)]\n",
    "        self.rest_slice = [slice(None,None),slice(prefix,None)]\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: tuple of tensor ``(prefix_index, rest_index)``\n",
    "        prefix_index ``(prefix_field)``\n",
    "        rest_index  ``(batch_szie,rest_field)``\n",
    "        \"\"\"\n",
    "        prefix_index, rest_index = x[self.prefix_slice],x[self.rest_slice]\n",
    "        prefix_index = prefix_index + self.prefix_offsets\n",
    "        rest_index = rest_index + self.rest_offsets\n",
    "        prefix_index = prefix_index.repeat(self.batch,1)\n",
    "        index = torch.concat([prefix_index,rest_index],dim = 1)\n",
    "        prefix_index, rest_index = index[self.prefix_slice], index[self.rest_slice]\n",
    "        return self.embedding(prefix_index), self.embedding(rest_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReFactorizationMachine_pow_sum_pow(torch.nn.Module):\n",
    "    # 对 sum(x ** 2)进行改写后的模块\n",
    "    def __init__(self,prefix,batch, reduce_sum=True):\n",
    "        super().__init__()\n",
    "        self.reduce_sum = reduce_sum\n",
    "        self.batch = batch\n",
    "        self.prefix_slice = [0,slice(None,prefix)]\n",
    "        self.rest_slice = [slice(None,None),slice(prefix,None)]\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
    "        \"\"\"\n",
    "        prefix_embed, rest_embed = x\n",
    "        # square_of_sum = torch.sum(x, dim=1) ** 2\n",
    "        square_of_sum = (torch.sum(rest_embed,dim = 1) + torch.sum(prefix_embed,dim = 0)) ** 2\n",
    "        prefix_embed, rest_embed = prefix_embed ** 2, rest_embed ** 2\n",
    "        # pow_embed = torch.concat([prefix_embed.repeat(self.batch,1,1),rest_embed],dim = 1)\n",
    "        # pow_prefix_embed, pow_rest_embed = pow_embed[self.prefix_slice],pow_embed[self.rest_slice]\n",
    "        sum_of_square = torch.sum(rest_embed, dim=1) + torch.sum(prefix_embed, dim = 0)\n",
    "        ix = square_of_sum - sum_of_square\n",
    "        if self.reduce_sum:\n",
    "            ix = torch.sum(ix, dim=1, keepdim=True)\n",
    "        return 0.5 * ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReFactorizationMachineModel_pow_sum_pow_embed(torch.nn.Module):\n",
    "  # 对 sum(x ** 2)和Embedding改写进行改写后的FM\n",
    "  def __init__(self, field_dims, embed_dim,prefix,batch):\n",
    "    super().__init__()\n",
    "    self.embedding = ReFeaturesEmbedding_embed(field_dims, embed_dim,prefix,batch)\n",
    "    self.linear = ReFeaturesLinear(field_dims,prefix,batch)\n",
    "    self.fm = ReFactorizationMachine_pow_sum_pow(prefix,batch,reduce_sum=True)\n",
    "\n",
    "  def forward(self, x):\n",
    "      \"\"\"\n",
    "      :param x: Long tensor of size ``(batch_size, num_fields)``\n",
    "      \"\"\"\n",
    "      x = self.linear(x) + self.fm(self.embedding(x))\n",
    "      return torch.sigmoid(x.squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_model_pow_sum_pow_embed = ReFactorizationMachineModel_pow_sum_pow_embed([100 for _ in range(100)],64,50,4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ReFactorizationMachineModel_pow_sum_pow_embed(torch.nn.Module):\n",
      "    def forward(self, x):\n",
      "        # No stacktrace found for following nodes\n",
      "        getitem = x[[0, slice(None, 50, None)]]\n",
      "        getitem_1 = x[[slice(None, None, None), slice(50, None, None)]]\n",
      "        linear_prefix_offsets = self.linear.prefix_offsets\n",
      "        add = getitem + linear_prefix_offsets;  getitem = linear_prefix_offsets = None\n",
      "        linear_rest_offsets = self.linear.rest_offsets\n",
      "        add_1 = getitem_1 + linear_rest_offsets;  getitem_1 = linear_rest_offsets = None\n",
      "        repeat = add.repeat(4096, 1);  add = None\n",
      "        concat = torch.concat([repeat, add_1], dim = 1);  repeat = add_1 = None\n",
      "        getitem_2 = concat[[0, slice(None, 50, None)]]\n",
      "        getitem_3 = concat[[slice(None, None, None), slice(50, None, None)]];  concat = None\n",
      "        linear_fc = self.linear.fc(getitem_2);  getitem_2 = None\n",
      "        linear_bias = self.linear.bias\n",
      "        add_2 = linear_fc + linear_bias;  linear_fc = linear_bias = None\n",
      "        repeat_1 = add_2.repeat(4096, 1, 1);  add_2 = None\n",
      "        linear_fc_1 = self.linear.fc(getitem_3);  getitem_3 = None\n",
      "        concat_1 = torch.concat([repeat_1, linear_fc_1], dim = 1);  repeat_1 = linear_fc_1 = None\n",
      "        getitem_4 = concat_1[[0, slice(None, 50, None)]]\n",
      "        getitem_5 = concat_1[[slice(None, None, None), slice(50, None, None)]];  concat_1 = None\n",
      "        sum_1 = torch.sum(getitem_4, dim = 0);  getitem_4 = None\n",
      "        sum_2 = torch.sum(getitem_5, dim = 1);  getitem_5 = None\n",
      "        add_3 = sum_1 + sum_2;  sum_1 = sum_2 = None\n",
      "        getitem_6 = x[[0, slice(None, 50, None)]]\n",
      "        getitem_7 = x[[slice(None, None, None), slice(50, None, None)]];  x = None\n",
      "        embedding_prefix_offsets = self.embedding.prefix_offsets\n",
      "        add_4 = getitem_6 + embedding_prefix_offsets;  getitem_6 = embedding_prefix_offsets = None\n",
      "        embedding_rest_offsets = self.embedding.rest_offsets\n",
      "        add_5 = getitem_7 + embedding_rest_offsets;  getitem_7 = embedding_rest_offsets = None\n",
      "        repeat_2 = add_4.repeat(4096, 1);  add_4 = None\n",
      "        concat_2 = torch.concat([repeat_2, add_5], dim = 1);  repeat_2 = add_5 = None\n",
      "        getitem_8 = concat_2[[0, slice(None, 50, None)]]\n",
      "        getitem_9 = concat_2[[slice(None, None, None), slice(50, None, None)]];  concat_2 = None\n",
      "        embedding_embedding = self.embedding.embedding(getitem_8);  getitem_8 = None\n",
      "        embedding_embedding_1 = self.embedding.embedding(getitem_9);  getitem_9 = None\n",
      "        sum_3 = torch.sum(embedding_embedding_1, dim = 1)\n",
      "        sum_4 = torch.sum(embedding_embedding, dim = 0)\n",
      "        add_6 = sum_3 + sum_4;  sum_3 = sum_4 = None\n",
      "        pow_1 = add_6 ** 2;  add_6 = None\n",
      "        pow_2 = embedding_embedding ** 2;  embedding_embedding = None\n",
      "        pow_3 = embedding_embedding_1 ** 2;  embedding_embedding_1 = None\n",
      "        sum_5 = torch.sum(pow_3, dim = 1);  pow_3 = None\n",
      "        sum_6 = torch.sum(pow_2, dim = 0);  pow_2 = None\n",
      "        add_7 = sum_5 + sum_6;  sum_5 = sum_6 = None\n",
      "        sub = pow_1 - add_7;  pow_1 = add_7 = None\n",
      "        sum_7 = torch.sum(sub, dim = 1, keepdim = True);  sub = None\n",
      "        mul = 0.5 * sum_7;  sum_7 = None\n",
      "        add_8 = add_3 + mul;  add_3 = mul = None\n",
      "        squeeze = add_8.squeeze(1);  add_8 = None\n",
      "        sigmoid = torch.sigmoid(squeeze);  squeeze = None\n",
      "        return sigmoid\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'class ReFactorizationMachineModel_pow_sum_pow_embed(torch.nn.Module):\\n    def forward(self, x):\\n        # No stacktrace found for following nodes\\n        getitem = x[[0, slice(None, 50, None)]]\\n        getitem_1 = x[[slice(None, None, None), slice(50, None, None)]]\\n        linear_prefix_offsets = self.linear.prefix_offsets\\n        add = getitem + linear_prefix_offsets;  getitem = linear_prefix_offsets = None\\n        linear_rest_offsets = self.linear.rest_offsets\\n        add_1 = getitem_1 + linear_rest_offsets;  getitem_1 = linear_rest_offsets = None\\n        repeat = add.repeat(4096, 1);  add = None\\n        concat = torch.concat([repeat, add_1], dim = 1);  repeat = add_1 = None\\n        getitem_2 = concat[[0, slice(None, 50, None)]]\\n        getitem_3 = concat[[slice(None, None, None), slice(50, None, None)]];  concat = None\\n        linear_fc = self.linear.fc(getitem_2);  getitem_2 = None\\n        linear_bias = self.linear.bias\\n        add_2 = linear_fc + linear_bias;  linear_fc = linear_bias = None\\n        repeat_1 = add_2.repeat(4096, 1, 1);  add_2 = None\\n        linear_fc_1 = self.linear.fc(getitem_3);  getitem_3 = None\\n        concat_1 = torch.concat([repeat_1, linear_fc_1], dim = 1);  repeat_1 = linear_fc_1 = None\\n        getitem_4 = concat_1[[0, slice(None, 50, None)]]\\n        getitem_5 = concat_1[[slice(None, None, None), slice(50, None, None)]];  concat_1 = None\\n        sum_1 = torch.sum(getitem_4, dim = 0);  getitem_4 = None\\n        sum_2 = torch.sum(getitem_5, dim = 1);  getitem_5 = None\\n        add_3 = sum_1 + sum_2;  sum_1 = sum_2 = None\\n        getitem_6 = x[[0, slice(None, 50, None)]]\\n        getitem_7 = x[[slice(None, None, None), slice(50, None, None)]];  x = None\\n        embedding_prefix_offsets = self.embedding.prefix_offsets\\n        add_4 = getitem_6 + embedding_prefix_offsets;  getitem_6 = embedding_prefix_offsets = None\\n        embedding_rest_offsets = self.embedding.rest_offsets\\n        add_5 = getitem_7 + embedding_rest_offsets;  getitem_7 = embedding_rest_offsets = None\\n        repeat_2 = add_4.repeat(4096, 1);  add_4 = None\\n        concat_2 = torch.concat([repeat_2, add_5], dim = 1);  repeat_2 = add_5 = None\\n        getitem_8 = concat_2[[0, slice(None, 50, None)]]\\n        getitem_9 = concat_2[[slice(None, None, None), slice(50, None, None)]];  concat_2 = None\\n        embedding_embedding = self.embedding.embedding(getitem_8);  getitem_8 = None\\n        embedding_embedding_1 = self.embedding.embedding(getitem_9);  getitem_9 = None\\n        sum_3 = torch.sum(embedding_embedding_1, dim = 1)\\n        sum_4 = torch.sum(embedding_embedding, dim = 0)\\n        add_6 = sum_3 + sum_4;  sum_3 = sum_4 = None\\n        pow_1 = add_6 ** 2;  add_6 = None\\n        pow_2 = embedding_embedding ** 2;  embedding_embedding = None\\n        pow_3 = embedding_embedding_1 ** 2;  embedding_embedding_1 = None\\n        sum_5 = torch.sum(pow_3, dim = 1);  pow_3 = None\\n        sum_6 = torch.sum(pow_2, dim = 0);  pow_2 = None\\n        add_7 = sum_5 + sum_6;  sum_5 = sum_6 = None\\n        sub = pow_1 - add_7;  pow_1 = add_7 = None\\n        sum_7 = torch.sum(sub, dim = 1, keepdim = True);  sub = None\\n        mul = 0.5 * sum_7;  sum_7 = None\\n        add_8 = add_3 + mul;  add_3 = mul = None\\n        squeeze = add_8.squeeze(1);  add_8 = None\\n        sigmoid = torch.sigmoid(squeeze);  squeeze = None\\n        return sigmoid\\n        '"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_model_pow_sum_pow_embed_trace = symbolic_trace(fm_model_pow_sum_pow_embed)\n",
    "fm_model_pow_sum_pow_embed_trace.print_readable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total true time 15.653610229492188 ms\n",
      "total time: 20.832061767578125 ms\n",
      "Op type        Op                          Average runtime (ms)    Pct total runtime\n",
      "-------------  ------------------------  ----------------------  -------------------\n",
      "call_module    embedding_embedding_1                  4.71473             22.6321\n",
      "call_function  pow_3                                  3.35765             16.1177\n",
      "call_function  sum_3                                  1.31798              6.32668\n",
      "call_function  sum_5                                  1.22976              5.90322\n",
      "call_function  concat                                 0.803232             3.85575\n",
      "call_function  concat_2                               0.786066             3.77335\n",
      "call_module    linear_fc_1                            0.569582             2.73416\n",
      "call_function  add_5                                  0.457525             2.19626\n",
      "call_method    repeat_2                               0.393867             1.89068\n",
      "call_function  add_1                                  0.158072             0.75879\n",
      "call_method    repeat                                 0.140429             0.674098\n",
      "call_function  concat_1                               0.133514             0.640908\n",
      "call_function  add_7                                  0.105143             0.504715\n",
      "call_function  sum_6                                  0.0898838            0.431469\n",
      "call_method    repeat_1                               0.0853539            0.409723\n",
      "call_module    linear_fc                              0.082016             0.393701\n",
      "call_function  add_6                                  0.0815392            0.391412\n",
      "call_function  sum_2                                  0.0751019            0.360511\n",
      "call_module    embedding_embedding                    0.0727177            0.349066\n",
      "call_function  mul                                    0.0696182            0.334188\n",
      "call_function  getitem                                0.067234             0.322743\n",
      "call_function  pow_1                                  0.0619888            0.297565\n",
      "call_function  sum_7                                  0.0610352            0.292987\n",
      "call_function  sub                                    0.0567436            0.272386\n",
      "call_function  getitem_8                              0.054121             0.259797\n",
      "call_function  sum_4                                  0.0457764            0.21974\n",
      "call_function  getitem_2                              0.0429153            0.206006\n",
      "call_function  getitem_4                              0.0391006            0.187695\n",
      "get_attr       linear_prefix_offsets                  0.0360012            0.172816\n",
      "call_function  add_3                                  0.0345707            0.165949\n",
      "call_function  sigmoid                                0.0326633            0.156794\n",
      "call_function  sum_1                                  0.0314713            0.151071\n",
      "call_function  add                                    0.0305176            0.146493\n",
      "call_method    squeeze                                0.0295639            0.141915\n",
      "placeholder    x                                      0.025034             0.12017\n",
      "call_function  add_2                                  0.0247955            0.119026\n",
      "call_function  getitem_6                              0.0238419            0.114448\n",
      "call_function  getitem_5                              0.0226498            0.108726\n",
      "call_function  getitem_9                              0.0226498            0.108726\n",
      "call_function  getitem_1                              0.0216961            0.104148\n",
      "call_function  add_8                                  0.0212193            0.101859\n",
      "call_function  getitem_7                              0.0200272            0.0961362\n",
      "get_attr       embedding_prefix_offsets               0.0200272            0.0961362\n",
      "call_function  pow_2                                  0.0197887            0.0949918\n",
      "call_function  getitem_3                              0.0188351            0.0904138\n",
      "call_function  add_4                                  0.0152588            0.0732467\n",
      "get_attr       linear_bias                            0.0138283            0.0663798\n",
      "get_attr       embedding_rest_offsets                 0.0135899            0.0652353\n",
      "get_attr       linear_rest_offsets                    0.0123978            0.0595129\n",
      "output         output                                 0.0104904            0.0503571\n"
     ]
    }
   ],
   "source": [
    "interp = utils.ProfilingInterpreter(fm_model_pow_sum_pow_embed)\n",
    "input_data = torch.zeros((4096,100),dtype=torch.long)\n",
    "interp.run(input_data)\n",
    "print(interp.summary(True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepctr-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
