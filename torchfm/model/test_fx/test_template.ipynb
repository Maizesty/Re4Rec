{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace success!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import pnn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.fx import subgraph_rewriter, symbolic_trace\n",
    "import utils\n",
    "from torch.fx import Proxy, Graph, GraphModule\n",
    "from torch.fx.passes.utils.matcher_utils import SubgraphMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class original_model_template(torch.nn.Module):\n",
    "  def __init__(self, embed_module, interact_modules, agg_module):\n",
    "    super().__init__()\n",
    "    self.embed_module = embed_module\n",
    "    self.interact_modules = nn.ModuleList(interact_modules)\n",
    "    self.agg_module = agg_module\n",
    "    \n",
    "  def forward(self, x):\n",
    "    embed = self.embed_module(x)\n",
    "    interact_results = [self.interact_modules[i](embed) for i in range(len(self.interact_modules))]\n",
    "    return self.agg_module(interact_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fused_model_template(torch.nn.Module):\n",
    "  def __init__(self, embed_module, interact_modules, fuse_module, modify_embedding_module, interact_modules_input_type):\n",
    "    super().__init__()\n",
    "    self.embed_module = embed_module\n",
    "    self.interact_modules = nn.ModuleList(interact_modules)\n",
    "    self.fuse_module = fuse_module\n",
    "    self.interact_modules_input_type = interact_modules_input_type\n",
    "    self.modify_embedding_module = modify_embedding_module\n",
    "  \n",
    "  def process(self, i, embed, x):\n",
    "    if self.interact_modules_input_type[i] == \"embed\":\n",
    "        return self.interact_modules[i](embed)\n",
    "    else:\n",
    "        return self.interact_modules[i](x)    \n",
    "      \n",
    "  def forward(self, x):\n",
    "    if self.modify_embedding_module:\n",
    "      embed = self.embed_module(x)\n",
    "      interact_results = [self.process(i, embed, x)  for i in range(len(self.interact_modules))]\n",
    "    else:\n",
    "      embed = x \n",
    "      interact_results = [self.interact_modules[i](embed)  for i in range(len(self.interact_modules))]\n",
    "    return self.fuse_module(interact_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class embed_module_single(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "      super().__init__()\n",
    "      self.embed = torch.nn.Embedding(1, 1)  \n",
    "      \n",
    "  def forward(self, x):\n",
    "    return self.embed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class product_sum_module(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "      super().__init__()\n",
    "  \n",
    "  def forward(self, x):\n",
    "    square_of_sum = torch.sum(x, dim=1) ** 2\n",
    "    sum_of_square = torch.sum(x ** 2, dim=1)\n",
    "    ix = square_of_sum - sum_of_square\n",
    "    ix = torch.sum(ix, dim=1, keepdim=True)\n",
    "    return 0.5 * ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp_module(torch.nn.Module):\n",
    "  def __init__(self, num_fields, embed_dim):\n",
    "    super().__init__()\n",
    "    self.mlp_layer = torch.nn.Linear(1, 1)\n",
    "    self.len_of_embed = num_fields * embed_dim\n",
    "    \n",
    "  def forward(self, x):\n",
    "    return self.mlp_layer(x.view(-1,self.len_of_embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class no_agg_module(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()  \n",
    "  \n",
    "  def forward(self, x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = original_model_template(embed_module_single(), [product_sum_module(),mlp_module(100,32) ],no_agg_module())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "traced = symbolic_trace(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class original_model_template(torch.nn.Module):\n",
      "    def forward(self, x):\n",
      "        # No stacktrace found for following nodes\n",
      "        embed_module_embed = self.embed_module.embed(x);  x = None\n",
      "        sum_1 = torch.sum(embed_module_embed, dim = 1)\n",
      "        pow_1 = sum_1 ** 2;  sum_1 = None\n",
      "        pow_2 = embed_module_embed ** 2\n",
      "        sum_2 = torch.sum(pow_2, dim = 1);  pow_2 = None\n",
      "        sub = pow_1 - sum_2;  pow_1 = sum_2 = None\n",
      "        sum_3 = torch.sum(sub, dim = 1, keepdim = True);  sub = None\n",
      "        mul = 0.5 * sum_3;  sum_3 = None\n",
      "        view = embed_module_embed.view(-1, 3200);  embed_module_embed = None\n",
      "        interact_modules_1_mlp_layer = getattr(self.interact_modules, \"1\").mlp_layer(view);  view = None\n",
      "        return [mul, interact_modules_1_mlp_layer]\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'class original_model_template(torch.nn.Module):\\n    def forward(self, x):\\n        # No stacktrace found for following nodes\\n        embed_module_embed = self.embed_module.embed(x);  x = None\\n        sum_1 = torch.sum(embed_module_embed, dim = 1)\\n        pow_1 = sum_1 ** 2;  sum_1 = None\\n        pow_2 = embed_module_embed ** 2\\n        sum_2 = torch.sum(pow_2, dim = 1);  pow_2 = None\\n        sub = pow_1 - sum_2;  pow_1 = sum_2 = None\\n        sum_3 = torch.sum(sub, dim = 1, keepdim = True);  sub = None\\n        mul = 0.5 * sum_3;  sum_3 = None\\n        view = embed_module_embed.view(-1, 3200);  embed_module_embed = None\\n        interact_modules_1_mlp_layer = getattr(self.interact_modules, \"1\").mlp_layer(view);  view = None\\n        return [mul, interact_modules_1_mlp_layer]\\n        '"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traced.print_readable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name                          target                                                  args                                    kwargs\n",
      "-------------  ----------------------------  ------------------------------------------------------  --------------------------------------  ---------------------------\n",
      "placeholder    x                             x                                                       ()                                      {}\n",
      "call_module    embed_module_embed            embed_module.embed                                      (x,)                                    {}\n",
      "call_function  sum_1                         <built-in method sum of type object at 0x7f5f2be81760>  (embed_module_embed,)                   {'dim': 1}\n",
      "call_function  pow_1                         <built-in function pow>                                 (sum_1, 2)                              {}\n",
      "call_function  pow_2                         <built-in function pow>                                 (embed_module_embed, 2)                 {}\n",
      "call_function  sum_2                         <built-in method sum of type object at 0x7f5f2be81760>  (pow_2,)                                {'dim': 1}\n",
      "call_function  sub                           <built-in function sub>                                 (pow_1, sum_2)                          {}\n",
      "call_function  sum_3                         <built-in method sum of type object at 0x7f5f2be81760>  (sub,)                                  {'dim': 1, 'keepdim': True}\n",
      "call_function  mul                           <built-in function mul>                                 (0.5, sum_3)                            {}\n",
      "call_method    view                          view                                                    (embed_module_embed, -1, 3200)          {}\n",
      "call_module    interact_modules_1_mlp_layer  interact_modules.1.mlp_layer                            (view,)                                 {}\n",
      "output         output                        output                                                  ((mul, interact_modules_1_mlp_layer),)  {}\n"
     ]
    }
   ],
   "source": [
    "traced.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm_model = dfm.DeepFactorizationMachineModel([100 for _ in range(100)],32,[400,400,400],0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm_model_traced = symbolic_trace(dfm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class DeepFactorizationMachineModel(torch.nn.Module):\n",
      "    def forward(self, x):\n",
      "        # No stacktrace found for following nodes\n",
      "        embedding_offsets = self.embedding.offsets\n",
      "        add = x + embedding_offsets;  embedding_offsets = None\n",
      "        embedding_embedding = self.embedding.embedding(add);  add = None\n",
      "        linear_offsets = self.linear.offsets\n",
      "        add_1 = x + linear_offsets;  x = linear_offsets = None\n",
      "        linear_fc = self.linear.fc(add_1);  add_1 = None\n",
      "        sum_1 = torch.sum(linear_fc, dim = 1);  linear_fc = None\n",
      "        linear_bias = self.linear.bias\n",
      "        add_2 = sum_1 + linear_bias;  sum_1 = linear_bias = None\n",
      "        sum_2 = torch.sum(embedding_embedding, dim = 1)\n",
      "        pow_1 = sum_2 ** 2;  sum_2 = None\n",
      "        pow_2 = embedding_embedding ** 2\n",
      "        sum_3 = torch.sum(pow_2, dim = 1);  pow_2 = None\n",
      "        sub = pow_1 - sum_3;  pow_1 = sum_3 = None\n",
      "        sum_4 = torch.sum(sub, dim = 1, keepdim = True);  sub = None\n",
      "        mul = 0.5 * sum_4;  sum_4 = None\n",
      "        add_3 = add_2 + mul;  add_2 = mul = None\n",
      "        view = embedding_embedding.view(-1, 3200);  embedding_embedding = None\n",
      "        mlp_mlp_0 = getattr(self.mlp.mlp, \"0\")(view);  view = None\n",
      "        mlp_mlp_1 = getattr(self.mlp.mlp, \"1\")(mlp_mlp_0);  mlp_mlp_0 = None\n",
      "        mlp_mlp_2 = getattr(self.mlp.mlp, \"2\")(mlp_mlp_1);  mlp_mlp_1 = None\n",
      "        mlp_mlp_3 = getattr(self.mlp.mlp, \"3\")(mlp_mlp_2);  mlp_mlp_2 = None\n",
      "        mlp_mlp_4 = getattr(self.mlp.mlp, \"4\")(mlp_mlp_3);  mlp_mlp_3 = None\n",
      "        mlp_mlp_5 = getattr(self.mlp.mlp, \"5\")(mlp_mlp_4);  mlp_mlp_4 = None\n",
      "        mlp_mlp_6 = getattr(self.mlp.mlp, \"6\")(mlp_mlp_5);  mlp_mlp_5 = None\n",
      "        mlp_mlp_7 = getattr(self.mlp.mlp, \"7\")(mlp_mlp_6);  mlp_mlp_6 = None\n",
      "        mlp_mlp_8 = getattr(self.mlp.mlp, \"8\")(mlp_mlp_7);  mlp_mlp_7 = None\n",
      "        mlp_mlp_9 = getattr(self.mlp.mlp, \"9\")(mlp_mlp_8);  mlp_mlp_8 = None\n",
      "        add_4 = add_3 + mlp_mlp_9;  add_3 = mlp_mlp_9 = None\n",
      "        squeeze = add_4.squeeze(1);  add_4 = None\n",
      "        sigmoid = torch.sigmoid(squeeze);  squeeze = None\n",
      "        return sigmoid\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'class DeepFactorizationMachineModel(torch.nn.Module):\\n    def forward(self, x):\\n        # No stacktrace found for following nodes\\n        embedding_offsets = self.embedding.offsets\\n        add = x + embedding_offsets;  embedding_offsets = None\\n        embedding_embedding = self.embedding.embedding(add);  add = None\\n        linear_offsets = self.linear.offsets\\n        add_1 = x + linear_offsets;  x = linear_offsets = None\\n        linear_fc = self.linear.fc(add_1);  add_1 = None\\n        sum_1 = torch.sum(linear_fc, dim = 1);  linear_fc = None\\n        linear_bias = self.linear.bias\\n        add_2 = sum_1 + linear_bias;  sum_1 = linear_bias = None\\n        sum_2 = torch.sum(embedding_embedding, dim = 1)\\n        pow_1 = sum_2 ** 2;  sum_2 = None\\n        pow_2 = embedding_embedding ** 2\\n        sum_3 = torch.sum(pow_2, dim = 1);  pow_2 = None\\n        sub = pow_1 - sum_3;  pow_1 = sum_3 = None\\n        sum_4 = torch.sum(sub, dim = 1, keepdim = True);  sub = None\\n        mul = 0.5 * sum_4;  sum_4 = None\\n        add_3 = add_2 + mul;  add_2 = mul = None\\n        view = embedding_embedding.view(-1, 3200);  embedding_embedding = None\\n        mlp_mlp_0 = getattr(self.mlp.mlp, \"0\")(view);  view = None\\n        mlp_mlp_1 = getattr(self.mlp.mlp, \"1\")(mlp_mlp_0);  mlp_mlp_0 = None\\n        mlp_mlp_2 = getattr(self.mlp.mlp, \"2\")(mlp_mlp_1);  mlp_mlp_1 = None\\n        mlp_mlp_3 = getattr(self.mlp.mlp, \"3\")(mlp_mlp_2);  mlp_mlp_2 = None\\n        mlp_mlp_4 = getattr(self.mlp.mlp, \"4\")(mlp_mlp_3);  mlp_mlp_3 = None\\n        mlp_mlp_5 = getattr(self.mlp.mlp, \"5\")(mlp_mlp_4);  mlp_mlp_4 = None\\n        mlp_mlp_6 = getattr(self.mlp.mlp, \"6\")(mlp_mlp_5);  mlp_mlp_5 = None\\n        mlp_mlp_7 = getattr(self.mlp.mlp, \"7\")(mlp_mlp_6);  mlp_mlp_6 = None\\n        mlp_mlp_8 = getattr(self.mlp.mlp, \"8\")(mlp_mlp_7);  mlp_mlp_7 = None\\n        mlp_mlp_9 = getattr(self.mlp.mlp, \"9\")(mlp_mlp_8);  mlp_mlp_8 = None\\n        add_4 = add_3 + mlp_mlp_9;  add_3 = mlp_mlp_9 = None\\n        squeeze = add_4.squeeze(1);  add_4 = None\\n        sigmoid = torch.sigmoid(squeeze);  squeeze = None\\n        return sigmoid\\n        '"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfm_model_traced.print_readable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name                 target                                                      args                             kwargs\n",
      "-------------  -------------------  ----------------------------------------------------------  -------------------------------  ---------------------------\n",
      "placeholder    x                    x                                                           ()                               {}\n",
      "get_attr       embedding_offsets    embedding.offsets                                           ()                               {}\n",
      "call_function  add                  <built-in function add>                                     (x, embedding_offsets)           {}\n",
      "call_module    embedding_embedding  embedding.embedding                                         (add,)                           {}\n",
      "get_attr       linear_offsets       linear.offsets                                              ()                               {}\n",
      "call_function  add_1                <built-in function add>                                     (x, linear_offsets)              {}\n",
      "call_module    linear_fc            linear.fc                                                   (add_1,)                         {}\n",
      "call_function  sum_1                <built-in method sum of type object at 0x7f5f2be81760>      (linear_fc,)                     {'dim': 1}\n",
      "get_attr       linear_bias          linear.bias                                                 ()                               {}\n",
      "call_function  add_2                <built-in function add>                                     (sum_1, linear_bias)             {}\n",
      "call_function  sum_2                <built-in method sum of type object at 0x7f5f2be81760>      (embedding_embedding,)           {'dim': 1}\n",
      "call_function  pow_1                <built-in function pow>                                     (sum_2, 2)                       {}\n",
      "call_function  pow_2                <built-in function pow>                                     (embedding_embedding, 2)         {}\n",
      "call_function  sum_3                <built-in method sum of type object at 0x7f5f2be81760>      (pow_2,)                         {'dim': 1}\n",
      "call_function  sub                  <built-in function sub>                                     (pow_1, sum_3)                   {}\n",
      "call_function  sum_4                <built-in method sum of type object at 0x7f5f2be81760>      (sub,)                           {'dim': 1, 'keepdim': True}\n",
      "call_function  mul                  <built-in function mul>                                     (0.5, sum_4)                     {}\n",
      "call_function  add_3                <built-in function add>                                     (add_2, mul)                     {}\n",
      "call_method    view                 view                                                        (embedding_embedding, -1, 3200)  {}\n",
      "call_module    mlp_mlp_0            mlp.mlp.0                                                   (view,)                          {}\n",
      "call_module    mlp_mlp_1            mlp.mlp.1                                                   (mlp_mlp_0,)                     {}\n",
      "call_module    mlp_mlp_2            mlp.mlp.2                                                   (mlp_mlp_1,)                     {}\n",
      "call_module    mlp_mlp_3            mlp.mlp.3                                                   (mlp_mlp_2,)                     {}\n",
      "call_module    mlp_mlp_4            mlp.mlp.4                                                   (mlp_mlp_3,)                     {}\n",
      "call_module    mlp_mlp_5            mlp.mlp.5                                                   (mlp_mlp_4,)                     {}\n",
      "call_module    mlp_mlp_6            mlp.mlp.6                                                   (mlp_mlp_5,)                     {}\n",
      "call_module    mlp_mlp_7            mlp.mlp.7                                                   (mlp_mlp_6,)                     {}\n",
      "call_module    mlp_mlp_8            mlp.mlp.8                                                   (mlp_mlp_7,)                     {}\n",
      "call_module    mlp_mlp_9            mlp.mlp.9                                                   (mlp_mlp_8,)                     {}\n",
      "call_function  add_4                <built-in function add>                                     (add_3, mlp_mlp_9)               {}\n",
      "call_method    squeeze              squeeze                                                     (add_4, 1)                       {}\n",
      "call_function  sigmoid              <built-in method sigmoid of type object at 0x7f5f2be81760>  (squeeze,)                       {}\n",
      "output         output               output                                                      (sigmoid,)                       {}\n"
     ]
    }
   ],
   "source": [
    "dfm_model_traced.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher =  SubgraphMatcher(traced.graph, match_output=False, match_placeholder=False,\n",
    "                            remove_overlapping_matches=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "_matches = matcher.match(dfm_model_traced.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[InternalMatch(anchors=[mul, interact_modules_1_mlp_layer], nodes_map={mul: mul, sum_3: sum_4, sub: sub, pow_1: pow_1, sum_1: sum_2, embed_module_embed: embedding_embedding, x: add, sum_2: sum_3, pow_2: pow_2, interact_modules_1_mlp_layer: mlp_mlp_0, view: view}, placeholder_nodes=[add], returning_nodes=[mul, mlp_mlp_0], name_node_map={})]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_matches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepctr-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
